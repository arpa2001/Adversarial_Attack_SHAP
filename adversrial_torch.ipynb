{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adversrial_torch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38IT8z02qMqm",
        "outputId": "a233098c-8145-4b0c-8fce-fbe60c3fa6f1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 18 12:26:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L8Cl8aKWmwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538701c8-041a-4db1-821a-83fc64b15a9d"
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEzZGMuyWupu"
      },
      "source": [
        "import torch, torchvision\n",
        "from torch import nn,optim\n",
        "from torch.autograd import Variable as var \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpbqMI0pqPY7"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agvTulUpW0T_"
      },
      "source": [
        "n_batch = 64\n",
        "learning_rate = 0.01\n",
        "num_epochs = 3\n",
        "n_print = 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVV4TXyDXQhA"
      },
      "source": [
        "T = torchvision.transforms.ToTensor()\n",
        "train_data = torchvision.datasets.MNIST('mnist_data',train=True,download=True,transform=T)\n",
        "val_data = torchvision.datasets.MNIST('mnist_data',train=False,download=True,transform=T)\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_data,batch_size = n_batch)\n",
        "val_dl = torch.utils.data.DataLoader(val_data,batch_size = n_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlj2zfqCSZKY",
        "outputId": "4912c7fb-88cf-4ddf-acac-4e89185a9fdf"
      },
      "source": [
        "val_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: mnist_data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txxh6-wETvpg"
      },
      "source": [
        "img = iter(train_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAAI2W5KT3Rd"
      },
      "source": [
        "img,label = img.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8mRKMmx3Cx"
      },
      "source": [
        "img = img[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwE32uKtz6Qj",
        "outputId": "7052a819-3ed6-4c5d-9ffc-6377e5ac4b72"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuA3fzkxUGPa"
      },
      "source": [
        "npimg = img.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7PljNiSy6Am",
        "outputId": "abff6ebf-da79-4c7c-f879-41dbddcc9729"
      },
      "source": [
        "npimg.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "A1mUbh9s0TEp",
        "outputId": "28d9ca2b-87dd-4a52-9242-16c2a2cb2c63"
      },
      "source": [
        "plt.imshow(train_data[2][0][0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efff004c790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uAnAwBP1TiR",
        "outputId": "7b31dc36-5690-480c-d324-b359fe56a364"
      },
      "source": [
        "train_data[0][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaIdatnI0wCm",
        "outputId": "7574da45-bb5c-4a96-a551-dfe8bafb912b"
      },
      "source": [
        "train_data[0][0][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xts_SpjRUoPS",
        "outputId": "5d0f25fb-ca32-4498-f2ce-116ed80f064a"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
              "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
              "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eUfQiI9UqP6"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.size(0)\n",
        "        x = self.pool(F.relu(self.conv1(x)))  \n",
        "        x = self.pool(F.relu(self.conv2(x)))  \n",
        "        x = x.view(n, -1)            \n",
        "        x = F.relu(self.fc1(x))               \n",
        "        x = F.relu(self.fc2(x))               \n",
        "        x = self.fc3(x)                       \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_TEz1QrU8ui"
      },
      "source": [
        "# model = ConvNet().to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# n_total_steps = len(train_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYgNflGapACX"
      },
      "source": [
        "# def validate(model,data):\n",
        "#   # To get validation accuracy = (correct/total)*100.\n",
        "#   total = 0\n",
        "#   correct = 0\n",
        "#   for i,(images,labels) in enumerate(data):\n",
        "#     images = images.to(device)\n",
        "#     labels = labels.to(device)\n",
        "#     x = model(images)\n",
        "#     value,pred = torch.max(x,1)\n",
        "#     total += x.size(0)\n",
        "#     correct += torch.sum(pred == labels)\n",
        "#   return correct*100./total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X8G665tVhP7"
      },
      "source": [
        "# for epoch in range(num_epochs):\n",
        "#     for i, (images, labels) in enumerate(train_dl):\n",
        "#         images = images.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         Acc = float(validate(model,val_dl))\n",
        "\n",
        "#         print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}', 'Acc:', Acc,'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRi90RXpdPe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8444ba6d-1dde-4c6e-f917-6105a2d2376e"
      },
      "source": [
        "mycnn = ConvNet()\n",
        "cec = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mycnn.parameters(),lr = learning_rate)\n",
        "\n",
        "def validate(model,data):\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for i,(images,labels) in enumerate(data):\n",
        "    images = var(images.cuda())\n",
        "    x = model(images)\n",
        "    value,pred = torch.max(x,1)\n",
        "    pred = pred.data.cpu()\n",
        "    total += x.size(0)\n",
        "    correct += torch.sum(pred == labels)\n",
        "  return correct*100./total\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for i,(images,labels) in enumerate(train_dl):\n",
        "    images = images\n",
        "    labels = labels\n",
        "    optimizer.zero_grad()\n",
        "    pred = mycnn(images)\n",
        "    loss = cec(pred,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (i+1) % n_print == 0:\n",
        "      # accuracy = float(validate(mycnn,val_dl))\n",
        "      print('Epoch :',e+1,'Batch :',i+1,'Loss :',float(loss.data),'Accuracy :')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 Batch : 10 Loss : 2.149012327194214 Accuracy :\n",
            "Epoch : 1 Batch : 20 Loss : 1.4160370826721191 Accuracy :\n",
            "Epoch : 1 Batch : 30 Loss : 0.5594965219497681 Accuracy :\n",
            "Epoch : 1 Batch : 40 Loss : 0.3481639623641968 Accuracy :\n",
            "Epoch : 1 Batch : 50 Loss : 0.3528524935245514 Accuracy :\n",
            "Epoch : 1 Batch : 60 Loss : 0.3664316236972809 Accuracy :\n",
            "Epoch : 1 Batch : 70 Loss : 0.24112443625926971 Accuracy :\n",
            "Epoch : 1 Batch : 80 Loss : 0.1687723994255066 Accuracy :\n",
            "Epoch : 1 Batch : 90 Loss : 0.19410450756549835 Accuracy :\n",
            "Epoch : 1 Batch : 100 Loss : 0.07920756191015244 Accuracy :\n",
            "Epoch : 1 Batch : 110 Loss : 0.42307668924331665 Accuracy :\n",
            "Epoch : 1 Batch : 120 Loss : 0.14587241411209106 Accuracy :\n",
            "Epoch : 1 Batch : 130 Loss : 0.23590433597564697 Accuracy :\n",
            "Epoch : 1 Batch : 140 Loss : 0.49549150466918945 Accuracy :\n",
            "Epoch : 1 Batch : 150 Loss : 0.34872329235076904 Accuracy :\n",
            "Epoch : 1 Batch : 160 Loss : 0.17799162864685059 Accuracy :\n",
            "Epoch : 1 Batch : 170 Loss : 0.2973323464393616 Accuracy :\n",
            "Epoch : 1 Batch : 180 Loss : 0.12149898707866669 Accuracy :\n",
            "Epoch : 1 Batch : 190 Loss : 0.13861927390098572 Accuracy :\n",
            "Epoch : 1 Batch : 200 Loss : 0.282015323638916 Accuracy :\n",
            "Epoch : 1 Batch : 210 Loss : 0.16280844807624817 Accuracy :\n",
            "Epoch : 1 Batch : 220 Loss : 0.13258357346057892 Accuracy :\n",
            "Epoch : 1 Batch : 230 Loss : 0.20414531230926514 Accuracy :\n",
            "Epoch : 1 Batch : 240 Loss : 0.09201911836862564 Accuracy :\n",
            "Epoch : 1 Batch : 250 Loss : 0.251766562461853 Accuracy :\n",
            "Epoch : 1 Batch : 260 Loss : 0.0360831655561924 Accuracy :\n",
            "Epoch : 1 Batch : 270 Loss : 0.18353599309921265 Accuracy :\n",
            "Epoch : 1 Batch : 280 Loss : 0.09208396822214127 Accuracy :\n",
            "Epoch : 1 Batch : 290 Loss : 0.07102629542350769 Accuracy :\n",
            "Epoch : 1 Batch : 300 Loss : 0.1471894085407257 Accuracy :\n",
            "Epoch : 1 Batch : 310 Loss : 0.11842980980873108 Accuracy :\n",
            "Epoch : 1 Batch : 320 Loss : 0.042875416576862335 Accuracy :\n",
            "Epoch : 1 Batch : 330 Loss : 0.09899340569972992 Accuracy :\n",
            "Epoch : 1 Batch : 340 Loss : 0.14063653349876404 Accuracy :\n",
            "Epoch : 1 Batch : 350 Loss : 0.008187786675989628 Accuracy :\n",
            "Epoch : 1 Batch : 360 Loss : 0.14468544721603394 Accuracy :\n",
            "Epoch : 1 Batch : 370 Loss : 0.08176097273826599 Accuracy :\n",
            "Epoch : 1 Batch : 380 Loss : 0.08839330077171326 Accuracy :\n",
            "Epoch : 1 Batch : 390 Loss : 0.07208754122257233 Accuracy :\n",
            "Epoch : 1 Batch : 400 Loss : 0.21104787290096283 Accuracy :\n",
            "Epoch : 1 Batch : 410 Loss : 0.02694806642830372 Accuracy :\n",
            "Epoch : 1 Batch : 420 Loss : 0.20075581967830658 Accuracy :\n",
            "Epoch : 1 Batch : 430 Loss : 0.20496731996536255 Accuracy :\n",
            "Epoch : 1 Batch : 440 Loss : 0.21542996168136597 Accuracy :\n",
            "Epoch : 1 Batch : 450 Loss : 0.2257099151611328 Accuracy :\n",
            "Epoch : 1 Batch : 460 Loss : 0.09811930358409882 Accuracy :\n",
            "Epoch : 1 Batch : 470 Loss : 0.40445205569267273 Accuracy :\n",
            "Epoch : 1 Batch : 480 Loss : 0.09155036509037018 Accuracy :\n",
            "Epoch : 1 Batch : 490 Loss : 0.14856305718421936 Accuracy :\n",
            "Epoch : 1 Batch : 500 Loss : 0.1585862934589386 Accuracy :\n",
            "Epoch : 1 Batch : 510 Loss : 0.014708605594933033 Accuracy :\n",
            "Epoch : 1 Batch : 520 Loss : 0.037061907351017 Accuracy :\n",
            "Epoch : 1 Batch : 530 Loss : 0.06300652027130127 Accuracy :\n",
            "Epoch : 1 Batch : 540 Loss : 0.05586494877934456 Accuracy :\n",
            "Epoch : 1 Batch : 550 Loss : 0.11519000679254532 Accuracy :\n",
            "Epoch : 1 Batch : 560 Loss : 0.013036479242146015 Accuracy :\n",
            "Epoch : 1 Batch : 570 Loss : 0.28386518359184265 Accuracy :\n",
            "Epoch : 1 Batch : 580 Loss : 0.2832038998603821 Accuracy :\n",
            "Epoch : 1 Batch : 590 Loss : 0.15247495472431183 Accuracy :\n",
            "Epoch : 1 Batch : 600 Loss : 0.23591160774230957 Accuracy :\n",
            "Epoch : 1 Batch : 610 Loss : 0.21325568854808807 Accuracy :\n",
            "Epoch : 1 Batch : 620 Loss : 0.12881721556186676 Accuracy :\n",
            "Epoch : 1 Batch : 630 Loss : 0.09680701047182083 Accuracy :\n",
            "Epoch : 1 Batch : 640 Loss : 0.031417153775691986 Accuracy :\n",
            "Epoch : 1 Batch : 650 Loss : 0.21928910911083221 Accuracy :\n",
            "Epoch : 1 Batch : 660 Loss : 0.1933678686618805 Accuracy :\n",
            "Epoch : 1 Batch : 670 Loss : 0.4037158489227295 Accuracy :\n",
            "Epoch : 1 Batch : 680 Loss : 0.08923569321632385 Accuracy :\n",
            "Epoch : 1 Batch : 690 Loss : 0.06752035021781921 Accuracy :\n",
            "Epoch : 1 Batch : 700 Loss : 0.29195210337638855 Accuracy :\n",
            "Epoch : 1 Batch : 710 Loss : 0.03618301823735237 Accuracy :\n",
            "Epoch : 1 Batch : 720 Loss : 0.16788537800312042 Accuracy :\n",
            "Epoch : 1 Batch : 730 Loss : 0.1102658286690712 Accuracy :\n",
            "Epoch : 1 Batch : 740 Loss : 0.09040190279483795 Accuracy :\n",
            "Epoch : 1 Batch : 750 Loss : 0.03444894403219223 Accuracy :\n",
            "Epoch : 1 Batch : 760 Loss : 0.1324358731508255 Accuracy :\n",
            "Epoch : 1 Batch : 770 Loss : 0.04684531316161156 Accuracy :\n",
            "Epoch : 1 Batch : 780 Loss : 0.3407517969608307 Accuracy :\n",
            "Epoch : 1 Batch : 790 Loss : 0.18369901180267334 Accuracy :\n",
            "Epoch : 1 Batch : 800 Loss : 0.026554599404335022 Accuracy :\n",
            "Epoch : 1 Batch : 810 Loss : 0.0239730104804039 Accuracy :\n",
            "Epoch : 1 Batch : 820 Loss : 0.07058856636285782 Accuracy :\n",
            "Epoch : 1 Batch : 830 Loss : 0.062308333814144135 Accuracy :\n",
            "Epoch : 1 Batch : 840 Loss : 0.03969687968492508 Accuracy :\n",
            "Epoch : 1 Batch : 850 Loss : 0.06512392312288284 Accuracy :\n",
            "Epoch : 1 Batch : 860 Loss : 0.1031632125377655 Accuracy :\n",
            "Epoch : 1 Batch : 870 Loss : 0.02428087405860424 Accuracy :\n",
            "Epoch : 1 Batch : 880 Loss : 0.11643324047327042 Accuracy :\n",
            "Epoch : 1 Batch : 890 Loss : 0.06081371754407883 Accuracy :\n",
            "Epoch : 1 Batch : 900 Loss : 0.04909224808216095 Accuracy :\n",
            "Epoch : 1 Batch : 910 Loss : 0.004019716288894415 Accuracy :\n",
            "Epoch : 1 Batch : 920 Loss : 0.08532004803419113 Accuracy :\n",
            "Epoch : 1 Batch : 930 Loss : 0.014887486584484577 Accuracy :\n",
            "Epoch : 2 Batch : 10 Loss : 0.22820132970809937 Accuracy :\n",
            "Epoch : 2 Batch : 20 Loss : 0.05885901674628258 Accuracy :\n",
            "Epoch : 2 Batch : 30 Loss : 0.03487400710582733 Accuracy :\n",
            "Epoch : 2 Batch : 40 Loss : 0.014780647121369839 Accuracy :\n",
            "Epoch : 2 Batch : 50 Loss : 0.024599941447377205 Accuracy :\n",
            "Epoch : 2 Batch : 60 Loss : 0.2217816710472107 Accuracy :\n",
            "Epoch : 2 Batch : 70 Loss : 0.2442954182624817 Accuracy :\n",
            "Epoch : 2 Batch : 80 Loss : 0.18817324936389923 Accuracy :\n",
            "Epoch : 2 Batch : 90 Loss : 0.054914865642786026 Accuracy :\n",
            "Epoch : 2 Batch : 100 Loss : 0.16507117450237274 Accuracy :\n",
            "Epoch : 2 Batch : 110 Loss : 0.2850266993045807 Accuracy :\n",
            "Epoch : 2 Batch : 120 Loss : 0.062479905784130096 Accuracy :\n",
            "Epoch : 2 Batch : 130 Loss : 0.07898671180009842 Accuracy :\n",
            "Epoch : 2 Batch : 140 Loss : 0.07306940108537674 Accuracy :\n",
            "Epoch : 2 Batch : 150 Loss : 0.0922231525182724 Accuracy :\n",
            "Epoch : 2 Batch : 160 Loss : 0.11513455212116241 Accuracy :\n",
            "Epoch : 2 Batch : 170 Loss : 0.06773140281438828 Accuracy :\n",
            "Epoch : 2 Batch : 180 Loss : 0.01006872858852148 Accuracy :\n",
            "Epoch : 2 Batch : 190 Loss : 0.07100989669561386 Accuracy :\n",
            "Epoch : 2 Batch : 200 Loss : 0.10071797668933868 Accuracy :\n",
            "Epoch : 2 Batch : 210 Loss : 0.1169486939907074 Accuracy :\n",
            "Epoch : 2 Batch : 220 Loss : 0.2979089319705963 Accuracy :\n",
            "Epoch : 2 Batch : 230 Loss : 0.10492172837257385 Accuracy :\n",
            "Epoch : 2 Batch : 240 Loss : 0.0280354805290699 Accuracy :\n",
            "Epoch : 2 Batch : 250 Loss : 0.15567533671855927 Accuracy :\n",
            "Epoch : 2 Batch : 260 Loss : 0.06076694279909134 Accuracy :\n",
            "Epoch : 2 Batch : 270 Loss : 0.09802061319351196 Accuracy :\n",
            "Epoch : 2 Batch : 280 Loss : 0.17459337413311005 Accuracy :\n",
            "Epoch : 2 Batch : 290 Loss : 0.11797187477350235 Accuracy :\n",
            "Epoch : 2 Batch : 300 Loss : 0.03267741575837135 Accuracy :\n",
            "Epoch : 2 Batch : 310 Loss : 0.2607254087924957 Accuracy :\n",
            "Epoch : 2 Batch : 320 Loss : 0.006579236127436161 Accuracy :\n",
            "Epoch : 2 Batch : 330 Loss : 0.04168630763888359 Accuracy :\n",
            "Epoch : 2 Batch : 340 Loss : 0.01890871860086918 Accuracy :\n",
            "Epoch : 2 Batch : 350 Loss : 0.012021465227007866 Accuracy :\n",
            "Epoch : 2 Batch : 360 Loss : 0.08011434972286224 Accuracy :\n",
            "Epoch : 2 Batch : 370 Loss : 0.02849743328988552 Accuracy :\n",
            "Epoch : 2 Batch : 380 Loss : 0.04751265421509743 Accuracy :\n",
            "Epoch : 2 Batch : 390 Loss : 0.03206730633974075 Accuracy :\n",
            "Epoch : 2 Batch : 400 Loss : 0.11002343893051147 Accuracy :\n",
            "Epoch : 2 Batch : 410 Loss : 0.22668316960334778 Accuracy :\n",
            "Epoch : 2 Batch : 420 Loss : 0.13478711247444153 Accuracy :\n",
            "Epoch : 2 Batch : 430 Loss : 0.0635228380560875 Accuracy :\n",
            "Epoch : 2 Batch : 440 Loss : 0.05194472521543503 Accuracy :\n",
            "Epoch : 2 Batch : 450 Loss : 0.2966822385787964 Accuracy :\n",
            "Epoch : 2 Batch : 460 Loss : 0.10896285623311996 Accuracy :\n",
            "Epoch : 2 Batch : 470 Loss : 0.1565435528755188 Accuracy :\n",
            "Epoch : 2 Batch : 480 Loss : 0.1401064097881317 Accuracy :\n",
            "Epoch : 2 Batch : 490 Loss : 0.06140513718128204 Accuracy :\n",
            "Epoch : 2 Batch : 500 Loss : 0.0503971204161644 Accuracy :\n",
            "Epoch : 2 Batch : 510 Loss : 0.0017170424107462168 Accuracy :\n",
            "Epoch : 2 Batch : 520 Loss : 0.006878637708723545 Accuracy :\n",
            "Epoch : 2 Batch : 530 Loss : 0.0586404986679554 Accuracy :\n",
            "Epoch : 2 Batch : 540 Loss : 0.160431370139122 Accuracy :\n",
            "Epoch : 2 Batch : 550 Loss : 0.027621807530522346 Accuracy :\n",
            "Epoch : 2 Batch : 560 Loss : 0.0057730344124138355 Accuracy :\n",
            "Epoch : 2 Batch : 570 Loss : 0.10125038027763367 Accuracy :\n",
            "Epoch : 2 Batch : 580 Loss : 0.11982491612434387 Accuracy :\n",
            "Epoch : 2 Batch : 590 Loss : 0.10657143592834473 Accuracy :\n",
            "Epoch : 2 Batch : 600 Loss : 0.1019749790430069 Accuracy :\n",
            "Epoch : 2 Batch : 610 Loss : 0.08706706762313843 Accuracy :\n",
            "Epoch : 2 Batch : 620 Loss : 0.19018320739269257 Accuracy :\n",
            "Epoch : 2 Batch : 630 Loss : 0.12676909565925598 Accuracy :\n",
            "Epoch : 2 Batch : 640 Loss : 0.010441655293107033 Accuracy :\n",
            "Epoch : 2 Batch : 650 Loss : 0.07840492576360703 Accuracy :\n",
            "Epoch : 2 Batch : 660 Loss : 0.11419860273599625 Accuracy :\n",
            "Epoch : 2 Batch : 670 Loss : 0.18187130987644196 Accuracy :\n",
            "Epoch : 2 Batch : 680 Loss : 0.04982384666800499 Accuracy :\n",
            "Epoch : 2 Batch : 690 Loss : 0.054673220962285995 Accuracy :\n",
            "Epoch : 2 Batch : 700 Loss : 0.07119938731193542 Accuracy :\n",
            "Epoch : 2 Batch : 710 Loss : 0.10617070645093918 Accuracy :\n",
            "Epoch : 2 Batch : 720 Loss : 0.05849064886569977 Accuracy :\n",
            "Epoch : 2 Batch : 730 Loss : 0.009983988478779793 Accuracy :\n",
            "Epoch : 2 Batch : 740 Loss : 0.05904912203550339 Accuracy :\n",
            "Epoch : 2 Batch : 750 Loss : 0.031870219856500626 Accuracy :\n",
            "Epoch : 2 Batch : 760 Loss : 0.003432102035731077 Accuracy :\n",
            "Epoch : 2 Batch : 770 Loss : 0.20233960449695587 Accuracy :\n",
            "Epoch : 2 Batch : 780 Loss : 0.18027079105377197 Accuracy :\n",
            "Epoch : 2 Batch : 790 Loss : 0.062164608389139175 Accuracy :\n",
            "Epoch : 2 Batch : 800 Loss : 0.028086671605706215 Accuracy :\n",
            "Epoch : 2 Batch : 810 Loss : 0.0037206988781690598 Accuracy :\n",
            "Epoch : 2 Batch : 820 Loss : 0.1205483078956604 Accuracy :\n",
            "Epoch : 2 Batch : 830 Loss : 0.03793700784444809 Accuracy :\n",
            "Epoch : 2 Batch : 840 Loss : 0.030717255547642708 Accuracy :\n",
            "Epoch : 2 Batch : 850 Loss : 0.014612337574362755 Accuracy :\n",
            "Epoch : 2 Batch : 860 Loss : 0.005222982261329889 Accuracy :\n",
            "Epoch : 2 Batch : 870 Loss : 0.0015783135313540697 Accuracy :\n",
            "Epoch : 2 Batch : 880 Loss : 0.009102960117161274 Accuracy :\n",
            "Epoch : 2 Batch : 890 Loss : 0.009636255912482738 Accuracy :\n",
            "Epoch : 2 Batch : 900 Loss : 0.06693533062934875 Accuracy :\n",
            "Epoch : 2 Batch : 910 Loss : 0.0010891362326219678 Accuracy :\n",
            "Epoch : 2 Batch : 920 Loss : 0.03068622015416622 Accuracy :\n",
            "Epoch : 2 Batch : 930 Loss : 0.028054019436240196 Accuracy :\n",
            "Epoch : 3 Batch : 10 Loss : 0.12756074965000153 Accuracy :\n",
            "Epoch : 3 Batch : 20 Loss : 0.09184995293617249 Accuracy :\n",
            "Epoch : 3 Batch : 30 Loss : 0.016249144449830055 Accuracy :\n",
            "Epoch : 3 Batch : 40 Loss : 0.03162398934364319 Accuracy :\n",
            "Epoch : 3 Batch : 50 Loss : 0.010541117750108242 Accuracy :\n",
            "Epoch : 3 Batch : 60 Loss : 0.02170339785516262 Accuracy :\n",
            "Epoch : 3 Batch : 70 Loss : 0.26307305693626404 Accuracy :\n",
            "Epoch : 3 Batch : 80 Loss : 0.13300752639770508 Accuracy :\n",
            "Epoch : 3 Batch : 90 Loss : 0.10204112529754639 Accuracy :\n",
            "Epoch : 3 Batch : 100 Loss : 0.1279984414577484 Accuracy :\n",
            "Epoch : 3 Batch : 110 Loss : 0.09324008971452713 Accuracy :\n",
            "Epoch : 3 Batch : 120 Loss : 0.053052596747875214 Accuracy :\n",
            "Epoch : 3 Batch : 130 Loss : 0.10606575012207031 Accuracy :\n",
            "Epoch : 3 Batch : 140 Loss : 0.018325209617614746 Accuracy :\n",
            "Epoch : 3 Batch : 150 Loss : 0.11965440958738327 Accuracy :\n",
            "Epoch : 3 Batch : 160 Loss : 0.09981977194547653 Accuracy :\n",
            "Epoch : 3 Batch : 170 Loss : 0.035128865391016006 Accuracy :\n",
            "Epoch : 3 Batch : 180 Loss : 0.12111590802669525 Accuracy :\n",
            "Epoch : 3 Batch : 190 Loss : 0.003118248423561454 Accuracy :\n",
            "Epoch : 3 Batch : 200 Loss : 0.025242479518055916 Accuracy :\n",
            "Epoch : 3 Batch : 210 Loss : 0.027887841686606407 Accuracy :\n",
            "Epoch : 3 Batch : 220 Loss : 0.07050873339176178 Accuracy :\n",
            "Epoch : 3 Batch : 230 Loss : 0.12761156260967255 Accuracy :\n",
            "Epoch : 3 Batch : 240 Loss : 0.001930472906678915 Accuracy :\n",
            "Epoch : 3 Batch : 250 Loss : 0.25176191329956055 Accuracy :\n",
            "Epoch : 3 Batch : 260 Loss : 0.005381218623369932 Accuracy :\n",
            "Epoch : 3 Batch : 270 Loss : 0.027508296072483063 Accuracy :\n",
            "Epoch : 3 Batch : 280 Loss : 0.10312779992818832 Accuracy :\n",
            "Epoch : 3 Batch : 290 Loss : 0.03975069895386696 Accuracy :\n",
            "Epoch : 3 Batch : 300 Loss : 0.09134484082460403 Accuracy :\n",
            "Epoch : 3 Batch : 310 Loss : 0.028698505833745003 Accuracy :\n",
            "Epoch : 3 Batch : 320 Loss : 0.007460776716470718 Accuracy :\n",
            "Epoch : 3 Batch : 330 Loss : 0.1221722960472107 Accuracy :\n",
            "Epoch : 3 Batch : 340 Loss : 0.060235071927309036 Accuracy :\n",
            "Epoch : 3 Batch : 350 Loss : 0.013405154459178448 Accuracy :\n",
            "Epoch : 3 Batch : 360 Loss : 0.01858249306678772 Accuracy :\n",
            "Epoch : 3 Batch : 370 Loss : 0.04595814272761345 Accuracy :\n",
            "Epoch : 3 Batch : 380 Loss : 0.10038315504789352 Accuracy :\n",
            "Epoch : 3 Batch : 390 Loss : 0.03597673028707504 Accuracy :\n",
            "Epoch : 3 Batch : 400 Loss : 0.1529577672481537 Accuracy :\n",
            "Epoch : 3 Batch : 410 Loss : 0.04728119447827339 Accuracy :\n",
            "Epoch : 3 Batch : 420 Loss : 0.14620377123355865 Accuracy :\n",
            "Epoch : 3 Batch : 430 Loss : 0.04876413196325302 Accuracy :\n",
            "Epoch : 3 Batch : 440 Loss : 0.03805726021528244 Accuracy :\n",
            "Epoch : 3 Batch : 450 Loss : 0.09520546346902847 Accuracy :\n",
            "Epoch : 3 Batch : 460 Loss : 0.051907122135162354 Accuracy :\n",
            "Epoch : 3 Batch : 470 Loss : 0.10203517973423004 Accuracy :\n",
            "Epoch : 3 Batch : 480 Loss : 0.1918022632598877 Accuracy :\n",
            "Epoch : 3 Batch : 490 Loss : 0.07453171908855438 Accuracy :\n",
            "Epoch : 3 Batch : 500 Loss : 0.06458757072687149 Accuracy :\n",
            "Epoch : 3 Batch : 510 Loss : 0.006882573012262583 Accuracy :\n",
            "Epoch : 3 Batch : 520 Loss : 0.017045248299837112 Accuracy :\n",
            "Epoch : 3 Batch : 530 Loss : 0.002737881150096655 Accuracy :\n",
            "Epoch : 3 Batch : 540 Loss : 0.03710375353693962 Accuracy :\n",
            "Epoch : 3 Batch : 550 Loss : 0.05456565320491791 Accuracy :\n",
            "Epoch : 3 Batch : 560 Loss : 0.009549981914460659 Accuracy :\n",
            "Epoch : 3 Batch : 570 Loss : 0.12927550077438354 Accuracy :\n",
            "Epoch : 3 Batch : 580 Loss : 0.10352269560098648 Accuracy :\n",
            "Epoch : 3 Batch : 590 Loss : 0.11084525287151337 Accuracy :\n",
            "Epoch : 3 Batch : 600 Loss : 0.1245543584227562 Accuracy :\n",
            "Epoch : 3 Batch : 610 Loss : 0.05375485494732857 Accuracy :\n",
            "Epoch : 3 Batch : 620 Loss : 0.03258567675948143 Accuracy :\n",
            "Epoch : 3 Batch : 630 Loss : 0.13360722362995148 Accuracy :\n",
            "Epoch : 3 Batch : 640 Loss : 0.012382556684315205 Accuracy :\n",
            "Epoch : 3 Batch : 650 Loss : 0.051253899931907654 Accuracy :\n",
            "Epoch : 3 Batch : 660 Loss : 0.10289184004068375 Accuracy :\n",
            "Epoch : 3 Batch : 670 Loss : 0.2981938123703003 Accuracy :\n",
            "Epoch : 3 Batch : 680 Loss : 0.014086929149925709 Accuracy :\n",
            "Epoch : 3 Batch : 690 Loss : 0.03381146490573883 Accuracy :\n",
            "Epoch : 3 Batch : 700 Loss : 0.06326654553413391 Accuracy :\n",
            "Epoch : 3 Batch : 710 Loss : 0.124776691198349 Accuracy :\n",
            "Epoch : 3 Batch : 720 Loss : 0.07212339341640472 Accuracy :\n",
            "Epoch : 3 Batch : 730 Loss : 0.004122326150536537 Accuracy :\n",
            "Epoch : 3 Batch : 740 Loss : 0.17035818099975586 Accuracy :\n",
            "Epoch : 3 Batch : 750 Loss : 0.07493903487920761 Accuracy :\n",
            "Epoch : 3 Batch : 760 Loss : 0.12760400772094727 Accuracy :\n",
            "Epoch : 3 Batch : 770 Loss : 0.004077642224729061 Accuracy :\n",
            "Epoch : 3 Batch : 780 Loss : 0.11787297576665878 Accuracy :\n",
            "Epoch : 3 Batch : 790 Loss : 0.19052720069885254 Accuracy :\n",
            "Epoch : 3 Batch : 800 Loss : 0.036958444863557816 Accuracy :\n",
            "Epoch : 3 Batch : 810 Loss : 0.008414977230131626 Accuracy :\n",
            "Epoch : 3 Batch : 820 Loss : 0.07934844493865967 Accuracy :\n",
            "Epoch : 3 Batch : 830 Loss : 0.01888907700777054 Accuracy :\n",
            "Epoch : 3 Batch : 840 Loss : 0.11321105062961578 Accuracy :\n",
            "Epoch : 3 Batch : 850 Loss : 0.014793386682868004 Accuracy :\n",
            "Epoch : 3 Batch : 860 Loss : 0.008215733803808689 Accuracy :\n",
            "Epoch : 3 Batch : 870 Loss : 0.002137620933353901 Accuracy :\n",
            "Epoch : 3 Batch : 880 Loss : 0.08919326215982437 Accuracy :\n",
            "Epoch : 3 Batch : 890 Loss : 0.0005002862308174372 Accuracy :\n",
            "Epoch : 3 Batch : 900 Loss : 0.08999252319335938 Accuracy :\n",
            "Epoch : 3 Batch : 910 Loss : 0.007907656952738762 Accuracy :\n",
            "Epoch : 3 Batch : 920 Loss : 0.056502748280763626 Accuracy :\n",
            "Epoch : 3 Batch : 930 Loss : 0.029040617868304253 Accuracy :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysjg-GiluSiN"
      },
      "source": [
        "with torch.no_grad():\n",
        "  outs = mycnn(torch.unsqueeze(train_data[0][0], 0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1gw4MUx2gKf",
        "outputId": "3420857b-b517-43f9-b9ae-95606235a0f6"
      },
      "source": [
        "torch.max(outs, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([17.3813]), indices=tensor([5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luto-IeB2ui7"
      },
      "source": [
        "outss = outs.argmax()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8pKvWYq3LTQ",
        "outputId": "a7a225c3-c85e-499b-8089-f4e47b246a1e"
      },
      "source": [
        "if outss== 5:\n",
        "  print('hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9nFgL2M595C",
        "outputId": "206c8f19-7639-4b14-9be3-8aa169b888bd"
      },
      "source": [
        "outss.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB92HWk36s4J"
      },
      "source": [
        "def compute_gradient(func, inp, **kwargs):\n",
        "    inp.requires_grad = True\n",
        "\n",
        "    loss = func(inp, **kwargs)\n",
        "    loss.backward()\n",
        "\n",
        "    inp.requires_grad = False\n",
        "\n",
        "    return inp.grad.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2QZLJcs6_q4"
      },
      "source": [
        "def func(inp, net=None, target=None):\n",
        "    out = net(inp)\n",
        "    loss = torch.nn.functional.nll_loss(out, target=torch.LongTensor([target]))\n",
        "\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT97z41v7Mah"
      },
      "source": [
        "def attack(tensor, net, eps=1e-3, n_iter=50):\n",
        "    new_tensor = tensor.detach().clone()\n",
        "\n",
        "    orig_prediction = net(tensor).argmax()\n",
        "\n",
        "    # print(f\"Original prediction: {orig_prediction.item()}\")\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        net.zero_grad()\n",
        "\n",
        "        grad = compute_gradient(\n",
        "                func, new_tensor, net=net, target=orig_prediction.item()\n",
        "                )\n",
        "        new_tensor = torch.clamp(new_tensor + eps * grad.sign(), -2, 2)\n",
        "        new_prediction = net(new_tensor).argmax()\n",
        "\n",
        "        if orig_prediction != new_prediction:\n",
        "            # print(f\"We fooled the network after {i} iterations!\")\n",
        "            # print(f\"New prediction: {new_prediction.item()}\")\n",
        "            break\n",
        "\n",
        "    return new_tensor, orig_prediction.item(), new_prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bYJFSNz7mrI"
      },
      "source": [
        "tensor = torch.unsqueeze(train_data[0][0], 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8KxxGI_7Zwy",
        "outputId": "1f8c1906-1ed6-4881-99dc-7189117bda91"
      },
      "source": [
        "new_tensor, orig_prediction, new_prediction = attack(\n",
        "            tensor, mycnn, eps=1e-3, n_iter=100\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original prediction: 5\n",
            "Loss: -17.381298065185547\n",
            "Loss: -16.980836868286133\n",
            "Loss: -16.577857971191406\n",
            "Loss: -16.163299560546875\n",
            "Loss: -15.75125789642334\n",
            "Loss: -15.325814247131348\n",
            "Loss: -14.89489459991455\n",
            "Loss: -14.463152885437012\n",
            "Loss: -14.025286674499512\n",
            "Loss: -13.60576343536377\n",
            "Loss: -13.186068534851074\n",
            "Loss: -12.835947036743164\n",
            "Loss: -12.487896919250488\n",
            "Loss: -12.139602661132812\n",
            "Loss: -11.793044090270996\n",
            "Loss: -11.443602561950684\n",
            "Loss: -11.094082832336426\n",
            "Loss: -10.746077537536621\n",
            "Loss: -10.402241706848145\n",
            "Loss: -10.115830421447754\n",
            "Loss: -9.833733558654785\n",
            "Loss: -9.538674354553223\n",
            "Loss: -9.249228477478027\n",
            "Loss: -8.99846363067627\n",
            "Loss: -8.769976615905762\n",
            "Loss: -8.551944732666016\n",
            "Loss: -8.322795867919922\n",
            "Loss: -8.100737571716309\n",
            "Loss: -7.869178771972656\n",
            "Loss: -7.669247627258301\n",
            "Loss: -7.445487022399902\n",
            "Loss: -7.238202095031738\n",
            "Loss: -7.029003143310547\n",
            "Loss: -6.813549518585205\n",
            "Loss: -6.599337100982666\n",
            "Loss: -6.3832783699035645\n",
            "Loss: -6.169867038726807\n",
            "Loss: -5.956784725189209\n",
            "Loss: -5.743082046508789\n",
            "Loss: -5.570262908935547\n",
            "Loss: -5.426980495452881\n",
            "Loss: -5.2531514167785645\n",
            "Loss: -5.140127658843994\n",
            "Loss: -4.979292869567871\n",
            "Loss: -4.854869365692139\n",
            "Loss: -4.705893039703369\n",
            "Loss: -4.577857971191406\n",
            "Loss: -4.431527137756348\n",
            "Loss: -4.303748607635498\n",
            "Loss: -4.151825428009033\n",
            "Loss: -4.021365165710449\n",
            "Loss: -3.8769490718841553\n",
            "Loss: -3.7612974643707275\n",
            "Loss: -3.6153182983398438\n",
            "Loss: -3.497767686843872\n",
            "Loss: -3.3617660999298096\n",
            "Loss: -3.244910717010498\n",
            "Loss: -3.1311092376708984\n",
            "We fooled the network after 57 iterations!\n",
            "New prediction: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yoZscuTg7zpZ",
        "outputId": "47835be4-13ea-48eb-f065-b6ce4e64b50d"
      },
      "source": [
        "plt.imshow(new_tensor[0][0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7effb8d0ba10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "J5Jj214j9XRW",
        "outputId": "6dd58ddb-1385-4f21-e38a-7836f46d0cd5"
      },
      "source": [
        "plt.savefig(\"res_1.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPQfz_-8-SCK"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcNiaaLu_DLv"
      },
      "source": [
        "filename = '/content/drive/MyDrive/adver_data/bad/img_1.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0RgG23S_HVc",
        "outputId": "8755a789-487a-421f-97a7-2a1c73c64f1c"
      },
      "source": [
        "cv2.imwrite(filename, new_tensor[0][0].numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU3TwmA9_Kwk",
        "outputId": "9db833d9-0622-46d7-b9a1-3652414d9f62"
      },
      "source": [
        "n = 0\n",
        "for i in range(15000):\n",
        "  if train_data[i][1] == 6:\n",
        "    filename_good = '/content/drive/MyDrive/adver_data/good/good_'+str(n)+'.jpg'\n",
        "    cv2.imwrite(filename_good, train_data[i][0][0].numpy())\n",
        "\n",
        "    tensor = torch.unsqueeze(train_data[i][0], 0)\n",
        "\n",
        "    new_tensor, orig_prediction, new_prediction = attack(\n",
        "            tensor, mycnn, eps=1e-3, n_iter=100\n",
        "            )\n",
        "    filename_bad = '/content/drive/MyDrive/adver_data/bad/bad_'+str(n)+'.jpg'\n",
        "    cv2.imwrite(filename_bad, new_tensor[0][0].numpy())\n",
        "\n",
        "    n += 1\n",
        "    print(str(i)+'img is appending...')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "14155img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -35.38043212890625\n",
            "Loss: -34.85356903076172\n",
            "Loss: -34.29658126831055\n",
            "Loss: -33.74332809448242\n",
            "Loss: -33.18679428100586\n",
            "Loss: -32.64364242553711\n",
            "Loss: -32.11105728149414\n",
            "Loss: -31.59322738647461\n",
            "Loss: -31.075746536254883\n",
            "Loss: -30.562971115112305\n",
            "Loss: -30.06368064880371\n",
            "Loss: -29.547983169555664\n",
            "Loss: -29.03870964050293\n",
            "Loss: -28.574302673339844\n",
            "Loss: -28.10685157775879\n",
            "Loss: -27.709693908691406\n",
            "Loss: -27.297399520874023\n",
            "Loss: -26.89124870300293\n",
            "Loss: -26.479280471801758\n",
            "Loss: -26.07241439819336\n",
            "Loss: -25.686016082763672\n",
            "Loss: -25.26536750793457\n",
            "Loss: -24.901742935180664\n",
            "Loss: -24.5255184173584\n",
            "Loss: -24.1585636138916\n",
            "Loss: -23.788253784179688\n",
            "Loss: -23.43366050720215\n",
            "Loss: -23.06428337097168\n",
            "Loss: -22.700645446777344\n",
            "Loss: -22.349933624267578\n",
            "Loss: -21.967988967895508\n",
            "Loss: -21.610992431640625\n",
            "Loss: -21.2520694732666\n",
            "Loss: -20.902990341186523\n",
            "Loss: -20.54776954650879\n",
            "Loss: -20.204160690307617\n",
            "Loss: -19.85124397277832\n",
            "Loss: -19.463035583496094\n",
            "Loss: -19.09256935119629\n",
            "Loss: -18.71108627319336\n",
            "Loss: -18.368820190429688\n",
            "Loss: -17.989038467407227\n",
            "Loss: -17.643943786621094\n",
            "Loss: -17.298336029052734\n",
            "Loss: -16.94793701171875\n",
            "Loss: -16.604225158691406\n",
            "Loss: -16.23512077331543\n",
            "Loss: -15.9256010055542\n",
            "Loss: -15.568764686584473\n",
            "Loss: -15.21229076385498\n",
            "Loss: -14.824175834655762\n",
            "Loss: -14.417640686035156\n",
            "Loss: -14.025912284851074\n",
            "Loss: -13.628592491149902\n",
            "Loss: -13.25513744354248\n",
            "Loss: -12.91396713256836\n",
            "Loss: -12.60679817199707\n",
            "Loss: -12.291586875915527\n",
            "Loss: -12.008859634399414\n",
            "Loss: -11.709869384765625\n",
            "Loss: -11.410243034362793\n",
            "Loss: -11.107434272766113\n",
            "Loss: -10.793667793273926\n",
            "Loss: -10.500283241271973\n",
            "Loss: -10.203730583190918\n",
            "Loss: -9.899706840515137\n",
            "Loss: -9.595063209533691\n",
            "Loss: -9.292601585388184\n",
            "Loss: -9.003028869628906\n",
            "Loss: -8.699670791625977\n",
            "Loss: -8.392367362976074\n",
            "Loss: -8.096858978271484\n",
            "Loss: -7.828790187835693\n",
            "Loss: -7.511274814605713\n",
            "Loss: -7.228493690490723\n",
            "Loss: -6.9176225662231445\n",
            "Loss: -6.625943183898926\n",
            "Loss: -6.332526206970215\n",
            "Loss: -6.060338020324707\n",
            "Loss: -5.747652530670166\n",
            "Loss: -5.463357448577881\n",
            "Loss: -5.174015522003174\n",
            "Loss: -4.915100574493408\n",
            "Loss: -4.65338134765625\n",
            "Loss: -4.36667537689209\n",
            "Loss: -4.110901355743408\n",
            "Loss: -3.858447790145874\n",
            "Loss: -3.6091020107269287\n",
            "Loss: -3.364049196243286\n",
            "Loss: -3.118844747543335\n",
            "Loss: -2.8695857524871826\n",
            "Loss: -2.6612842082977295\n",
            "Loss: -2.4732882976531982\n",
            "Loss: -2.2984988689422607\n",
            "Loss: -2.1169631481170654\n",
            "Loss: -1.924575924873352\n",
            "Loss: -1.709692120552063\n",
            "Loss: -1.464525580406189\n",
            "Loss: -1.233534812927246\n",
            "Loss: -0.957697331905365\n",
            "14159img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -42.690834045410156\n",
            "Loss: -42.01003646850586\n",
            "Loss: -41.32571792602539\n",
            "Loss: -40.63399124145508\n",
            "Loss: -39.94789123535156\n",
            "Loss: -39.264686584472656\n",
            "Loss: -38.58049774169922\n",
            "Loss: -37.901912689208984\n",
            "Loss: -37.21427536010742\n",
            "Loss: -36.53544235229492\n",
            "Loss: -35.854496002197266\n",
            "Loss: -35.174560546875\n",
            "Loss: -34.50608825683594\n",
            "Loss: -33.854183197021484\n",
            "Loss: -33.207763671875\n",
            "Loss: -32.553340911865234\n",
            "Loss: -31.91740608215332\n",
            "Loss: -31.31175422668457\n",
            "Loss: -30.709827423095703\n",
            "Loss: -30.111310958862305\n",
            "Loss: -29.53719139099121\n",
            "Loss: -28.95203971862793\n",
            "Loss: -28.381250381469727\n",
            "Loss: -27.79030990600586\n",
            "Loss: -27.214065551757812\n",
            "Loss: -26.690580368041992\n",
            "Loss: -26.23240089416504\n",
            "Loss: -25.765670776367188\n",
            "Loss: -25.302122116088867\n",
            "Loss: -24.840410232543945\n",
            "Loss: -24.39129066467285\n",
            "Loss: -23.937532424926758\n",
            "Loss: -23.485166549682617\n",
            "Loss: -23.04783058166504\n",
            "Loss: -22.616132736206055\n",
            "Loss: -22.182626724243164\n",
            "Loss: -21.753042221069336\n",
            "Loss: -21.328340530395508\n",
            "Loss: -20.894521713256836\n",
            "Loss: -20.467199325561523\n",
            "Loss: -20.009065628051758\n",
            "Loss: -19.577024459838867\n",
            "Loss: -19.122655868530273\n",
            "Loss: -18.678483963012695\n",
            "Loss: -18.224563598632812\n",
            "Loss: -17.774633407592773\n",
            "Loss: -17.326980590820312\n",
            "Loss: -16.873395919799805\n",
            "Loss: -16.43886375427246\n",
            "Loss: -16.001224517822266\n",
            "Loss: -15.567065238952637\n",
            "Loss: -15.19848918914795\n",
            "Loss: -14.84959602355957\n",
            "Loss: -14.492568016052246\n",
            "Loss: -14.109137535095215\n",
            "Loss: -13.74143123626709\n",
            "Loss: -13.359557151794434\n",
            "Loss: -12.994197845458984\n",
            "Loss: -12.61552906036377\n",
            "Loss: -12.244200706481934\n",
            "Loss: -11.855203628540039\n",
            "Loss: -11.49787712097168\n",
            "Loss: -11.112162590026855\n",
            "Loss: -10.766730308532715\n",
            "Loss: -10.41236400604248\n",
            "Loss: -10.041661262512207\n",
            "Loss: -9.679492950439453\n",
            "Loss: -9.339920997619629\n",
            "Loss: -8.984254837036133\n",
            "Loss: -8.630609512329102\n",
            "Loss: -8.2922945022583\n",
            "Loss: -7.919735908508301\n",
            "Loss: -7.56919002532959\n",
            "Loss: -7.208966255187988\n",
            "Loss: -6.867711067199707\n",
            "Loss: -6.501377582550049\n",
            "Loss: -6.160068511962891\n",
            "Loss: -5.806155681610107\n",
            "Loss: -5.44158411026001\n",
            "Loss: -5.1452412605285645\n",
            "Loss: -4.86264705657959\n",
            "Loss: -4.573630332946777\n",
            "Loss: -4.31005859375\n",
            "Loss: -4.036319255828857\n",
            "Loss: -3.790830373764038\n",
            "Loss: -3.5304293632507324\n",
            "Loss: -3.283733606338501\n",
            "Loss: -2.977067232131958\n",
            "Loss: -2.6920275688171387\n",
            "Loss: -2.4028775691986084\n",
            "We fooled the network after 89 iterations!\n",
            "New prediction: 5\n",
            "14167img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -15.325526237487793\n",
            "Loss: -14.904535293579102\n",
            "Loss: -14.453984260559082\n",
            "Loss: -14.011839866638184\n",
            "Loss: -13.571961402893066\n",
            "Loss: -13.138104438781738\n",
            "Loss: -12.708566665649414\n",
            "Loss: -12.259540557861328\n",
            "Loss: -11.83093547821045\n",
            "Loss: -11.40122127532959\n",
            "Loss: -10.975776672363281\n",
            "Loss: -10.533146858215332\n",
            "Loss: -10.095209121704102\n",
            "Loss: -9.678132057189941\n",
            "Loss: -9.299382209777832\n",
            "Loss: -8.92345142364502\n",
            "Loss: -8.551376342773438\n",
            "Loss: -8.164369583129883\n",
            "Loss: -7.797586917877197\n",
            "Loss: -7.438425064086914\n",
            "Loss: -7.088901996612549\n",
            "Loss: -6.746878623962402\n",
            "Loss: -6.441304683685303\n",
            "Loss: -6.051919937133789\n",
            "Loss: -5.643366813659668\n",
            "Loss: -5.175414562225342\n",
            "Loss: -4.629718780517578\n",
            "Loss: -4.081037521362305\n",
            "We fooled the network after 27 iterations!\n",
            "New prediction: 5\n",
            "14172img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -40.26356506347656\n",
            "Loss: -39.70355224609375\n",
            "Loss: -39.13058853149414\n",
            "Loss: -38.547359466552734\n",
            "Loss: -37.96260452270508\n",
            "Loss: -37.38197708129883\n",
            "Loss: -36.80634307861328\n",
            "Loss: -36.2215576171875\n",
            "Loss: -35.63877487182617\n",
            "Loss: -35.0676155090332\n",
            "Loss: -34.484771728515625\n",
            "Loss: -33.90460205078125\n",
            "Loss: -33.326629638671875\n",
            "Loss: -32.7457160949707\n",
            "Loss: -32.16203308105469\n",
            "Loss: -31.60908317565918\n",
            "Loss: -31.045564651489258\n",
            "Loss: -30.487024307250977\n",
            "Loss: -29.924880981445312\n",
            "Loss: -29.369903564453125\n",
            "Loss: -28.817279815673828\n",
            "Loss: -28.269332885742188\n",
            "Loss: -27.719186782836914\n",
            "Loss: -27.195268630981445\n",
            "Loss: -26.766145706176758\n",
            "Loss: -26.321809768676758\n",
            "Loss: -25.8903751373291\n",
            "Loss: -25.447328567504883\n",
            "Loss: -25.0107364654541\n",
            "Loss: -24.586517333984375\n",
            "Loss: -24.153512954711914\n",
            "Loss: -23.72245979309082\n",
            "Loss: -23.29799461364746\n",
            "Loss: -22.881399154663086\n",
            "Loss: -22.452014923095703\n",
            "Loss: -22.01712989807129\n",
            "Loss: -21.594776153564453\n",
            "Loss: -21.15297508239746\n",
            "Loss: -20.712377548217773\n",
            "Loss: -20.27557373046875\n",
            "Loss: -19.839813232421875\n",
            "Loss: -19.427383422851562\n",
            "Loss: -19.01324462890625\n",
            "Loss: -18.61823844909668\n",
            "Loss: -18.218740463256836\n",
            "Loss: -17.82242774963379\n",
            "Loss: -17.4124698638916\n",
            "Loss: -16.97142219543457\n",
            "Loss: -16.485572814941406\n",
            "Loss: -16.07134437561035\n",
            "Loss: -15.602776527404785\n",
            "Loss: -15.174490928649902\n",
            "Loss: -14.72071647644043\n",
            "Loss: -14.295973777770996\n",
            "Loss: -13.811932563781738\n",
            "Loss: -13.339255332946777\n",
            "Loss: -12.878706932067871\n",
            "Loss: -12.42905044555664\n",
            "Loss: -11.982165336608887\n",
            "Loss: -11.573403358459473\n",
            "Loss: -11.242886543273926\n",
            "Loss: -10.894471168518066\n",
            "Loss: -10.561025619506836\n",
            "Loss: -10.099230766296387\n",
            "Loss: -9.674927711486816\n",
            "Loss: -9.220805168151855\n",
            "Loss: -8.79434585571289\n",
            "Loss: -8.349074363708496\n",
            "Loss: -7.921123504638672\n",
            "Loss: -7.4979448318481445\n",
            "Loss: -7.098456382751465\n",
            "Loss: -6.729857921600342\n",
            "Loss: -6.346001148223877\n",
            "Loss: -5.721208572387695\n",
            "Loss: -5.184892654418945\n",
            "Loss: -4.570863246917725\n",
            "Loss: -4.014066696166992\n",
            "Loss: -3.495915412902832\n",
            "Loss: -2.911137342453003\n",
            "Loss: -2.324139356613159\n",
            "We fooled the network after 79 iterations!\n",
            "New prediction: 4\n",
            "14178img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -17.405900955200195\n",
            "Loss: -17.02676773071289\n",
            "Loss: -16.640066146850586\n",
            "Loss: -16.26312255859375\n",
            "Loss: -15.890549659729004\n",
            "Loss: -15.517882347106934\n",
            "Loss: -15.145251274108887\n",
            "Loss: -14.77230167388916\n",
            "Loss: -14.40112018585205\n",
            "Loss: -14.03488826751709\n",
            "Loss: -13.678545951843262\n",
            "Loss: -13.376673698425293\n",
            "Loss: -13.10728645324707\n",
            "Loss: -12.840182304382324\n",
            "Loss: -12.581450462341309\n",
            "Loss: -12.325861930847168\n",
            "Loss: -12.070862770080566\n",
            "Loss: -11.816729545593262\n",
            "Loss: -11.563408851623535\n",
            "Loss: -11.31952953338623\n",
            "Loss: -11.068501472473145\n",
            "Loss: -10.816786766052246\n",
            "Loss: -10.569414138793945\n",
            "Loss: -10.320712089538574\n",
            "Loss: -10.074639320373535\n",
            "Loss: -9.831925392150879\n",
            "Loss: -9.596298217773438\n",
            "Loss: -9.357521057128906\n",
            "Loss: -9.114493370056152\n",
            "Loss: -8.876519203186035\n",
            "Loss: -8.670893669128418\n",
            "Loss: -8.46608829498291\n",
            "Loss: -8.259624481201172\n",
            "Loss: -8.067834854125977\n",
            "Loss: -7.878822326660156\n",
            "Loss: -7.687906265258789\n",
            "Loss: -7.503929615020752\n",
            "Loss: -7.327488899230957\n",
            "Loss: -7.1392903327941895\n",
            "Loss: -6.967833995819092\n",
            "Loss: -6.790085792541504\n",
            "Loss: -6.604459762573242\n",
            "Loss: -6.424055576324463\n",
            "Loss: -6.245603561401367\n",
            "Loss: -6.066706657409668\n",
            "Loss: -5.9056901931762695\n",
            "Loss: -5.728628158569336\n",
            "Loss: -5.553527355194092\n",
            "Loss: -5.392787456512451\n",
            "Loss: -5.205796241760254\n",
            "Loss: -5.030918121337891\n",
            "Loss: -4.84897518157959\n",
            "Loss: -4.594502925872803\n",
            "We fooled the network after 52 iterations!\n",
            "New prediction: 0\n",
            "14186img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -30.40269660949707\n",
            "Loss: -29.8542537689209\n",
            "Loss: -29.28508186340332\n",
            "Loss: -28.716753005981445\n",
            "Loss: -28.145872116088867\n",
            "Loss: -27.588903427124023\n",
            "Loss: -27.030611038208008\n",
            "Loss: -26.47247886657715\n",
            "Loss: -25.912145614624023\n",
            "Loss: -25.295799255371094\n",
            "Loss: -24.65171241760254\n",
            "Loss: -24.053720474243164\n",
            "Loss: -23.387727737426758\n",
            "Loss: -22.758695602416992\n",
            "Loss: -22.122507095336914\n",
            "Loss: -21.481115341186523\n",
            "Loss: -20.84414291381836\n",
            "Loss: -20.213279724121094\n",
            "Loss: -19.569570541381836\n",
            "Loss: -18.97185707092285\n",
            "Loss: -18.39826774597168\n",
            "Loss: -17.794147491455078\n",
            "Loss: -17.192026138305664\n",
            "Loss: -16.625919342041016\n",
            "Loss: -16.132652282714844\n",
            "Loss: -15.643651962280273\n",
            "Loss: -15.152375221252441\n",
            "Loss: -14.670655250549316\n",
            "Loss: -14.173251152038574\n",
            "Loss: -13.696181297302246\n",
            "Loss: -13.219134330749512\n",
            "Loss: -12.739789009094238\n",
            "Loss: -12.266226768493652\n",
            "Loss: -11.820977210998535\n",
            "Loss: -11.380327224731445\n",
            "Loss: -10.945881843566895\n",
            "Loss: -10.547826766967773\n",
            "Loss: -10.199200630187988\n",
            "Loss: -9.856671333312988\n",
            "Loss: -9.511009216308594\n",
            "Loss: -9.165682792663574\n",
            "Loss: -8.845870018005371\n",
            "Loss: -8.495776176452637\n",
            "Loss: -8.187207221984863\n",
            "Loss: -7.849635124206543\n",
            "Loss: -7.517524719238281\n",
            "Loss: -7.171191215515137\n",
            "Loss: -6.842556953430176\n",
            "Loss: -6.529416084289551\n",
            "Loss: -6.197517395019531\n",
            "Loss: -5.821654796600342\n",
            "Loss: -5.338102340698242\n",
            "Loss: -4.883533477783203\n",
            "Loss: -4.365596771240234\n",
            "Loss: -3.879476308822632\n",
            "Loss: -3.35286021232605\n",
            "Loss: -2.907627820968628\n",
            "Loss: -2.440974712371826\n",
            "We fooled the network after 57 iterations!\n",
            "New prediction: 4\n",
            "14190img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -31.6395320892334\n",
            "Loss: -30.95682716369629\n",
            "Loss: -30.240638732910156\n",
            "Loss: -29.528451919555664\n",
            "Loss: -28.814558029174805\n",
            "Loss: -28.10846519470215\n",
            "Loss: -27.405900955200195\n",
            "Loss: -26.68763542175293\n",
            "Loss: -25.997766494750977\n",
            "Loss: -25.302946090698242\n",
            "Loss: -24.61078453063965\n",
            "Loss: -23.93385124206543\n",
            "Loss: -23.277355194091797\n",
            "Loss: -22.60927391052246\n",
            "Loss: -21.958101272583008\n",
            "Loss: -21.291948318481445\n",
            "Loss: -20.744611740112305\n",
            "Loss: -20.209247589111328\n",
            "Loss: -19.6676025390625\n",
            "Loss: -19.148700714111328\n",
            "Loss: -18.621557235717773\n",
            "Loss: -18.107694625854492\n",
            "Loss: -17.593425750732422\n",
            "Loss: -17.11211585998535\n",
            "Loss: -16.63842010498047\n",
            "Loss: -16.153257369995117\n",
            "Loss: -15.679850578308105\n",
            "Loss: -15.193726539611816\n",
            "Loss: -14.708128929138184\n",
            "Loss: -14.21906566619873\n",
            "Loss: -13.738551139831543\n",
            "Loss: -13.243680953979492\n",
            "Loss: -12.779160499572754\n",
            "Loss: -12.315168380737305\n",
            "Loss: -11.853809356689453\n",
            "Loss: -11.467877388000488\n",
            "Loss: -11.068751335144043\n",
            "Loss: -10.694958686828613\n",
            "Loss: -10.31708812713623\n",
            "Loss: -9.9517822265625\n",
            "Loss: -9.570775032043457\n",
            "Loss: -9.200315475463867\n",
            "Loss: -8.864387512207031\n",
            "Loss: -8.522965431213379\n",
            "Loss: -8.199908256530762\n",
            "Loss: -7.955585479736328\n",
            "Loss: -7.694589614868164\n",
            "Loss: -7.455549240112305\n",
            "Loss: -7.192883491516113\n",
            "Loss: -6.944066047668457\n",
            "Loss: -6.694919586181641\n",
            "Loss: -6.438228607177734\n",
            "Loss: -6.2095866203308105\n",
            "Loss: -5.966518878936768\n",
            "Loss: -5.711910247802734\n",
            "Loss: -5.460412979125977\n",
            "Loss: -5.256422996520996\n",
            "Loss: -4.978460788726807\n",
            "Loss: -4.728285312652588\n",
            "Loss: -4.463865280151367\n",
            "Loss: -4.219043731689453\n",
            "Loss: -3.954259157180786\n",
            "Loss: -3.7337403297424316\n",
            "Loss: -3.5068881511688232\n",
            "Loss: -3.288184642791748\n",
            "Loss: -3.0321261882781982\n",
            "Loss: -2.8118858337402344\n",
            "Loss: -2.5594823360443115\n",
            "Loss: -2.3309595584869385\n",
            "Loss: -2.119514226913452\n",
            "Loss: -1.9084943532943726\n",
            "Loss: -1.7147042751312256\n",
            "Loss: -1.5344868898391724\n",
            "Loss: -1.317484974861145\n",
            "Loss: -1.109648585319519\n",
            "Loss: -0.9136776328086853\n",
            "We fooled the network after 75 iterations!\n",
            "New prediction: 0\n",
            "14232img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -30.083980560302734\n",
            "Loss: -29.614694595336914\n",
            "Loss: -29.128366470336914\n",
            "Loss: -28.6303653717041\n",
            "Loss: -28.14715003967285\n",
            "Loss: -27.67864418029785\n",
            "Loss: -27.197919845581055\n",
            "Loss: -26.71734046936035\n",
            "Loss: -26.250137329101562\n",
            "Loss: -25.82060432434082\n",
            "Loss: -25.37565803527832\n",
            "Loss: -24.924592971801758\n",
            "Loss: -24.484066009521484\n",
            "Loss: -24.04075050354004\n",
            "Loss: -23.64153289794922\n",
            "Loss: -23.21114158630371\n",
            "Loss: -22.80903434753418\n",
            "Loss: -22.3987979888916\n",
            "Loss: -22.002946853637695\n",
            "Loss: -21.605730056762695\n",
            "Loss: -21.200613021850586\n",
            "Loss: -20.82015609741211\n",
            "Loss: -20.44001579284668\n",
            "Loss: -20.07805061340332\n",
            "Loss: -19.715627670288086\n",
            "Loss: -19.341611862182617\n",
            "Loss: -18.978435516357422\n",
            "Loss: -18.619691848754883\n",
            "Loss: -18.241483688354492\n",
            "Loss: -17.871299743652344\n",
            "Loss: -17.52077865600586\n",
            "Loss: -17.166305541992188\n",
            "Loss: -16.787534713745117\n",
            "Loss: -16.438791275024414\n",
            "Loss: -16.058988571166992\n",
            "Loss: -15.702078819274902\n",
            "Loss: -15.371113777160645\n",
            "Loss: -15.025670051574707\n",
            "Loss: -14.6779203414917\n",
            "Loss: -14.316554069519043\n",
            "Loss: -13.990049362182617\n",
            "Loss: -13.65404987335205\n",
            "Loss: -13.296500205993652\n",
            "Loss: -12.972203254699707\n",
            "Loss: -12.618468284606934\n",
            "Loss: -12.30114459991455\n",
            "Loss: -11.974291801452637\n",
            "Loss: -11.629376411437988\n",
            "Loss: -11.304217338562012\n",
            "Loss: -10.943825721740723\n",
            "Loss: -10.641526222229004\n",
            "Loss: -10.352950096130371\n",
            "Loss: -10.071016311645508\n",
            "Loss: -9.808566093444824\n",
            "Loss: -9.521806716918945\n",
            "Loss: -9.257939338684082\n",
            "Loss: -9.007014274597168\n",
            "Loss: -8.709118843078613\n",
            "Loss: -8.389945983886719\n",
            "Loss: -8.116803169250488\n",
            "Loss: -7.845380783081055\n",
            "Loss: -7.539709091186523\n",
            "Loss: -7.257722854614258\n",
            "Loss: -7.005402565002441\n",
            "Loss: -6.7161149978637695\n",
            "Loss: -6.429133415222168\n",
            "Loss: -6.1578803062438965\n",
            "Loss: -5.872186660766602\n",
            "Loss: -5.595944881439209\n",
            "Loss: -5.3230695724487305\n",
            "Loss: -4.966058254241943\n",
            "Loss: -4.589461326599121\n",
            "Loss: -4.267362117767334\n",
            "Loss: -3.952789545059204\n",
            "Loss: -3.607994794845581\n",
            "Loss: -3.2789809703826904\n",
            "Loss: -2.9393978118896484\n",
            "Loss: -2.438464403152466\n",
            "Loss: -1.8958913087844849\n",
            "Loss: -1.3734116554260254\n",
            "We fooled the network after 79 iterations!\n",
            "New prediction: 8\n",
            "14237img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -16.53179359436035\n",
            "Loss: -15.905072212219238\n",
            "Loss: -15.255697250366211\n",
            "Loss: -14.604815483093262\n",
            "Loss: -13.980053901672363\n",
            "Loss: -13.342901229858398\n",
            "Loss: -12.773758888244629\n",
            "Loss: -12.207926750183105\n",
            "Loss: -11.636587142944336\n",
            "Loss: -11.099719047546387\n",
            "Loss: -10.580387115478516\n",
            "Loss: -10.057817459106445\n",
            "Loss: -9.631606101989746\n",
            "Loss: -9.228426933288574\n",
            "Loss: -8.823090553283691\n",
            "Loss: -8.413155555725098\n",
            "Loss: -8.022479057312012\n",
            "Loss: -7.623702049255371\n",
            "Loss: -7.221518516540527\n",
            "Loss: -6.847294330596924\n",
            "Loss: -6.472691535949707\n",
            "Loss: -6.096856594085693\n",
            "Loss: -5.738944053649902\n",
            "Loss: -5.357757568359375\n",
            "Loss: -5.000570774078369\n",
            "Loss: -4.653351306915283\n",
            "We fooled the network after 25 iterations!\n",
            "New prediction: 0\n",
            "14248img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -13.519526481628418\n",
            "Loss: -13.189030647277832\n",
            "Loss: -12.857146263122559\n",
            "Loss: -12.523625373840332\n",
            "Loss: -12.193170547485352\n",
            "Loss: -11.871326446533203\n",
            "Loss: -11.545381546020508\n",
            "Loss: -11.22093677520752\n",
            "Loss: -10.89311695098877\n",
            "Loss: -10.579482078552246\n",
            "Loss: -10.263309478759766\n",
            "Loss: -9.95966911315918\n",
            "Loss: -9.647021293640137\n",
            "Loss: -9.33438777923584\n",
            "Loss: -9.018362998962402\n",
            "Loss: -8.699980735778809\n",
            "Loss: -8.401433944702148\n",
            "Loss: -8.085451126098633\n",
            "Loss: -7.7691168785095215\n",
            "Loss: -7.451330184936523\n",
            "Loss: -7.125710487365723\n",
            "Loss: -6.7557454109191895\n",
            "Loss: -6.408287048339844\n",
            "Loss: -6.076629161834717\n",
            "Loss: -5.751013278961182\n",
            "Loss: -5.419539451599121\n",
            "Loss: -5.078271865844727\n",
            "Loss: -4.744231700897217\n",
            "Loss: -4.454153060913086\n",
            "Loss: -4.188762187957764\n",
            "Loss: -3.9193308353424072\n",
            "Loss: -3.6504335403442383\n",
            "Loss: -3.3804447650909424\n",
            "Loss: -3.135756254196167\n",
            "We fooled the network after 33 iterations!\n",
            "New prediction: 8\n",
            "14256img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -24.938749313354492\n",
            "Loss: -24.42205810546875\n",
            "Loss: -23.90391731262207\n",
            "Loss: -23.38314437866211\n",
            "Loss: -22.861783981323242\n",
            "Loss: -22.348962783813477\n",
            "Loss: -21.8465633392334\n",
            "Loss: -21.3395938873291\n",
            "Loss: -20.841550827026367\n",
            "Loss: -20.33949851989746\n",
            "Loss: -19.840333938598633\n",
            "Loss: -19.39458656311035\n",
            "Loss: -18.925039291381836\n",
            "Loss: -18.463239669799805\n",
            "Loss: -17.9997501373291\n",
            "Loss: -17.536623001098633\n",
            "Loss: -17.07536506652832\n",
            "Loss: -16.61408805847168\n",
            "Loss: -16.161205291748047\n",
            "Loss: -15.715581893920898\n",
            "Loss: -15.258793830871582\n",
            "Loss: -14.780734062194824\n",
            "Loss: -14.307869911193848\n",
            "Loss: -13.820542335510254\n",
            "Loss: -13.317215919494629\n",
            "Loss: -12.845484733581543\n",
            "Loss: -12.356600761413574\n",
            "Loss: -11.84660816192627\n",
            "Loss: -11.350342750549316\n",
            "Loss: -10.852476119995117\n",
            "Loss: -10.348248481750488\n",
            "Loss: -9.81618595123291\n",
            "Loss: -9.301980972290039\n",
            "Loss: -8.871485710144043\n",
            "Loss: -8.442246437072754\n",
            "Loss: -8.018107414245605\n",
            "Loss: -7.584268569946289\n",
            "Loss: -7.169076919555664\n",
            "Loss: -6.748800277709961\n",
            "Loss: -6.331425189971924\n",
            "Loss: -5.963002681732178\n",
            "Loss: -5.587602138519287\n",
            "Loss: -5.237041473388672\n",
            "Loss: -4.906216144561768\n",
            "Loss: -4.638660430908203\n",
            "Loss: -4.359467029571533\n",
            "Loss: -4.168267250061035\n",
            "Loss: -3.9807894229888916\n",
            "We fooled the network after 47 iterations!\n",
            "New prediction: 2\n",
            "14265img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -24.100648880004883\n",
            "Loss: -23.57147979736328\n",
            "Loss: -23.006000518798828\n",
            "Loss: -22.43731117248535\n",
            "Loss: -21.863691329956055\n",
            "Loss: -21.31389808654785\n",
            "Loss: -20.790632247924805\n",
            "Loss: -20.249109268188477\n",
            "Loss: -19.730199813842773\n",
            "Loss: -19.287839889526367\n",
            "Loss: -18.826580047607422\n",
            "Loss: -18.379976272583008\n",
            "Loss: -17.930688858032227\n",
            "Loss: -17.518247604370117\n",
            "Loss: -17.120376586914062\n",
            "Loss: -16.6854305267334\n",
            "Loss: -16.2734317779541\n",
            "Loss: -15.860957145690918\n",
            "Loss: -15.4404878616333\n",
            "Loss: -15.060846328735352\n",
            "Loss: -14.696601867675781\n",
            "Loss: -14.323960304260254\n",
            "Loss: -13.99013614654541\n",
            "Loss: -13.623129844665527\n",
            "Loss: -13.271040916442871\n",
            "Loss: -12.92841625213623\n",
            "Loss: -12.57906436920166\n",
            "Loss: -12.215994834899902\n",
            "Loss: -11.875825881958008\n",
            "Loss: -11.548653602600098\n",
            "Loss: -11.214170455932617\n",
            "Loss: -10.877840995788574\n",
            "Loss: -10.59105110168457\n",
            "Loss: -10.301596641540527\n",
            "Loss: -10.02460765838623\n",
            "Loss: -9.742264747619629\n",
            "Loss: -9.468968391418457\n",
            "Loss: -9.207451820373535\n",
            "Loss: -8.928542137145996\n",
            "Loss: -8.645339012145996\n",
            "Loss: -8.374456405639648\n",
            "Loss: -8.104537010192871\n",
            "Loss: -7.831396102905273\n",
            "Loss: -7.558156490325928\n",
            "Loss: -7.2811431884765625\n",
            "Loss: -7.016890048980713\n",
            "Loss: -6.7350592613220215\n",
            "Loss: -6.463333606719971\n",
            "Loss: -6.190858364105225\n",
            "Loss: -5.915769577026367\n",
            "Loss: -5.637041091918945\n",
            "Loss: -5.366434574127197\n",
            "Loss: -5.089733600616455\n",
            "Loss: -4.813655853271484\n",
            "Loss: -4.544078826904297\n",
            "Loss: -4.267447471618652\n",
            "Loss: -3.977154493331909\n",
            "Loss: -3.700087308883667\n",
            "Loss: -3.4274919033050537\n",
            "Loss: -3.178166151046753\n",
            "Loss: -2.8947818279266357\n",
            "Loss: -2.621387004852295\n",
            "Loss: -2.3663811683654785\n",
            "Loss: -2.1555023193359375\n",
            "We fooled the network after 63 iterations!\n",
            "New prediction: 8\n",
            "14292img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -40.05894088745117\n",
            "Loss: -39.51396179199219\n",
            "Loss: -38.973392486572266\n",
            "Loss: -38.420127868652344\n",
            "Loss: -37.8713493347168\n",
            "Loss: -37.3249626159668\n",
            "Loss: -36.77808380126953\n",
            "Loss: -36.2313232421875\n",
            "Loss: -35.67991638183594\n",
            "Loss: -35.20157241821289\n",
            "Loss: -34.74562454223633\n",
            "Loss: -34.28741455078125\n",
            "Loss: -33.80824661254883\n",
            "Loss: -33.33906173706055\n",
            "Loss: -32.87625503540039\n",
            "Loss: -32.39839172363281\n",
            "Loss: -31.923906326293945\n",
            "Loss: -31.46190071105957\n",
            "Loss: -30.994890213012695\n",
            "Loss: -30.51369285583496\n",
            "Loss: -30.038393020629883\n",
            "Loss: -29.554344177246094\n",
            "Loss: -29.074003219604492\n",
            "Loss: -28.58228874206543\n",
            "Loss: -28.098068237304688\n",
            "Loss: -27.611400604248047\n",
            "Loss: -27.126428604125977\n",
            "Loss: -26.642648696899414\n",
            "Loss: -26.251310348510742\n",
            "Loss: -25.83913230895996\n",
            "Loss: -25.429487228393555\n",
            "Loss: -25.027936935424805\n",
            "Loss: -24.628686904907227\n",
            "Loss: -24.222299575805664\n",
            "Loss: -23.823577880859375\n",
            "Loss: -23.43254280090332\n",
            "Loss: -23.04741096496582\n",
            "Loss: -22.653884887695312\n",
            "Loss: -22.26176643371582\n",
            "Loss: -21.863985061645508\n",
            "Loss: -21.480220794677734\n",
            "Loss: -21.09001922607422\n",
            "Loss: -20.711742401123047\n",
            "Loss: -20.326833724975586\n",
            "Loss: -19.961620330810547\n",
            "Loss: -19.595666885375977\n",
            "Loss: -19.240596771240234\n",
            "Loss: -18.877216339111328\n",
            "Loss: -18.531564712524414\n",
            "Loss: -18.178747177124023\n",
            "Loss: -17.82291603088379\n",
            "Loss: -17.46587562561035\n",
            "Loss: -17.1356258392334\n",
            "Loss: -16.7956485748291\n",
            "Loss: -16.46394920349121\n",
            "Loss: -16.133710861206055\n",
            "Loss: -15.809844017028809\n",
            "Loss: -15.484618186950684\n",
            "Loss: -15.153791427612305\n",
            "Loss: -14.813979148864746\n",
            "Loss: -14.484271049499512\n",
            "Loss: -14.16363525390625\n",
            "Loss: -13.845078468322754\n",
            "Loss: -13.52710247039795\n",
            "Loss: -13.202933311462402\n",
            "Loss: -12.88158893585205\n",
            "Loss: -12.578782081604004\n",
            "Loss: -12.262919425964355\n",
            "Loss: -11.949565887451172\n",
            "Loss: -11.639840126037598\n",
            "Loss: -11.331034660339355\n",
            "Loss: -11.040587425231934\n",
            "Loss: -10.714491844177246\n",
            "Loss: -10.392390251159668\n",
            "Loss: -10.07540225982666\n",
            "Loss: -9.754615783691406\n",
            "Loss: -9.484858512878418\n",
            "Loss: -9.21624755859375\n",
            "Loss: -8.944063186645508\n",
            "Loss: -8.68722915649414\n",
            "Loss: -8.408320426940918\n",
            "Loss: -8.124890327453613\n",
            "Loss: -7.845296859741211\n",
            "Loss: -7.571441173553467\n",
            "Loss: -7.300480842590332\n",
            "Loss: -7.004289627075195\n",
            "Loss: -6.712305068969727\n",
            "Loss: -6.423266410827637\n",
            "Loss: -6.133096694946289\n",
            "Loss: -5.848336219787598\n",
            "Loss: -5.554042339324951\n",
            "Loss: -5.26068115234375\n",
            "Loss: -4.944057464599609\n",
            "Loss: -4.626036167144775\n",
            "Loss: -4.298970699310303\n",
            "Loss: -3.988651990890503\n",
            "Loss: -3.6966960430145264\n",
            "Loss: -3.4390525817871094\n",
            "Loss: -3.191685199737549\n",
            "Loss: -2.963474988937378\n",
            "14300img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -27.176660537719727\n",
            "Loss: -26.586328506469727\n",
            "Loss: -25.998563766479492\n",
            "Loss: -25.36908531188965\n",
            "Loss: -24.75257682800293\n",
            "Loss: -24.156036376953125\n",
            "Loss: -23.523969650268555\n",
            "Loss: -22.933393478393555\n",
            "Loss: -22.2938289642334\n",
            "Loss: -21.631031036376953\n",
            "Loss: -20.957725524902344\n",
            "Loss: -20.300382614135742\n",
            "Loss: -19.623058319091797\n",
            "Loss: -18.963727951049805\n",
            "Loss: -18.29011344909668\n",
            "Loss: -17.619417190551758\n",
            "Loss: -17.01953125\n",
            "Loss: -16.529972076416016\n",
            "Loss: -16.091150283813477\n",
            "Loss: -15.64941120147705\n",
            "Loss: -15.197176933288574\n",
            "Loss: -14.782084465026855\n",
            "Loss: -14.32800006866455\n",
            "Loss: -13.876864433288574\n",
            "Loss: -13.436053276062012\n",
            "Loss: -13.028840065002441\n",
            "Loss: -12.654948234558105\n",
            "Loss: -12.316817283630371\n",
            "Loss: -12.014511108398438\n",
            "Loss: -11.688775062561035\n",
            "Loss: -11.359223365783691\n",
            "Loss: -11.018279075622559\n",
            "Loss: -10.703413009643555\n",
            "Loss: -10.381863594055176\n",
            "Loss: -10.054289817810059\n",
            "Loss: -9.707671165466309\n",
            "Loss: -9.382418632507324\n",
            "Loss: -9.055709838867188\n",
            "Loss: -8.730025291442871\n",
            "Loss: -8.417150497436523\n",
            "Loss: -8.123738288879395\n",
            "Loss: -7.7929487228393555\n",
            "Loss: -7.473196029663086\n",
            "Loss: -7.138582229614258\n",
            "Loss: -6.815108299255371\n",
            "Loss: -6.492822170257568\n",
            "Loss: -6.163941383361816\n",
            "Loss: -5.836677551269531\n",
            "Loss: -5.516082286834717\n",
            "Loss: -5.179740905761719\n",
            "Loss: -4.85549259185791\n",
            "Loss: -4.558746337890625\n",
            "Loss: -4.310675144195557\n",
            "Loss: -4.048092365264893\n",
            "We fooled the network after 53 iterations!\n",
            "New prediction: 5\n",
            "14308img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -47.517852783203125\n",
            "Loss: -46.901607513427734\n",
            "Loss: -46.27678298950195\n",
            "Loss: -45.64745330810547\n",
            "Loss: -45.029937744140625\n",
            "Loss: -44.417633056640625\n",
            "Loss: -43.80721664428711\n",
            "Loss: -43.194705963134766\n",
            "Loss: -42.574642181396484\n",
            "Loss: -41.97488021850586\n",
            "Loss: -41.368263244628906\n",
            "Loss: -40.792484283447266\n",
            "Loss: -40.16421890258789\n",
            "Loss: -39.546504974365234\n",
            "Loss: -38.93606185913086\n",
            "Loss: -38.3310661315918\n",
            "Loss: -37.751399993896484\n",
            "Loss: -37.132328033447266\n",
            "Loss: -36.489341735839844\n",
            "Loss: -35.868465423583984\n",
            "Loss: -35.21258544921875\n",
            "Loss: -34.576507568359375\n",
            "Loss: -33.95267105102539\n",
            "Loss: -33.300174713134766\n",
            "Loss: -32.66825485229492\n",
            "Loss: -32.053829193115234\n",
            "Loss: -31.420686721801758\n",
            "Loss: -30.7855281829834\n",
            "Loss: -30.16483497619629\n",
            "Loss: -29.534921646118164\n",
            "Loss: -28.91999626159668\n",
            "Loss: -28.308258056640625\n",
            "Loss: -27.675918579101562\n",
            "Loss: -27.060733795166016\n",
            "Loss: -26.43292236328125\n",
            "Loss: -25.831077575683594\n",
            "Loss: -25.199567794799805\n",
            "Loss: -24.592201232910156\n",
            "Loss: -24.013092041015625\n",
            "Loss: -23.408166885375977\n",
            "Loss: -22.86650276184082\n",
            "Loss: -22.305025100708008\n",
            "Loss: -21.71942710876465\n",
            "Loss: -21.160877227783203\n",
            "Loss: -20.5662784576416\n",
            "Loss: -20.005001068115234\n",
            "Loss: -19.552074432373047\n",
            "Loss: -19.04485511779785\n",
            "Loss: -18.569961547851562\n",
            "Loss: -18.14213752746582\n",
            "Loss: -17.755910873413086\n",
            "Loss: -17.37654685974121\n",
            "Loss: -16.999128341674805\n",
            "Loss: -16.599760055541992\n",
            "Loss: -16.199811935424805\n",
            "Loss: -15.830889701843262\n",
            "Loss: -15.451750755310059\n",
            "Loss: -15.088976860046387\n",
            "Loss: -14.703369140625\n",
            "Loss: -14.329939842224121\n",
            "Loss: -13.951193809509277\n",
            "Loss: -13.577888488769531\n",
            "Loss: -13.212725639343262\n",
            "Loss: -12.84521484375\n",
            "Loss: -12.507176399230957\n",
            "Loss: -12.163722038269043\n",
            "Loss: -11.80667781829834\n",
            "Loss: -11.462693214416504\n",
            "Loss: -11.13086986541748\n",
            "Loss: -10.793322563171387\n",
            "Loss: -10.465864181518555\n",
            "Loss: -10.143624305725098\n",
            "Loss: -9.814279556274414\n",
            "Loss: -9.497452735900879\n",
            "Loss: -9.180238723754883\n",
            "Loss: -8.847129821777344\n",
            "Loss: -8.546984672546387\n",
            "Loss: -8.215102195739746\n",
            "Loss: -7.892607688903809\n",
            "Loss: -7.603026390075684\n",
            "Loss: -7.298474311828613\n",
            "Loss: -6.981705665588379\n",
            "Loss: -6.677888870239258\n",
            "Loss: -6.374791145324707\n",
            "Loss: -6.072649002075195\n",
            "Loss: -5.72813606262207\n",
            "Loss: -5.416799545288086\n",
            "Loss: -5.096461296081543\n",
            "Loss: -4.780052185058594\n",
            "Loss: -4.448180675506592\n",
            "Loss: -4.1415019035339355\n",
            "Loss: -3.791438341140747\n",
            "Loss: -3.4665238857269287\n",
            "Loss: -3.116884469985962\n",
            "Loss: -2.7985002994537354\n",
            "Loss: -2.453913688659668\n",
            "Loss: -2.067847967147827\n",
            "Loss: -1.713576316833496\n",
            "Loss: -1.5128551721572876\n",
            "Loss: -1.381527066230774\n",
            "14315img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -20.705286026000977\n",
            "Loss: -20.102266311645508\n",
            "Loss: -19.46524429321289\n",
            "Loss: -18.85689926147461\n",
            "Loss: -18.234861373901367\n",
            "Loss: -17.59037208557129\n",
            "Loss: -16.982532501220703\n",
            "Loss: -16.402332305908203\n",
            "Loss: -15.821797370910645\n",
            "Loss: -15.22748851776123\n",
            "Loss: -14.703944206237793\n",
            "Loss: -14.19664478302002\n",
            "Loss: -13.729976654052734\n",
            "Loss: -13.279378890991211\n",
            "Loss: -12.92179012298584\n",
            "Loss: -12.544293403625488\n",
            "Loss: -12.178730010986328\n",
            "Loss: -11.810429573059082\n",
            "Loss: -11.496175765991211\n",
            "Loss: -11.125129699707031\n",
            "Loss: -10.755532264709473\n",
            "Loss: -10.404623031616211\n",
            "Loss: -10.022415161132812\n",
            "Loss: -9.66473388671875\n",
            "Loss: -9.298261642456055\n",
            "Loss: -8.935723304748535\n",
            "Loss: -8.595006942749023\n",
            "Loss: -8.222002029418945\n",
            "Loss: -7.883379936218262\n",
            "Loss: -7.5721635818481445\n",
            "Loss: -7.28186559677124\n",
            "Loss: -7.011844635009766\n",
            "Loss: -6.751151084899902\n",
            "Loss: -6.472818374633789\n",
            "Loss: -6.228751182556152\n",
            "Loss: -6.003344535827637\n",
            "Loss: -5.771728515625\n",
            "Loss: -5.497869491577148\n",
            "Loss: -5.260563850402832\n",
            "Loss: -4.9904327392578125\n",
            "Loss: -4.73142147064209\n",
            "Loss: -4.4568939208984375\n",
            "Loss: -4.20809268951416\n",
            "Loss: -3.9407460689544678\n",
            "Loss: -3.6878411769866943\n",
            "Loss: -3.4679622650146484\n",
            "Loss: -3.214067220687866\n",
            "Loss: -3.0056989192962646\n",
            "We fooled the network after 47 iterations!\n",
            "New prediction: 8\n",
            "14328img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -27.631132125854492\n",
            "Loss: -27.083547592163086\n",
            "Loss: -26.53421974182129\n",
            "Loss: -26.072683334350586\n",
            "Loss: -25.613100051879883\n",
            "Loss: -25.157936096191406\n",
            "Loss: -24.72733497619629\n",
            "Loss: -24.307382583618164\n",
            "Loss: -23.881772994995117\n",
            "Loss: -23.455930709838867\n",
            "Loss: -23.044870376586914\n",
            "Loss: -22.629405975341797\n",
            "Loss: -22.228750228881836\n",
            "Loss: -21.840097427368164\n",
            "Loss: -21.441591262817383\n",
            "Loss: -21.063920974731445\n",
            "Loss: -20.669641494750977\n",
            "Loss: -20.292186737060547\n",
            "Loss: -19.912912368774414\n",
            "Loss: -19.545251846313477\n",
            "Loss: -19.158737182617188\n",
            "Loss: -18.82741355895996\n",
            "Loss: -18.483999252319336\n",
            "Loss: -18.145917892456055\n",
            "Loss: -17.80430793762207\n",
            "Loss: -17.468494415283203\n",
            "Loss: -17.1319637298584\n",
            "Loss: -16.793607711791992\n",
            "Loss: -16.442827224731445\n",
            "Loss: -16.143495559692383\n",
            "Loss: -15.815442085266113\n",
            "Loss: -15.430848121643066\n",
            "Loss: -15.002692222595215\n",
            "Loss: -14.563750267028809\n",
            "Loss: -14.133414268493652\n",
            "Loss: -13.747906684875488\n",
            "Loss: -13.320462226867676\n",
            "Loss: -12.926739692687988\n",
            "Loss: -12.51352596282959\n",
            "Loss: -12.115311622619629\n",
            "Loss: -11.722947120666504\n",
            "Loss: -11.341584205627441\n",
            "Loss: -10.957879066467285\n",
            "Loss: -10.583392143249512\n",
            "Loss: -10.200833320617676\n",
            "Loss: -9.857025146484375\n",
            "Loss: -9.569132804870605\n",
            "Loss: -9.263407707214355\n",
            "Loss: -8.949200630187988\n",
            "Loss: -8.679646492004395\n",
            "Loss: -8.3391695022583\n",
            "Loss: -8.063243865966797\n",
            "Loss: -7.780717849731445\n",
            "Loss: -7.475049018859863\n",
            "Loss: -7.204876899719238\n",
            "Loss: -6.910913944244385\n",
            "Loss: -6.648631572723389\n",
            "Loss: -6.401296138763428\n",
            "Loss: -6.194266319274902\n",
            "Loss: -5.982978820800781\n",
            "Loss: -5.761387825012207\n",
            "Loss: -5.538717746734619\n",
            "Loss: -5.331536293029785\n",
            "Loss: -5.128382682800293\n",
            "Loss: -4.912376880645752\n",
            "Loss: -4.700174331665039\n",
            "Loss: -4.500110149383545\n",
            "Loss: -4.301093101501465\n",
            "Loss: -4.0819783210754395\n",
            "Loss: -3.8861358165740967\n",
            "Loss: -3.7119128704071045\n",
            "Loss: -3.501291036605835\n",
            "Loss: -3.2648885250091553\n",
            "Loss: -3.0777971744537354\n",
            "Loss: -2.941232919692993\n",
            "Loss: -2.7963359355926514\n",
            "Loss: -2.6431684494018555\n",
            "Loss: -2.4834706783294678\n",
            "Loss: -2.313131332397461\n",
            "Loss: -2.137514114379883\n",
            "Loss: -1.9450145959854126\n",
            "Loss: -1.709605097770691\n",
            "Loss: -1.4495595693588257\n",
            "We fooled the network after 82 iterations!\n",
            "New prediction: 5\n",
            "14330img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -27.175670623779297\n",
            "Loss: -26.50118064880371\n",
            "Loss: -25.7923526763916\n",
            "Loss: -25.071395874023438\n",
            "Loss: -24.36725425720215\n",
            "Loss: -23.672101974487305\n",
            "Loss: -22.997079849243164\n",
            "Loss: -22.311376571655273\n",
            "Loss: -21.639799118041992\n",
            "Loss: -20.97943687438965\n",
            "Loss: -20.306982040405273\n",
            "Loss: -19.661161422729492\n",
            "Loss: -19.02064323425293\n",
            "Loss: -18.39250373840332\n",
            "Loss: -17.81892204284668\n",
            "Loss: -17.301328659057617\n",
            "Loss: -16.76453971862793\n",
            "Loss: -16.2535343170166\n",
            "Loss: -15.729174613952637\n",
            "Loss: -15.194963455200195\n",
            "Loss: -14.689419746398926\n",
            "Loss: -14.16373348236084\n",
            "Loss: -13.64513111114502\n",
            "Loss: -13.125664710998535\n",
            "Loss: -12.604891777038574\n",
            "Loss: -12.06989574432373\n",
            "Loss: -11.581755638122559\n",
            "Loss: -11.155619621276855\n",
            "Loss: -10.739413261413574\n",
            "Loss: -10.318882942199707\n",
            "Loss: -9.907181739807129\n",
            "Loss: -9.494141578674316\n",
            "Loss: -9.10236644744873\n",
            "Loss: -8.699376106262207\n",
            "Loss: -8.339142799377441\n",
            "Loss: -7.952153205871582\n",
            "Loss: -7.578100204467773\n",
            "Loss: -7.206297874450684\n",
            "Loss: -6.836367607116699\n",
            "Loss: -6.452195167541504\n",
            "Loss: -6.081872463226318\n",
            "Loss: -5.695524215698242\n",
            "Loss: -5.320024013519287\n",
            "Loss: -4.954473972320557\n",
            "Loss: -4.5923895835876465\n",
            "Loss: -4.2343902587890625\n",
            "Loss: -3.9281961917877197\n",
            "Loss: -3.6638309955596924\n",
            "Loss: -3.440523862838745\n",
            "Loss: -3.207167863845825\n",
            "Loss: -2.9882967472076416\n",
            "Loss: -2.768193006515503\n",
            "Loss: -2.5565359592437744\n",
            "Loss: -2.359935760498047\n",
            "Loss: -2.1838066577911377\n",
            "Loss: -2.024522304534912\n",
            "Loss: -1.8523097038269043\n",
            "Loss: -1.6671396493911743\n",
            "Loss: -1.4905036687850952\n",
            "Loss: -1.3199841976165771\n",
            "We fooled the network after 59 iterations!\n",
            "New prediction: 1\n",
            "14339img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -47.091007232666016\n",
            "Loss: -46.55473709106445\n",
            "Loss: -45.98381042480469\n",
            "Loss: -45.44278335571289\n",
            "Loss: -44.89426040649414\n",
            "Loss: -44.329837799072266\n",
            "Loss: -43.7765998840332\n",
            "Loss: -43.218509674072266\n",
            "Loss: -42.6647834777832\n",
            "Loss: -42.11053466796875\n",
            "Loss: -41.559898376464844\n",
            "Loss: -40.99536895751953\n",
            "Loss: -40.470008850097656\n",
            "Loss: -39.91605758666992\n",
            "Loss: -39.39284896850586\n",
            "Loss: -38.8646125793457\n",
            "Loss: -38.331695556640625\n",
            "Loss: -37.79500198364258\n",
            "Loss: -37.26737976074219\n",
            "Loss: -36.735809326171875\n",
            "Loss: -36.208274841308594\n",
            "Loss: -35.674983978271484\n",
            "Loss: -35.151275634765625\n",
            "Loss: -34.617835998535156\n",
            "Loss: -34.088802337646484\n",
            "Loss: -33.5828857421875\n",
            "Loss: -33.030216217041016\n",
            "Loss: -32.49941635131836\n",
            "Loss: -31.98484230041504\n",
            "Loss: -31.46268081665039\n",
            "Loss: -30.93297576904297\n",
            "Loss: -30.404882431030273\n",
            "Loss: -29.90618324279785\n",
            "Loss: -29.43686866760254\n",
            "Loss: -28.91505241394043\n",
            "Loss: -28.437273025512695\n",
            "Loss: -27.9158878326416\n",
            "Loss: -27.36113929748535\n",
            "Loss: -26.86741065979004\n",
            "Loss: -26.34163475036621\n",
            "Loss: -25.825483322143555\n",
            "Loss: -25.316972732543945\n",
            "Loss: -24.799985885620117\n",
            "Loss: -24.28868293762207\n",
            "Loss: -23.801837921142578\n",
            "Loss: -23.29646873474121\n",
            "Loss: -22.813135147094727\n",
            "Loss: -22.331165313720703\n",
            "Loss: -21.806055068969727\n",
            "Loss: -21.329435348510742\n",
            "Loss: -20.8344669342041\n",
            "Loss: -20.33525276184082\n",
            "Loss: -19.85543441772461\n",
            "Loss: -19.39156150817871\n",
            "Loss: -18.88859748840332\n",
            "Loss: -18.496305465698242\n",
            "Loss: -18.100059509277344\n",
            "Loss: -17.755287170410156\n",
            "Loss: -17.452730178833008\n",
            "Loss: -17.1082820892334\n",
            "Loss: -16.810705184936523\n",
            "Loss: -16.482927322387695\n",
            "Loss: -16.151716232299805\n",
            "Loss: -15.878323554992676\n",
            "Loss: -15.575636863708496\n",
            "Loss: -15.302300453186035\n",
            "Loss: -15.009568214416504\n",
            "Loss: -14.721283912658691\n",
            "Loss: -14.481047630310059\n",
            "Loss: -14.168355941772461\n",
            "Loss: -13.897290229797363\n",
            "Loss: -13.645169258117676\n",
            "Loss: -13.362143516540527\n",
            "Loss: -13.094436645507812\n",
            "Loss: -12.804612159729004\n",
            "Loss: -12.544219970703125\n",
            "Loss: -12.256966590881348\n",
            "Loss: -12.003275871276855\n",
            "Loss: -11.692587852478027\n",
            "Loss: -11.427842140197754\n",
            "Loss: -11.12806224822998\n",
            "Loss: -10.853221893310547\n",
            "Loss: -10.577019691467285\n",
            "Loss: -10.302342414855957\n",
            "Loss: -10.000189781188965\n",
            "Loss: -9.758865356445312\n",
            "Loss: -9.481203079223633\n",
            "Loss: -9.204530715942383\n",
            "Loss: -8.914422035217285\n",
            "Loss: -8.615666389465332\n",
            "Loss: -8.376514434814453\n",
            "Loss: -8.098465919494629\n",
            "Loss: -7.811740875244141\n",
            "Loss: -7.563601493835449\n",
            "Loss: -7.287055015563965\n",
            "Loss: -7.014171600341797\n",
            "Loss: -6.773615837097168\n",
            "Loss: -6.503006935119629\n",
            "Loss: -6.254499435424805\n",
            "Loss: -5.982758522033691\n",
            "14344img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -21.87112808227539\n",
            "Loss: -21.280065536499023\n",
            "Loss: -20.64318084716797\n",
            "Loss: -20.01336669921875\n",
            "Loss: -19.374235153198242\n",
            "Loss: -18.739215850830078\n",
            "Loss: -18.135194778442383\n",
            "Loss: -17.542375564575195\n",
            "Loss: -16.9378662109375\n",
            "Loss: -16.32594108581543\n",
            "Loss: -15.70224666595459\n",
            "Loss: -15.095492362976074\n",
            "Loss: -14.497007369995117\n",
            "Loss: -13.879607200622559\n",
            "Loss: -13.283973693847656\n",
            "Loss: -12.670685768127441\n",
            "Loss: -12.172426223754883\n",
            "Loss: -11.655884742736816\n",
            "Loss: -11.200417518615723\n",
            "Loss: -10.70223331451416\n",
            "Loss: -10.220598220825195\n",
            "Loss: -9.823873519897461\n",
            "Loss: -9.439900398254395\n",
            "Loss: -9.068039894104004\n",
            "Loss: -8.704998970031738\n",
            "Loss: -8.322729110717773\n",
            "Loss: -7.969351768493652\n",
            "Loss: -7.596213340759277\n",
            "Loss: -7.283210277557373\n",
            "Loss: -6.978111743927002\n",
            "Loss: -6.643733024597168\n",
            "Loss: -6.328631401062012\n",
            "Loss: -5.981581687927246\n",
            "Loss: -5.673466682434082\n",
            "Loss: -5.317699432373047\n",
            "Loss: -5.001026153564453\n",
            "Loss: -4.668942451477051\n",
            "Loss: -4.3679609298706055\n",
            "Loss: -4.073727130889893\n",
            "Loss: -3.7907094955444336\n",
            "Loss: -3.506195306777954\n",
            "Loss: -3.2511017322540283\n",
            "Loss: -2.95385479927063\n",
            "We fooled the network after 42 iterations!\n",
            "New prediction: 8\n",
            "14347img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -7.553145885467529\n",
            "Loss: -7.125643730163574\n",
            "Loss: -6.703303337097168\n",
            "Loss: -6.313165664672852\n",
            "Loss: -5.889861106872559\n",
            "Loss: -5.488117218017578\n",
            "Loss: -5.095021724700928\n",
            "Loss: -4.768459796905518\n",
            "Loss: -4.451441764831543\n",
            "Loss: -4.112988471984863\n",
            "Loss: -3.780927896499634\n",
            "Loss: -3.4697577953338623\n",
            "Loss: -3.1215999126434326\n",
            "Loss: -2.7906076908111572\n",
            "Loss: -2.4681475162506104\n",
            "We fooled the network after 14 iterations!\n",
            "New prediction: 8\n",
            "14372img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -2.645307779312134\n",
            "We fooled the network after 0 iterations!\n",
            "New prediction: 8\n",
            "14374img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -24.146425247192383\n",
            "Loss: -23.605554580688477\n",
            "Loss: -23.002796173095703\n",
            "Loss: -22.389755249023438\n",
            "Loss: -21.76512908935547\n",
            "Loss: -21.178747177124023\n",
            "Loss: -20.604278564453125\n",
            "Loss: -20.028467178344727\n",
            "Loss: -19.472558975219727\n",
            "Loss: -18.93354606628418\n",
            "Loss: -18.391084671020508\n",
            "Loss: -17.847408294677734\n",
            "Loss: -17.337526321411133\n",
            "Loss: -16.854137420654297\n",
            "Loss: -16.388227462768555\n",
            "Loss: -15.941618919372559\n",
            "Loss: -15.528420448303223\n",
            "Loss: -15.061092376708984\n",
            "Loss: -14.658744812011719\n",
            "Loss: -14.192176818847656\n",
            "Loss: -13.786002159118652\n",
            "Loss: -13.371594429016113\n",
            "Loss: -12.924933433532715\n",
            "Loss: -12.546185493469238\n",
            "Loss: -12.133918762207031\n",
            "Loss: -11.796857833862305\n",
            "Loss: -11.477415084838867\n",
            "Loss: -11.146707534790039\n",
            "Loss: -10.884796142578125\n",
            "Loss: -10.59166431427002\n",
            "Loss: -10.310690879821777\n",
            "Loss: -10.049559593200684\n",
            "Loss: -9.751131057739258\n",
            "Loss: -9.48099136352539\n",
            "Loss: -9.218698501586914\n",
            "Loss: -8.931750297546387\n",
            "Loss: -8.688251495361328\n",
            "Loss: -8.394197463989258\n",
            "Loss: -8.10247802734375\n",
            "Loss: -7.814698219299316\n",
            "Loss: -7.516687393188477\n",
            "Loss: -7.238898277282715\n",
            "Loss: -6.953607559204102\n",
            "Loss: -6.66450834274292\n",
            "Loss: -6.399929046630859\n",
            "Loss: -6.126540184020996\n",
            "Loss: -5.856884956359863\n",
            "Loss: -5.6186418533325195\n",
            "Loss: -5.336599826812744\n",
            "Loss: -5.092655181884766\n",
            "Loss: -4.828121185302734\n",
            "Loss: -4.56647253036499\n",
            "Loss: -4.316042423248291\n",
            "Loss: -4.083912372589111\n",
            "Loss: -3.873870849609375\n",
            "Loss: -3.671692132949829\n",
            "Loss: -3.482811450958252\n",
            "Loss: -3.2906978130340576\n",
            "We fooled the network after 57 iterations!\n",
            "New prediction: 5\n",
            "14377img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -22.362163543701172\n",
            "Loss: -21.783920288085938\n",
            "Loss: -21.151403427124023\n",
            "Loss: -20.498231887817383\n",
            "Loss: -19.845731735229492\n",
            "Loss: -19.20732307434082\n",
            "Loss: -18.558103561401367\n",
            "Loss: -17.93498420715332\n",
            "Loss: -17.288856506347656\n",
            "Loss: -16.64637565612793\n",
            "Loss: -16.014652252197266\n",
            "Loss: -15.355439186096191\n",
            "Loss: -14.738709449768066\n",
            "Loss: -14.098852157592773\n",
            "Loss: -13.461562156677246\n",
            "Loss: -12.825222969055176\n",
            "Loss: -12.19098949432373\n",
            "Loss: -11.556868553161621\n",
            "Loss: -10.944777488708496\n",
            "Loss: -10.408543586730957\n",
            "Loss: -9.887898445129395\n",
            "Loss: -9.35205364227295\n",
            "Loss: -8.799715995788574\n",
            "Loss: -8.228328704833984\n",
            "Loss: -7.721949100494385\n",
            "Loss: -7.260475158691406\n",
            "Loss: -6.875175476074219\n",
            "Loss: -6.533565521240234\n",
            "Loss: -6.205137252807617\n",
            "Loss: -5.865207195281982\n",
            "Loss: -5.541424751281738\n",
            "Loss: -5.220677375793457\n",
            "Loss: -4.905581951141357\n",
            "Loss: -4.573995113372803\n",
            "Loss: -4.291735649108887\n",
            "Loss: -3.965681314468384\n",
            "Loss: -3.637349843978882\n",
            "Loss: -3.2904884815216064\n",
            "We fooled the network after 37 iterations!\n",
            "New prediction: 8\n",
            "14399img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -36.07329177856445\n",
            "Loss: -35.471466064453125\n",
            "Loss: -34.861148834228516\n",
            "Loss: -34.25192642211914\n",
            "Loss: -33.63142776489258\n",
            "Loss: -32.985469818115234\n",
            "Loss: -32.37382507324219\n",
            "Loss: -31.761215209960938\n",
            "Loss: -31.150041580200195\n",
            "Loss: -30.539772033691406\n",
            "Loss: -29.960237503051758\n",
            "Loss: -29.446487426757812\n",
            "Loss: -28.94829559326172\n",
            "Loss: -28.447050094604492\n",
            "Loss: -27.951799392700195\n",
            "Loss: -27.44719886779785\n",
            "Loss: -26.941707611083984\n",
            "Loss: -26.442312240600586\n",
            "Loss: -25.90882682800293\n",
            "Loss: -25.39121437072754\n",
            "Loss: -24.881818771362305\n",
            "Loss: -24.36302375793457\n",
            "Loss: -23.850805282592773\n",
            "Loss: -23.331682205200195\n",
            "Loss: -22.8183650970459\n",
            "Loss: -22.295026779174805\n",
            "Loss: -21.77305793762207\n",
            "Loss: -21.257143020629883\n",
            "Loss: -20.766508102416992\n",
            "Loss: -20.288270950317383\n",
            "Loss: -19.824951171875\n",
            "Loss: -19.34828758239746\n",
            "Loss: -18.87367820739746\n",
            "Loss: -18.41636085510254\n",
            "Loss: -17.877553939819336\n",
            "Loss: -17.32678985595703\n",
            "Loss: -16.7763729095459\n",
            "Loss: -16.22974395751953\n",
            "Loss: -15.777532577514648\n",
            "Loss: -15.334993362426758\n",
            "Loss: -14.87688159942627\n",
            "Loss: -14.435516357421875\n",
            "Loss: -13.997944831848145\n",
            "Loss: -13.549054145812988\n",
            "Loss: -13.170220375061035\n",
            "Loss: -12.836430549621582\n",
            "Loss: -12.503493309020996\n",
            "Loss: -12.1775484085083\n",
            "Loss: -11.861117362976074\n",
            "Loss: -11.524184226989746\n",
            "Loss: -11.183612823486328\n",
            "Loss: -10.85261344909668\n",
            "Loss: -10.501860618591309\n",
            "Loss: -10.116927146911621\n",
            "Loss: -9.7247314453125\n",
            "Loss: -9.320832252502441\n",
            "Loss: -8.910285949707031\n",
            "Loss: -8.539658546447754\n",
            "Loss: -8.117592811584473\n",
            "Loss: -7.726264953613281\n",
            "Loss: -7.317164421081543\n",
            "Loss: -6.938451766967773\n",
            "Loss: -6.565018653869629\n",
            "Loss: -6.201714992523193\n",
            "Loss: -5.869260311126709\n",
            "Loss: -5.591314315795898\n",
            "Loss: -5.310189247131348\n",
            "Loss: -5.013832092285156\n",
            "Loss: -4.7455902099609375\n",
            "Loss: -4.447253704071045\n",
            "Loss: -4.137650012969971\n",
            "Loss: -3.840148687362671\n",
            "Loss: -3.5226757526397705\n",
            "We fooled the network after 72 iterations!\n",
            "New prediction: 8\n",
            "14400img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -9.021567344665527\n",
            "Loss: -8.62015438079834\n",
            "Loss: -8.222780227661133\n",
            "Loss: -7.833780765533447\n",
            "Loss: -7.541994094848633\n",
            "Loss: -7.232267379760742\n",
            "Loss: -6.929296970367432\n",
            "Loss: -6.66567325592041\n",
            "Loss: -6.402360916137695\n",
            "Loss: -6.146967887878418\n",
            "Loss: -5.89439582824707\n",
            "Loss: -5.6369171142578125\n",
            "Loss: -5.38360595703125\n",
            "Loss: -5.126426696777344\n",
            "Loss: -4.852438449859619\n",
            "Loss: -4.586429119110107\n",
            "Loss: -4.3367838859558105\n",
            "Loss: -4.0896100997924805\n",
            "Loss: -3.8943850994110107\n",
            "Loss: -3.71181321144104\n",
            "Loss: -3.5851006507873535\n",
            "Loss: -3.453833818435669\n",
            "Loss: -3.3220489025115967\n",
            "Loss: -3.1892194747924805\n",
            "Loss: -3.0496835708618164\n",
            "Loss: -2.9093751907348633\n",
            "Loss: -2.7695465087890625\n",
            "Loss: -2.5887818336486816\n",
            "Loss: -2.370957136154175\n",
            "Loss: -2.1627357006073\n",
            "Loss: -2.056220293045044\n",
            "Loss: -1.945522665977478\n",
            "Loss: -1.8338850736618042\n",
            "Loss: -1.7151814699172974\n",
            "Loss: -1.5983164310455322\n",
            "Loss: -1.4840854406356812\n",
            "Loss: -1.3721895217895508\n",
            "We fooled the network after 36 iterations!\n",
            "New prediction: 2\n",
            "14405img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -39.00784683227539\n",
            "Loss: -38.46135330200195\n",
            "Loss: -37.86460494995117\n",
            "Loss: -37.256710052490234\n",
            "Loss: -36.66017150878906\n",
            "Loss: -36.05604553222656\n",
            "Loss: -35.49435806274414\n",
            "Loss: -35.02600860595703\n",
            "Loss: -34.56739807128906\n",
            "Loss: -34.09947967529297\n",
            "Loss: -33.641841888427734\n",
            "Loss: -33.2209358215332\n",
            "Loss: -32.77177047729492\n",
            "Loss: -32.35796356201172\n",
            "Loss: -31.899118423461914\n",
            "Loss: -31.47051239013672\n",
            "Loss: -30.9899845123291\n",
            "Loss: -30.54396629333496\n",
            "Loss: -30.079742431640625\n",
            "Loss: -29.622358322143555\n",
            "Loss: -29.169775009155273\n",
            "Loss: -28.703325271606445\n",
            "Loss: -28.26898193359375\n",
            "Loss: -27.820053100585938\n",
            "Loss: -27.37030601501465\n",
            "Loss: -26.94852638244629\n",
            "Loss: -26.518510818481445\n",
            "Loss: -26.084638595581055\n",
            "Loss: -25.651296615600586\n",
            "Loss: -25.227083206176758\n",
            "Loss: -24.8006534576416\n",
            "Loss: -24.408220291137695\n",
            "Loss: -23.99210548400879\n",
            "Loss: -23.606863021850586\n",
            "Loss: -23.195341110229492\n",
            "Loss: -22.776750564575195\n",
            "Loss: -22.433271408081055\n",
            "Loss: -21.99997901916504\n",
            "Loss: -21.620651245117188\n",
            "Loss: -21.259098052978516\n",
            "Loss: -20.874361038208008\n",
            "Loss: -20.53097152709961\n",
            "Loss: -20.16575050354004\n",
            "Loss: -19.824256896972656\n",
            "Loss: -19.459566116333008\n",
            "Loss: -19.112768173217773\n",
            "Loss: -18.784345626831055\n",
            "Loss: -18.3974552154541\n",
            "Loss: -18.073650360107422\n",
            "Loss: -17.736303329467773\n",
            "Loss: -17.361719131469727\n",
            "Loss: -17.026769638061523\n",
            "Loss: -16.697622299194336\n",
            "Loss: -16.35390281677246\n",
            "Loss: -16.080930709838867\n",
            "Loss: -15.746253967285156\n",
            "Loss: -15.469522476196289\n",
            "Loss: -15.191802024841309\n",
            "Loss: -14.857487678527832\n",
            "Loss: -14.574095726013184\n",
            "Loss: -14.334365844726562\n",
            "Loss: -14.07568645477295\n",
            "Loss: -13.783034324645996\n",
            "Loss: -13.512825965881348\n",
            "Loss: -13.263224601745605\n",
            "Loss: -13.02047348022461\n",
            "Loss: -12.735527992248535\n",
            "Loss: -12.5216703414917\n",
            "Loss: -12.226550102233887\n",
            "Loss: -11.97276782989502\n",
            "Loss: -11.768157958984375\n",
            "Loss: -11.452958106994629\n",
            "Loss: -11.239120483398438\n",
            "Loss: -10.959468841552734\n",
            "Loss: -10.742264747619629\n",
            "Loss: -10.514925956726074\n",
            "Loss: -10.242321968078613\n",
            "Loss: -9.999895095825195\n",
            "Loss: -9.723978042602539\n",
            "Loss: -9.483366966247559\n",
            "Loss: -9.2208833694458\n",
            "Loss: -8.949603080749512\n",
            "Loss: -8.703977584838867\n",
            "Loss: -8.408482551574707\n",
            "Loss: -8.160298347473145\n",
            "Loss: -7.85759973526001\n",
            "Loss: -7.591422080993652\n",
            "Loss: -7.328920364379883\n",
            "Loss: -7.073919773101807\n",
            "Loss: -6.797381401062012\n",
            "Loss: -6.456625938415527\n",
            "Loss: -6.089466094970703\n",
            "Loss: -5.755862712860107\n",
            "Loss: -5.440415382385254\n",
            "Loss: -5.111441612243652\n",
            "Loss: -4.733101844787598\n",
            "Loss: -4.35982608795166\n",
            "Loss: -4.012364387512207\n",
            "Loss: -3.6409952640533447\n",
            "Loss: -3.2492618560791016\n",
            "14416img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -25.219831466674805\n",
            "Loss: -24.80782699584961\n",
            "Loss: -24.3901309967041\n",
            "Loss: -23.98030662536621\n",
            "Loss: -23.5805721282959\n",
            "Loss: -23.17275047302246\n",
            "Loss: -22.762887954711914\n",
            "Loss: -22.352609634399414\n",
            "Loss: -21.948305130004883\n",
            "Loss: -21.53960609436035\n",
            "Loss: -21.134119033813477\n",
            "Loss: -20.728744506835938\n",
            "Loss: -20.323436737060547\n",
            "Loss: -19.91459846496582\n",
            "Loss: -19.55384635925293\n",
            "Loss: -19.2047176361084\n",
            "Loss: -18.845048904418945\n",
            "Loss: -18.491628646850586\n",
            "Loss: -18.13469123840332\n",
            "Loss: -17.774873733520508\n",
            "Loss: -17.42552947998047\n",
            "Loss: -17.05376625061035\n",
            "Loss: -16.659732818603516\n",
            "Loss: -16.29421043395996\n",
            "Loss: -15.9218111038208\n",
            "Loss: -15.543295860290527\n",
            "Loss: -15.17253303527832\n",
            "Loss: -14.792330741882324\n",
            "Loss: -14.41433048248291\n",
            "Loss: -14.048413276672363\n",
            "Loss: -13.675760269165039\n",
            "Loss: -13.301066398620605\n",
            "Loss: -12.939892768859863\n",
            "Loss: -12.563105583190918\n",
            "Loss: -12.21610164642334\n",
            "Loss: -11.886895179748535\n",
            "Loss: -11.555203437805176\n",
            "Loss: -11.25504207611084\n",
            "Loss: -10.922762870788574\n",
            "Loss: -10.607867240905762\n",
            "Loss: -10.304949760437012\n",
            "Loss: -9.975809097290039\n",
            "Loss: -9.704557418823242\n",
            "Loss: -9.413010597229004\n",
            "Loss: -9.185162544250488\n",
            "Loss: -8.95389461517334\n",
            "Loss: -8.719131469726562\n",
            "Loss: -8.51557445526123\n",
            "Loss: -8.317708969116211\n",
            "Loss: -8.105484008789062\n",
            "Loss: -7.890890121459961\n",
            "Loss: -7.64015531539917\n",
            "Loss: -7.4051055908203125\n",
            "Loss: -7.167328834533691\n",
            "Loss: -6.908352851867676\n",
            "Loss: -6.662269115447998\n",
            "Loss: -6.446915626525879\n",
            "Loss: -6.238460540771484\n",
            "Loss: -6.015620231628418\n",
            "Loss: -5.8022541999816895\n",
            "Loss: -5.614593982696533\n",
            "Loss: -5.415160655975342\n",
            "Loss: -5.259313583374023\n",
            "Loss: -5.120236396789551\n",
            "Loss: -4.973778247833252\n",
            "Loss: -4.8345441818237305\n",
            "Loss: -4.699962615966797\n",
            "Loss: -4.558065891265869\n",
            "Loss: -4.442336082458496\n",
            "Loss: -4.3322577476501465\n",
            "We fooled the network after 69 iterations!\n",
            "New prediction: 0\n",
            "14418img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -43.76652908325195\n",
            "Loss: -43.118526458740234\n",
            "Loss: -42.45328140258789\n",
            "Loss: -41.78831100463867\n",
            "Loss: -41.17377853393555\n",
            "Loss: -40.52769088745117\n",
            "Loss: -39.93034744262695\n",
            "Loss: -39.275691986083984\n",
            "Loss: -38.660518646240234\n",
            "Loss: -38.0207405090332\n",
            "Loss: -37.39013671875\n",
            "Loss: -36.7458610534668\n",
            "Loss: -36.096946716308594\n",
            "Loss: -35.45741653442383\n",
            "Loss: -34.78571319580078\n",
            "Loss: -34.11647415161133\n",
            "Loss: -33.50136184692383\n",
            "Loss: -32.85442352294922\n",
            "Loss: -32.229644775390625\n",
            "Loss: -31.59014320373535\n",
            "Loss: -30.969572067260742\n",
            "Loss: -30.332780838012695\n",
            "Loss: -29.668670654296875\n",
            "Loss: -29.038251876831055\n",
            "Loss: -28.377004623413086\n",
            "Loss: -27.73586082458496\n",
            "Loss: -27.085254669189453\n",
            "Loss: -26.440275192260742\n",
            "Loss: -25.80720329284668\n",
            "Loss: -25.165437698364258\n",
            "Loss: -24.53293800354004\n",
            "Loss: -23.90656852722168\n",
            "Loss: -23.290159225463867\n",
            "Loss: -22.690622329711914\n",
            "Loss: -22.078683853149414\n",
            "Loss: -21.491989135742188\n",
            "Loss: -20.863004684448242\n",
            "Loss: -20.318017959594727\n",
            "Loss: -19.820470809936523\n",
            "Loss: -19.317148208618164\n",
            "Loss: -18.821216583251953\n",
            "Loss: -18.34345054626465\n",
            "Loss: -17.892881393432617\n",
            "Loss: -17.418590545654297\n",
            "Loss: -17.056215286254883\n",
            "Loss: -16.659812927246094\n",
            "Loss: -16.289770126342773\n",
            "Loss: -15.9107027053833\n",
            "Loss: -15.544785499572754\n",
            "Loss: -15.163806915283203\n",
            "Loss: -14.802477836608887\n",
            "Loss: -14.439249992370605\n",
            "Loss: -14.0742826461792\n",
            "Loss: -13.708273887634277\n",
            "Loss: -13.354304313659668\n",
            "Loss: -13.018150329589844\n",
            "Loss: -12.684866905212402\n",
            "Loss: -12.365094184875488\n",
            "Loss: -12.049752235412598\n",
            "Loss: -11.724135398864746\n",
            "Loss: -11.402907371520996\n",
            "Loss: -11.05001163482666\n",
            "Loss: -10.692954063415527\n",
            "Loss: -10.358597755432129\n",
            "Loss: -10.041329383850098\n",
            "Loss: -9.735760688781738\n",
            "Loss: -9.4382905960083\n",
            "Loss: -9.14251708984375\n",
            "Loss: -8.86125659942627\n",
            "Loss: -8.564688682556152\n",
            "Loss: -8.283485412597656\n",
            "Loss: -7.986261367797852\n",
            "Loss: -7.709072113037109\n",
            "Loss: -7.447518348693848\n",
            "Loss: -7.18028450012207\n",
            "Loss: -6.93254280090332\n",
            "Loss: -6.680197715759277\n",
            "Loss: -6.414814472198486\n",
            "Loss: -6.174415588378906\n",
            "Loss: -5.932596206665039\n",
            "Loss: -5.686312198638916\n",
            "Loss: -5.451826095581055\n",
            "Loss: -5.24493408203125\n",
            "Loss: -5.025779724121094\n",
            "Loss: -4.803077697753906\n",
            "Loss: -4.585451126098633\n",
            "Loss: -4.3740105628967285\n",
            "Loss: -4.158698081970215\n",
            "Loss: -3.9697744846343994\n",
            "Loss: -3.752913236618042\n",
            "Loss: -3.5253188610076904\n",
            "Loss: -3.3040354251861572\n",
            "Loss: -3.0920026302337646\n",
            "Loss: -2.8770430088043213\n",
            "Loss: -2.689336061477661\n",
            "Loss: -2.4871432781219482\n",
            "Loss: -2.28861403465271\n",
            "Loss: -2.083818197250366\n",
            "Loss: -1.8926094770431519\n",
            "Loss: -1.710815191268921\n",
            "14421img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -18.20148277282715\n",
            "Loss: -17.565040588378906\n",
            "Loss: -16.917280197143555\n",
            "Loss: -16.3341064453125\n",
            "Loss: -15.784674644470215\n",
            "Loss: -15.281174659729004\n",
            "Loss: -14.777668952941895\n",
            "Loss: -14.286723136901855\n",
            "Loss: -13.781529426574707\n",
            "Loss: -13.236930847167969\n",
            "Loss: -12.69689655303955\n",
            "Loss: -12.156754493713379\n",
            "Loss: -11.625356674194336\n",
            "Loss: -11.110786437988281\n",
            "Loss: -10.617634773254395\n",
            "Loss: -10.15392780303955\n",
            "Loss: -9.709338188171387\n",
            "Loss: -9.260268211364746\n",
            "Loss: -8.820096015930176\n",
            "Loss: -8.488005638122559\n",
            "Loss: -8.126518249511719\n",
            "Loss: -7.78848934173584\n",
            "Loss: -7.447454452514648\n",
            "Loss: -7.096538066864014\n",
            "Loss: -6.773316383361816\n",
            "Loss: -6.414809226989746\n",
            "Loss: -6.078331470489502\n",
            "Loss: -5.713723182678223\n",
            "Loss: -5.404648780822754\n",
            "Loss: -5.104087829589844\n",
            "Loss: -4.713253974914551\n",
            "Loss: -4.346108436584473\n",
            "Loss: -4.012922763824463\n",
            "Loss: -3.7061879634857178\n",
            "Loss: -3.118783473968506\n",
            "Loss: -2.3952724933624268\n",
            "Loss: -1.8124783039093018\n",
            "We fooled the network after 36 iterations!\n",
            "New prediction: 4\n",
            "14423img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -40.6619758605957\n",
            "Loss: -39.840736389160156\n",
            "Loss: -38.96314239501953\n",
            "Loss: -38.095890045166016\n",
            "Loss: -37.23811340332031\n",
            "Loss: -36.36875534057617\n",
            "Loss: -35.522098541259766\n",
            "Loss: -34.686641693115234\n",
            "Loss: -33.86571502685547\n",
            "Loss: -33.02806091308594\n",
            "Loss: -32.209774017333984\n",
            "Loss: -31.38732147216797\n",
            "Loss: -30.57170867919922\n",
            "Loss: -29.74601173400879\n",
            "Loss: -28.930866241455078\n",
            "Loss: -28.11479377746582\n",
            "Loss: -27.296049118041992\n",
            "Loss: -26.507280349731445\n",
            "Loss: -25.685731887817383\n",
            "Loss: -24.86433982849121\n",
            "Loss: -24.064538955688477\n",
            "Loss: -23.26541519165039\n",
            "Loss: -22.472652435302734\n",
            "Loss: -21.64530372619629\n",
            "Loss: -20.846189498901367\n",
            "Loss: -20.17209815979004\n",
            "Loss: -19.4793758392334\n",
            "Loss: -18.783954620361328\n",
            "Loss: -18.029834747314453\n",
            "Loss: -17.29874610900879\n",
            "Loss: -16.560731887817383\n",
            "Loss: -15.84810733795166\n",
            "Loss: -15.1976957321167\n",
            "Loss: -14.589150428771973\n",
            "Loss: -13.984410285949707\n",
            "Loss: -13.403109550476074\n",
            "Loss: -12.819249153137207\n",
            "Loss: -12.184500694274902\n",
            "Loss: -11.561163902282715\n",
            "Loss: -10.965437889099121\n",
            "Loss: -10.350329399108887\n",
            "Loss: -9.767095565795898\n",
            "Loss: -9.213875770568848\n",
            "Loss: -8.650460243225098\n",
            "Loss: -8.086688041687012\n",
            "Loss: -7.554121971130371\n",
            "Loss: -7.129250526428223\n",
            "Loss: -6.624619483947754\n",
            "Loss: -6.121545791625977\n",
            "Loss: -5.515864849090576\n",
            "Loss: -4.928307056427002\n",
            "Loss: -4.3370256423950195\n",
            "Loss: -3.750173807144165\n",
            "We fooled the network after 52 iterations!\n",
            "New prediction: 5\n",
            "14437img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -25.794084548950195\n",
            "Loss: -25.186357498168945\n",
            "Loss: -24.566770553588867\n",
            "Loss: -23.932634353637695\n",
            "Loss: -23.33359146118164\n",
            "Loss: -22.71360969543457\n",
            "Loss: -22.09449005126953\n",
            "Loss: -21.493099212646484\n",
            "Loss: -20.91109275817871\n",
            "Loss: -20.30628776550293\n",
            "Loss: -19.709543228149414\n",
            "Loss: -19.218446731567383\n",
            "Loss: -18.726869583129883\n",
            "Loss: -18.22744369506836\n",
            "Loss: -17.73350715637207\n",
            "Loss: -17.240144729614258\n",
            "Loss: -16.77156639099121\n",
            "Loss: -16.293100357055664\n",
            "Loss: -15.82361125946045\n",
            "Loss: -15.34987735748291\n",
            "Loss: -14.877449035644531\n",
            "Loss: -14.42241382598877\n",
            "Loss: -13.95532512664795\n",
            "Loss: -13.483534812927246\n",
            "Loss: -13.018308639526367\n",
            "Loss: -12.57491683959961\n",
            "Loss: -12.152242660522461\n",
            "Loss: -11.733492851257324\n",
            "Loss: -11.329777717590332\n",
            "Loss: -10.93493938446045\n",
            "Loss: -10.533384323120117\n",
            "Loss: -10.144661903381348\n",
            "Loss: -9.774848937988281\n",
            "Loss: -9.367247581481934\n",
            "Loss: -8.979876518249512\n",
            "Loss: -8.617173194885254\n",
            "Loss: -8.246783256530762\n",
            "Loss: -7.862768173217773\n",
            "Loss: -7.512816429138184\n",
            "Loss: -7.13870096206665\n",
            "Loss: -6.762372970581055\n",
            "Loss: -6.401904582977295\n",
            "Loss: -6.0351457595825195\n",
            "Loss: -5.670313358306885\n",
            "Loss: -5.307412624359131\n",
            "Loss: -4.966118335723877\n",
            "Loss: -4.67209005355835\n",
            "Loss: -4.377886772155762\n",
            "Loss: -4.096955299377441\n",
            "Loss: -3.8242685794830322\n",
            "Loss: -3.5175812244415283\n",
            "Loss: -3.2225539684295654\n",
            "Loss: -2.9671971797943115\n",
            "Loss: -2.7595999240875244\n",
            "Loss: -2.5321452617645264\n",
            "Loss: -2.3027760982513428\n",
            "Loss: -2.062450408935547\n",
            "Loss: -1.847304344177246\n",
            "We fooled the network after 57 iterations!\n",
            "New prediction: 2\n",
            "14453img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -21.10007667541504\n",
            "Loss: -20.511476516723633\n",
            "Loss: -19.90627098083496\n",
            "Loss: -19.2965030670166\n",
            "Loss: -18.68846321105957\n",
            "Loss: -18.07576560974121\n",
            "Loss: -17.435462951660156\n",
            "Loss: -16.772367477416992\n",
            "Loss: -16.085405349731445\n",
            "Loss: -15.409796714782715\n",
            "Loss: -14.748445510864258\n",
            "Loss: -14.089634895324707\n",
            "Loss: -13.466643333435059\n",
            "Loss: -12.854344367980957\n",
            "Loss: -12.246306419372559\n",
            "Loss: -11.616255760192871\n",
            "Loss: -10.985709190368652\n",
            "Loss: -10.340666770935059\n",
            "Loss: -9.704985618591309\n",
            "Loss: -9.073479652404785\n",
            "Loss: -8.4813871383667\n",
            "Loss: -7.909670829772949\n",
            "Loss: -7.335098743438721\n",
            "Loss: -6.835614204406738\n",
            "Loss: -6.39571475982666\n",
            "Loss: -5.964055061340332\n",
            "Loss: -5.5323262214660645\n",
            "Loss: -5.116477012634277\n",
            "Loss: -4.794094562530518\n",
            "Loss: -4.4219651222229\n",
            "Loss: -4.129993438720703\n",
            "Loss: -3.8075215816497803\n",
            "Loss: -3.5549232959747314\n",
            "Loss: -3.299537181854248\n",
            "Loss: -3.0412850379943848\n",
            "Loss: -2.7914559841156006\n",
            "Loss: -2.521652936935425\n",
            "Loss: -2.248505115509033\n",
            "Loss: -1.9570132493972778\n",
            "Loss: -1.7155622243881226\n",
            "Loss: -1.46404230594635\n",
            "Loss: -1.188792109489441\n",
            "Loss: -0.8177215456962585\n",
            "Loss: -0.4587832987308502\n",
            "Loss: -0.19277280569076538\n",
            "Loss: 0.04552491754293442\n",
            "We fooled the network after 45 iterations!\n",
            "New prediction: 3\n",
            "14473img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -29.378583908081055\n",
            "Loss: -28.78311538696289\n",
            "Loss: -28.157217025756836\n",
            "Loss: -27.51953887939453\n",
            "Loss: -26.889684677124023\n",
            "Loss: -26.259166717529297\n",
            "Loss: -25.63313102722168\n",
            "Loss: -25.016008377075195\n",
            "Loss: -24.385568618774414\n",
            "Loss: -23.782270431518555\n",
            "Loss: -23.157878875732422\n",
            "Loss: -22.564146041870117\n",
            "Loss: -22.07819175720215\n",
            "Loss: -21.583911895751953\n",
            "Loss: -21.097183227539062\n",
            "Loss: -20.610273361206055\n",
            "Loss: -20.13059425354004\n",
            "Loss: -19.641016006469727\n",
            "Loss: -19.114763259887695\n",
            "Loss: -18.52984046936035\n",
            "Loss: -17.95854377746582\n",
            "Loss: -17.376867294311523\n",
            "Loss: -16.795011520385742\n",
            "Loss: -16.20808219909668\n",
            "Loss: -15.627030372619629\n",
            "Loss: -15.03171443939209\n",
            "Loss: -14.44315242767334\n",
            "Loss: -13.853096961975098\n",
            "Loss: -13.26058292388916\n",
            "Loss: -12.697614669799805\n",
            "Loss: -12.178182601928711\n",
            "Loss: -11.631047248840332\n",
            "Loss: -11.118042945861816\n",
            "Loss: -10.607012748718262\n",
            "Loss: -10.153386116027832\n",
            "Loss: -9.747167587280273\n",
            "Loss: -9.33316707611084\n",
            "Loss: -8.919292449951172\n",
            "Loss: -8.490635871887207\n",
            "Loss: -8.064518928527832\n",
            "Loss: -7.587835311889648\n",
            "Loss: -6.980125427246094\n",
            "Loss: -6.222166538238525\n",
            "Loss: -5.398248195648193\n",
            "Loss: -4.557884693145752\n",
            "Loss: -3.7342638969421387\n",
            "Loss: -2.93692946434021\n",
            "We fooled the network after 46 iterations!\n",
            "New prediction: 4\n",
            "14487img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -13.743969917297363\n",
            "Loss: -13.179952621459961\n",
            "Loss: -12.478938102722168\n",
            "Loss: -11.843921661376953\n",
            "Loss: -11.173723220825195\n",
            "Loss: -10.54053020477295\n",
            "Loss: -9.878893852233887\n",
            "Loss: -9.245996475219727\n",
            "Loss: -8.527848243713379\n",
            "Loss: -7.905865669250488\n",
            "Loss: -7.303256034851074\n",
            "Loss: -6.706328868865967\n",
            "Loss: -6.139985084533691\n",
            "Loss: -5.554781436920166\n",
            "Loss: -4.988831043243408\n",
            "Loss: -4.425657272338867\n",
            "Loss: -3.835702896118164\n",
            "Loss: -3.241563558578491\n",
            "We fooled the network after 17 iterations!\n",
            "New prediction: 5\n",
            "14492img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -8.589348793029785\n",
            "Loss: -8.070169448852539\n",
            "Loss: -7.49006462097168\n",
            "Loss: -6.890651702880859\n",
            "Loss: -6.362974643707275\n",
            "Loss: -5.892157554626465\n",
            "Loss: -5.453144550323486\n",
            "Loss: -5.047743797302246\n",
            "Loss: -4.6310296058654785\n",
            "Loss: -4.217414855957031\n",
            "Loss: -3.800158977508545\n",
            "Loss: -3.4210259914398193\n",
            "Loss: -3.0838828086853027\n",
            "Loss: -2.831284761428833\n",
            "Loss: -2.559554100036621\n",
            "Loss: -2.305542469024658\n",
            "Loss: -2.065070867538452\n",
            "Loss: -1.8750083446502686\n",
            "Loss: -1.6693576574325562\n",
            "Loss: -1.482602596282959\n",
            "Loss: -1.2850252389907837\n",
            "Loss: -1.0794882774353027\n",
            "Loss: -0.8871557116508484\n",
            "Loss: -0.6957687139511108\n",
            "Loss: -0.4796203076839447\n",
            "We fooled the network after 24 iterations!\n",
            "New prediction: 8\n",
            "14500img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -6.433041572570801\n",
            "Loss: -5.966913223266602\n",
            "Loss: -5.516697883605957\n",
            "Loss: -5.100322723388672\n",
            "Loss: -4.6886210441589355\n",
            "Loss: -4.300877094268799\n",
            "Loss: -3.926973581314087\n",
            "Loss: -3.5578324794769287\n",
            "Loss: -3.1490328311920166\n",
            "Loss: -2.765958070755005\n",
            "Loss: -2.4130446910858154\n",
            "We fooled the network after 10 iterations!\n",
            "New prediction: 8\n",
            "14524img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -36.6929931640625\n",
            "Loss: -36.043556213378906\n",
            "Loss: -35.36117172241211\n",
            "Loss: -34.69529342651367\n",
            "Loss: -34.02827835083008\n",
            "Loss: -33.363948822021484\n",
            "Loss: -32.71992874145508\n",
            "Loss: -32.072933197021484\n",
            "Loss: -31.436952590942383\n",
            "Loss: -30.786895751953125\n",
            "Loss: -30.140676498413086\n",
            "Loss: -29.513683319091797\n",
            "Loss: -28.913026809692383\n",
            "Loss: -28.290739059448242\n",
            "Loss: -27.68617820739746\n",
            "Loss: -27.07669448852539\n",
            "Loss: -26.455759048461914\n",
            "Loss: -25.850143432617188\n",
            "Loss: -25.225873947143555\n",
            "Loss: -24.615665435791016\n",
            "Loss: -23.99728775024414\n",
            "Loss: -23.38750648498535\n",
            "Loss: -22.895410537719727\n",
            "Loss: -22.40952491760254\n",
            "Loss: -21.916900634765625\n",
            "Loss: -21.442516326904297\n",
            "Loss: -20.962121963500977\n",
            "Loss: -20.485021591186523\n",
            "Loss: -19.99506950378418\n",
            "Loss: -19.530797958374023\n",
            "Loss: -19.060115814208984\n",
            "Loss: -18.607751846313477\n",
            "Loss: -18.1087589263916\n",
            "Loss: -17.60296630859375\n",
            "Loss: -17.11817741394043\n",
            "Loss: -16.609046936035156\n",
            "Loss: -16.098894119262695\n",
            "Loss: -15.618180274963379\n",
            "Loss: -15.133352279663086\n",
            "Loss: -14.663703918457031\n",
            "Loss: -14.12351131439209\n",
            "Loss: -13.598920822143555\n",
            "Loss: -13.080815315246582\n",
            "Loss: -12.552702903747559\n",
            "Loss: -12.022439002990723\n",
            "Loss: -11.4900541305542\n",
            "Loss: -10.982413291931152\n",
            "Loss: -10.446043968200684\n",
            "Loss: -9.920924186706543\n",
            "Loss: -9.388123512268066\n",
            "Loss: -8.889098167419434\n",
            "Loss: -8.440852165222168\n",
            "Loss: -8.005995750427246\n",
            "Loss: -7.559176445007324\n",
            "Loss: -7.238800525665283\n",
            "Loss: -6.907835006713867\n",
            "Loss: -6.562527179718018\n",
            "Loss: -6.218085289001465\n",
            "Loss: -5.888397216796875\n",
            "Loss: -5.571418285369873\n",
            "Loss: -5.259766578674316\n",
            "Loss: -4.992773056030273\n",
            "Loss: -4.729046821594238\n",
            "Loss: -4.474387168884277\n",
            "Loss: -4.221188545227051\n",
            "Loss: -3.9701950550079346\n",
            "Loss: -3.741375684738159\n",
            "Loss: -3.492525815963745\n",
            "Loss: -3.246121406555176\n",
            "Loss: -3.0162301063537598\n",
            "Loss: -2.6675922870635986\n",
            "Loss: -2.3172998428344727\n",
            "Loss: -2.063340425491333\n",
            "Loss: -1.782252550125122\n",
            "We fooled the network after 73 iterations!\n",
            "New prediction: 0\n",
            "14525img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -21.3544979095459\n",
            "Loss: -20.82124137878418\n",
            "Loss: -20.252216339111328\n",
            "Loss: -19.674102783203125\n",
            "Loss: -19.0871524810791\n",
            "Loss: -18.515878677368164\n",
            "Loss: -17.953187942504883\n",
            "Loss: -17.392290115356445\n",
            "Loss: -16.852048873901367\n",
            "Loss: -16.311420440673828\n",
            "Loss: -15.791420936584473\n",
            "Loss: -15.30025863647461\n",
            "Loss: -14.765443801879883\n",
            "Loss: -14.273768424987793\n",
            "Loss: -13.860810279846191\n",
            "Loss: -13.465499877929688\n",
            "Loss: -13.124987602233887\n",
            "Loss: -12.778874397277832\n",
            "Loss: -12.448945045471191\n",
            "Loss: -12.122688293457031\n",
            "Loss: -11.78661823272705\n",
            "Loss: -11.463138580322266\n",
            "Loss: -11.109169006347656\n",
            "Loss: -10.770112991333008\n",
            "Loss: -10.429784774780273\n",
            "Loss: -10.092305183410645\n",
            "Loss: -9.76311206817627\n",
            "Loss: -9.433087348937988\n",
            "Loss: -9.115200996398926\n",
            "Loss: -8.77748966217041\n",
            "Loss: -8.464482307434082\n",
            "Loss: -8.14501953125\n",
            "Loss: -7.8050127029418945\n",
            "Loss: -7.478295803070068\n",
            "Loss: -7.165867805480957\n",
            "Loss: -6.8388214111328125\n",
            "Loss: -6.521594524383545\n",
            "Loss: -6.207167625427246\n",
            "Loss: -5.897024631500244\n",
            "Loss: -5.58320426940918\n",
            "Loss: -5.259175777435303\n",
            "Loss: -4.958103179931641\n",
            "Loss: -4.694530487060547\n",
            "Loss: -4.460991859436035\n",
            "Loss: -4.2437744140625\n",
            "Loss: -4.020351409912109\n",
            "Loss: -3.813124895095825\n",
            "Loss: -3.6033663749694824\n",
            "Loss: -3.3993382453918457\n",
            "Loss: -3.1863067150115967\n",
            "Loss: -2.9427330493927\n",
            "Loss: -2.716282367706299\n",
            "Loss: -2.4240853786468506\n",
            "We fooled the network after 52 iterations!\n",
            "New prediction: 8\n",
            "14526img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -43.171966552734375\n",
            "Loss: -42.59375\n",
            "Loss: -42.00127029418945\n",
            "Loss: -41.402557373046875\n",
            "Loss: -40.806026458740234\n",
            "Loss: -40.2060661315918\n",
            "Loss: -39.62056350708008\n",
            "Loss: -39.034549713134766\n",
            "Loss: -38.44570541381836\n",
            "Loss: -37.8700065612793\n",
            "Loss: -37.28841781616211\n",
            "Loss: -36.71284866333008\n",
            "Loss: -36.13604736328125\n",
            "Loss: -35.55460739135742\n",
            "Loss: -34.99837112426758\n",
            "Loss: -34.41886520385742\n",
            "Loss: -33.850643157958984\n",
            "Loss: -33.29302215576172\n",
            "Loss: -32.727779388427734\n",
            "Loss: -32.164669036865234\n",
            "Loss: -31.603464126586914\n",
            "Loss: -31.04515838623047\n",
            "Loss: -30.478370666503906\n",
            "Loss: -29.933923721313477\n",
            "Loss: -29.33159065246582\n",
            "Loss: -28.715673446655273\n",
            "Loss: -28.096105575561523\n",
            "Loss: -27.45029640197754\n",
            "Loss: -26.799482345581055\n",
            "Loss: -26.132524490356445\n",
            "Loss: -25.43732261657715\n",
            "Loss: -24.725101470947266\n",
            "Loss: -23.970788955688477\n",
            "Loss: -23.256290435791016\n",
            "Loss: -22.5090389251709\n",
            "Loss: -21.795717239379883\n",
            "Loss: -21.08336067199707\n",
            "Loss: -20.348888397216797\n",
            "Loss: -19.596891403198242\n",
            "Loss: -18.851232528686523\n",
            "Loss: -18.10879135131836\n",
            "Loss: -17.269237518310547\n",
            "Loss: -16.39204216003418\n",
            "Loss: -15.544028282165527\n",
            "Loss: -14.556477546691895\n",
            "Loss: -13.654831886291504\n",
            "Loss: -12.687134742736816\n",
            "Loss: -11.800213813781738\n",
            "Loss: -10.986303329467773\n",
            "Loss: -10.120109558105469\n",
            "Loss: -9.288870811462402\n",
            "Loss: -8.467923164367676\n",
            "Loss: -7.634604454040527\n",
            "Loss: -6.832780838012695\n",
            "Loss: -6.04224967956543\n",
            "Loss: -5.2539777755737305\n",
            "Loss: -4.518494606018066\n",
            "Loss: -3.80875301361084\n",
            "Loss: -3.1092188358306885\n",
            "Loss: -2.578697919845581\n",
            "Loss: -2.0505714416503906\n",
            "Loss: -1.5700145959854126\n",
            "Loss: -1.1209851503372192\n",
            "We fooled the network after 62 iterations!\n",
            "New prediction: 5\n",
            "14565img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -29.598241806030273\n",
            "Loss: -29.07014274597168\n",
            "Loss: -28.509557723999023\n",
            "Loss: -27.912599563598633\n",
            "Loss: -27.30503273010254\n",
            "Loss: -26.694400787353516\n",
            "Loss: -26.100141525268555\n",
            "Loss: -25.491064071655273\n",
            "Loss: -24.88646697998047\n",
            "Loss: -24.28419303894043\n",
            "Loss: -23.67940902709961\n",
            "Loss: -23.076841354370117\n",
            "Loss: -22.474056243896484\n",
            "Loss: -21.875951766967773\n",
            "Loss: -21.29953956604004\n",
            "Loss: -20.697431564331055\n",
            "Loss: -20.084716796875\n",
            "Loss: -19.46924591064453\n",
            "Loss: -18.804208755493164\n",
            "Loss: -18.265554428100586\n",
            "Loss: -17.703353881835938\n",
            "Loss: -17.161346435546875\n",
            "Loss: -16.621318817138672\n",
            "Loss: -16.110570907592773\n",
            "Loss: -15.630485534667969\n",
            "Loss: -15.163642883300781\n",
            "Loss: -14.675726890563965\n",
            "Loss: -14.210532188415527\n",
            "Loss: -13.761662483215332\n",
            "Loss: -13.31533145904541\n",
            "Loss: -12.86904239654541\n",
            "Loss: -12.4496488571167\n",
            "Loss: -12.032050132751465\n",
            "Loss: -11.604452133178711\n",
            "Loss: -11.177168846130371\n",
            "Loss: -10.75633716583252\n",
            "Loss: -10.346251487731934\n",
            "Loss: -9.955755233764648\n",
            "Loss: -9.56893253326416\n",
            "Loss: -9.181670188903809\n",
            "Loss: -8.7886962890625\n",
            "Loss: -8.397915840148926\n",
            "Loss: -8.000621795654297\n",
            "Loss: -7.616350173950195\n",
            "Loss: -7.229459762573242\n",
            "Loss: -6.843876838684082\n",
            "Loss: -6.482684135437012\n",
            "Loss: -6.103681564331055\n",
            "Loss: -5.738256454467773\n",
            "Loss: -5.3929123878479\n",
            "Loss: -5.055583953857422\n",
            "Loss: -4.703022003173828\n",
            "Loss: -4.341334342956543\n",
            "Loss: -3.996591806411743\n",
            "Loss: -3.669058322906494\n",
            "Loss: -3.4189674854278564\n",
            "Loss: -3.184235095977783\n",
            "Loss: -2.949634313583374\n",
            "Loss: -2.7225072383880615\n",
            "Loss: -2.4760582447052\n",
            "Loss: -2.2195093631744385\n",
            "Loss: -1.9533203840255737\n",
            "Loss: -1.6883114576339722\n",
            "Loss: -1.4079755544662476\n",
            "We fooled the network after 63 iterations!\n",
            "New prediction: 8\n",
            "14570img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -29.222009658813477\n",
            "Loss: -28.58856964111328\n",
            "Loss: -27.93250846862793\n",
            "Loss: -27.271604537963867\n",
            "Loss: -26.62214469909668\n",
            "Loss: -25.97897720336914\n",
            "Loss: -25.328189849853516\n",
            "Loss: -24.70524787902832\n",
            "Loss: -24.096548080444336\n",
            "Loss: -23.51429557800293\n",
            "Loss: -23.004613876342773\n",
            "Loss: -22.513933181762695\n",
            "Loss: -22.013887405395508\n",
            "Loss: -21.51645851135254\n",
            "Loss: -21.010759353637695\n",
            "Loss: -20.508543014526367\n",
            "Loss: -20.012826919555664\n",
            "Loss: -19.517126083374023\n",
            "Loss: -19.02992820739746\n",
            "Loss: -18.544496536254883\n",
            "Loss: -18.177000045776367\n",
            "Loss: -17.811626434326172\n",
            "Loss: -17.43708038330078\n",
            "Loss: -17.063085556030273\n",
            "Loss: -16.69087791442871\n",
            "Loss: -16.324525833129883\n",
            "Loss: -15.968073844909668\n",
            "Loss: -15.629704475402832\n",
            "Loss: -15.29619312286377\n",
            "Loss: -14.940054893493652\n",
            "Loss: -14.604329109191895\n",
            "Loss: -14.265727043151855\n",
            "Loss: -13.938982963562012\n",
            "Loss: -13.611647605895996\n",
            "Loss: -13.258856773376465\n",
            "Loss: -12.878691673278809\n",
            "Loss: -12.478262901306152\n",
            "Loss: -12.127121925354004\n",
            "Loss: -11.867064476013184\n",
            "Loss: -11.614106178283691\n",
            "Loss: -11.369391441345215\n",
            "Loss: -11.122328758239746\n",
            "Loss: -10.87331485748291\n",
            "Loss: -10.638391494750977\n",
            "Loss: -10.389902114868164\n",
            "Loss: -10.136574745178223\n",
            "Loss: -9.886259078979492\n",
            "Loss: -9.642571449279785\n",
            "Loss: -9.374242782592773\n",
            "Loss: -9.064160346984863\n",
            "Loss: -8.78915023803711\n",
            "Loss: -8.52345085144043\n",
            "Loss: -8.27682876586914\n",
            "Loss: -8.019683837890625\n",
            "Loss: -7.733165264129639\n",
            "Loss: -7.468049049377441\n",
            "Loss: -7.205854415893555\n",
            "Loss: -6.944040775299072\n",
            "Loss: -6.688802719116211\n",
            "Loss: -6.434910297393799\n",
            "Loss: -6.17172384262085\n",
            "Loss: -5.917634963989258\n",
            "Loss: -5.661973476409912\n",
            "Loss: -5.4073333740234375\n",
            "Loss: -5.201174736022949\n",
            "Loss: -4.9758453369140625\n",
            "Loss: -4.766753196716309\n",
            "Loss: -4.6059956550598145\n",
            "Loss: -4.395362854003906\n",
            "Loss: -4.2169389724731445\n",
            "Loss: -4.039501190185547\n",
            "Loss: -3.8393890857696533\n",
            "Loss: -3.6477725505828857\n",
            "Loss: -3.471993923187256\n",
            "Loss: -3.281855583190918\n",
            "Loss: -3.123886823654175\n",
            "Loss: -2.978834390640259\n",
            "Loss: -2.8183043003082275\n",
            "Loss: -2.639328956604004\n",
            "Loss: -2.461199998855591\n",
            "Loss: -2.3069818019866943\n",
            "We fooled the network after 80 iterations!\n",
            "New prediction: 8\n",
            "14576img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -28.971914291381836\n",
            "Loss: -28.29876708984375\n",
            "Loss: -27.58841323852539\n",
            "Loss: -26.863744735717773\n",
            "Loss: -26.16315269470215\n",
            "Loss: -25.48020362854004\n",
            "Loss: -24.807785034179688\n",
            "Loss: -24.136945724487305\n",
            "Loss: -23.452552795410156\n",
            "Loss: -22.7712345123291\n",
            "Loss: -22.089656829833984\n",
            "Loss: -21.430124282836914\n",
            "Loss: -20.794191360473633\n",
            "Loss: -20.17890167236328\n",
            "Loss: -19.579805374145508\n",
            "Loss: -19.10897445678711\n",
            "Loss: -18.630077362060547\n",
            "Loss: -18.14589500427246\n",
            "Loss: -17.68663787841797\n",
            "Loss: -17.24171257019043\n",
            "Loss: -16.794748306274414\n",
            "Loss: -16.347566604614258\n",
            "Loss: -15.914490699768066\n",
            "Loss: -15.50412368774414\n",
            "Loss: -15.108464241027832\n",
            "Loss: -14.720023155212402\n",
            "Loss: -14.338793754577637\n",
            "Loss: -13.971222877502441\n",
            "Loss: -13.624016761779785\n",
            "Loss: -13.271012306213379\n",
            "Loss: -12.910426139831543\n",
            "Loss: -12.588624954223633\n",
            "Loss: -12.235075950622559\n",
            "Loss: -11.894491195678711\n",
            "Loss: -11.565873146057129\n",
            "Loss: -11.229056358337402\n",
            "Loss: -10.875785827636719\n",
            "Loss: -10.553874015808105\n",
            "Loss: -10.191710472106934\n",
            "Loss: -9.887417793273926\n",
            "Loss: -9.587560653686523\n",
            "Loss: -9.306309700012207\n",
            "Loss: -9.006293296813965\n",
            "Loss: -8.720755577087402\n",
            "Loss: -8.456734657287598\n",
            "Loss: -8.168658256530762\n",
            "Loss: -7.879864692687988\n",
            "Loss: -7.618719577789307\n",
            "Loss: -7.344130516052246\n",
            "Loss: -7.067873477935791\n",
            "Loss: -6.812660217285156\n",
            "Loss: -6.54472541809082\n",
            "Loss: -6.258846282958984\n",
            "Loss: -5.9677839279174805\n",
            "Loss: -5.687680244445801\n",
            "Loss: -5.435359954833984\n",
            "Loss: -5.176533222198486\n",
            "Loss: -4.939988136291504\n",
            "Loss: -4.693462371826172\n",
            "Loss: -4.500453948974609\n",
            "Loss: -4.259209632873535\n",
            "Loss: -4.039229869842529\n",
            "Loss: -3.842188835144043\n",
            "Loss: -3.61344051361084\n",
            "Loss: -3.304119348526001\n",
            "Loss: -3.0107741355895996\n",
            "Loss: -2.749459743499756\n",
            "Loss: -2.398939847946167\n",
            "Loss: -2.041633367538452\n",
            "Loss: -1.6414387226104736\n",
            "Loss: -1.3059425354003906\n",
            "Loss: -0.9458962082862854\n",
            "We fooled the network after 71 iterations!\n",
            "New prediction: 8\n",
            "14583img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -27.174772262573242\n",
            "Loss: -26.439191818237305\n",
            "Loss: -25.652484893798828\n",
            "Loss: -24.854158401489258\n",
            "Loss: -24.056211471557617\n",
            "Loss: -23.273481369018555\n",
            "Loss: -22.46280860900879\n",
            "Loss: -21.71147346496582\n",
            "Loss: -20.986486434936523\n",
            "Loss: -20.23008918762207\n",
            "Loss: -19.467802047729492\n",
            "Loss: -18.74689483642578\n",
            "Loss: -18.030351638793945\n",
            "Loss: -17.305017471313477\n",
            "Loss: -16.60995101928711\n",
            "Loss: -15.927533149719238\n",
            "Loss: -15.225117683410645\n",
            "Loss: -14.525883674621582\n",
            "Loss: -13.847859382629395\n",
            "Loss: -13.197628021240234\n",
            "Loss: -12.573598861694336\n",
            "Loss: -11.885096549987793\n",
            "Loss: -11.226075172424316\n",
            "Loss: -10.531896591186523\n",
            "Loss: -9.857462882995605\n",
            "Loss: -9.250060081481934\n",
            "Loss: -8.65457534790039\n",
            "Loss: -8.066141128540039\n",
            "Loss: -7.505504608154297\n",
            "Loss: -6.922162055969238\n",
            "Loss: -6.35139274597168\n",
            "Loss: -5.765673637390137\n",
            "Loss: -5.229644775390625\n",
            "Loss: -4.59814453125\n",
            "Loss: -3.9580633640289307\n",
            "Loss: -3.3476173877716064\n",
            "Loss: -2.9577810764312744\n",
            "Loss: -2.547560691833496\n",
            "Loss: -2.1547746658325195\n",
            "Loss: -1.8148223161697388\n",
            "Loss: -1.5428825616836548\n",
            "Loss: -1.3522042036056519\n",
            "Loss: -1.1458549499511719\n",
            "We fooled the network after 42 iterations!\n",
            "New prediction: 8\n",
            "14593img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -8.470885276794434\n",
            "Loss: -8.068170547485352\n",
            "Loss: -7.6452155113220215\n",
            "Loss: -7.232654571533203\n",
            "Loss: -6.864988803863525\n",
            "Loss: -6.502285957336426\n",
            "Loss: -6.151108741760254\n",
            "Loss: -5.876730442047119\n",
            "Loss: -5.630832672119141\n",
            "Loss: -5.370312213897705\n",
            "Loss: -5.112761974334717\n",
            "We fooled the network after 10 iterations!\n",
            "New prediction: 8\n",
            "14604img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -28.051361083984375\n",
            "Loss: -27.516788482666016\n",
            "Loss: -26.985149383544922\n",
            "Loss: -26.45333480834961\n",
            "Loss: -25.920766830444336\n",
            "Loss: -25.386520385742188\n",
            "Loss: -24.86711311340332\n",
            "Loss: -24.35419273376465\n",
            "Loss: -23.896465301513672\n",
            "Loss: -23.44742202758789\n",
            "Loss: -23.009967803955078\n",
            "Loss: -22.57218360900879\n",
            "Loss: -22.15382194519043\n",
            "Loss: -21.731855392456055\n",
            "Loss: -21.295679092407227\n",
            "Loss: -20.861154556274414\n",
            "Loss: -20.433259963989258\n",
            "Loss: -20.093210220336914\n",
            "Loss: -19.736669540405273\n",
            "Loss: -19.388324737548828\n",
            "Loss: -19.041366577148438\n",
            "Loss: -18.694637298583984\n",
            "Loss: -18.3538818359375\n",
            "Loss: -18.023561477661133\n",
            "Loss: -17.69205665588379\n",
            "Loss: -17.35152816772461\n",
            "Loss: -17.02241325378418\n",
            "Loss: -16.69751739501953\n",
            "Loss: -16.37177085876465\n",
            "Loss: -16.059261322021484\n",
            "Loss: -15.745329856872559\n",
            "Loss: -15.4306001663208\n",
            "Loss: -15.12496280670166\n",
            "Loss: -14.812439918518066\n",
            "Loss: -14.523261070251465\n",
            "Loss: -14.22411823272705\n",
            "Loss: -13.921833992004395\n",
            "Loss: -13.639131546020508\n",
            "Loss: -13.343445777893066\n",
            "Loss: -13.062981605529785\n",
            "Loss: -12.782541275024414\n",
            "Loss: -12.505570411682129\n",
            "Loss: -12.224105834960938\n",
            "Loss: -11.947418212890625\n",
            "Loss: -11.683693885803223\n",
            "Loss: -11.42502212524414\n",
            "Loss: -11.160981178283691\n",
            "Loss: -10.903401374816895\n",
            "Loss: -10.650559425354004\n",
            "Loss: -10.388167381286621\n",
            "Loss: -10.125443458557129\n",
            "Loss: -9.869133949279785\n",
            "Loss: -9.60995101928711\n",
            "Loss: -9.348464012145996\n",
            "Loss: -9.093138694763184\n",
            "Loss: -8.82713794708252\n",
            "Loss: -8.569880485534668\n",
            "Loss: -8.319626808166504\n",
            "Loss: -8.062333106994629\n",
            "Loss: -7.81476354598999\n",
            "Loss: -7.552770614624023\n",
            "Loss: -7.277899742126465\n",
            "Loss: -6.996283054351807\n",
            "Loss: -6.741199016571045\n",
            "Loss: -6.486056327819824\n",
            "Loss: -6.241517066955566\n",
            "Loss: -5.990085601806641\n",
            "Loss: -5.741625785827637\n",
            "Loss: -5.500431060791016\n",
            "Loss: -5.2577290534973145\n",
            "Loss: -5.003017425537109\n",
            "Loss: -4.768098831176758\n",
            "Loss: -4.550655364990234\n",
            "Loss: -4.367309093475342\n",
            "Loss: -4.172117233276367\n",
            "Loss: -3.9971764087677\n",
            "Loss: -3.8060271739959717\n",
            "Loss: -3.6654090881347656\n",
            "Loss: -3.5187785625457764\n",
            "Loss: -3.3701019287109375\n",
            "Loss: -3.2308223247528076\n",
            "Loss: -3.109496593475342\n",
            "Loss: -2.980100154876709\n",
            "Loss: -2.851043939590454\n",
            "Loss: -2.7206790447235107\n",
            "Loss: -2.6000630855560303\n",
            "Loss: -2.4656388759613037\n",
            "Loss: -2.334911823272705\n",
            "Loss: -2.1350395679473877\n",
            "Loss: -1.89203679561615\n",
            "Loss: -1.6883991956710815\n",
            "Loss: -1.4603221416473389\n",
            "Loss: -1.2234982252120972\n",
            "Loss: -1.011603832244873\n",
            "We fooled the network after 93 iterations!\n",
            "New prediction: 5\n",
            "14608img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -41.339256286621094\n",
            "Loss: -40.65740203857422\n",
            "Loss: -39.9589958190918\n",
            "Loss: -39.2285270690918\n",
            "Loss: -38.54971694946289\n",
            "Loss: -37.87494659423828\n",
            "Loss: -37.20695877075195\n",
            "Loss: -36.54763412475586\n",
            "Loss: -35.90326690673828\n",
            "Loss: -35.25999069213867\n",
            "Loss: -34.56768035888672\n",
            "Loss: -33.894832611083984\n",
            "Loss: -33.248233795166016\n",
            "Loss: -32.59400177001953\n",
            "Loss: -31.946203231811523\n",
            "Loss: -31.311933517456055\n",
            "Loss: -30.676347732543945\n",
            "Loss: -30.034954071044922\n",
            "Loss: -29.392900466918945\n",
            "Loss: -28.759191513061523\n",
            "Loss: -28.135988235473633\n",
            "Loss: -27.499540328979492\n",
            "Loss: -26.883935928344727\n",
            "Loss: -26.26280403137207\n",
            "Loss: -25.6086368560791\n",
            "Loss: -25.00238609313965\n",
            "Loss: -24.369232177734375\n",
            "Loss: -23.748111724853516\n",
            "Loss: -23.142934799194336\n",
            "Loss: -22.550662994384766\n",
            "Loss: -22.0459041595459\n",
            "Loss: -21.520872116088867\n",
            "Loss: -20.996896743774414\n",
            "Loss: -20.54195213317871\n",
            "Loss: -20.09215545654297\n",
            "Loss: -19.642255783081055\n",
            "Loss: -19.17935562133789\n",
            "Loss: -18.7338809967041\n",
            "Loss: -18.28071403503418\n",
            "Loss: -17.850278854370117\n",
            "Loss: -17.397836685180664\n",
            "Loss: -16.938922882080078\n",
            "Loss: -16.50802993774414\n",
            "Loss: -16.052289962768555\n",
            "Loss: -15.608115196228027\n",
            "Loss: -15.166471481323242\n",
            "Loss: -14.733685493469238\n",
            "Loss: -14.312254905700684\n",
            "Loss: -13.887162208557129\n",
            "Loss: -13.485877990722656\n",
            "Loss: -13.105923652648926\n",
            "Loss: -12.703688621520996\n",
            "Loss: -12.307950019836426\n",
            "Loss: -11.929547309875488\n",
            "Loss: -11.545059204101562\n",
            "Loss: -11.153495788574219\n",
            "Loss: -10.750235557556152\n",
            "Loss: -10.364940643310547\n",
            "Loss: -9.973525047302246\n",
            "Loss: -9.668696403503418\n",
            "Loss: -9.386757850646973\n",
            "Loss: -9.076534271240234\n",
            "Loss: -8.787092208862305\n",
            "Loss: -8.494856834411621\n",
            "Loss: -8.190163612365723\n",
            "Loss: -7.896876335144043\n",
            "Loss: -7.579953193664551\n",
            "Loss: -7.272810935974121\n",
            "Loss: -6.960140705108643\n",
            "Loss: -6.646112442016602\n",
            "Loss: -6.36370849609375\n",
            "Loss: -6.053386211395264\n",
            "Loss: -5.698993682861328\n",
            "Loss: -5.333648681640625\n",
            "Loss: -4.99235725402832\n",
            "Loss: -4.6406755447387695\n",
            "Loss: -4.296806812286377\n",
            "Loss: -3.9696590900421143\n",
            "Loss: -3.747770071029663\n",
            "Loss: -3.526899576187134\n",
            "We fooled the network after 79 iterations!\n",
            "New prediction: 8\n",
            "14610img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -24.932180404663086\n",
            "Loss: -24.167959213256836\n",
            "Loss: -23.33525848388672\n",
            "Loss: -22.484235763549805\n",
            "Loss: -21.70125961303711\n",
            "Loss: -20.91383171081543\n",
            "Loss: -20.178102493286133\n",
            "Loss: -19.46153450012207\n",
            "Loss: -18.718473434448242\n",
            "Loss: -18.015474319458008\n",
            "Loss: -17.469573974609375\n",
            "Loss: -16.884889602661133\n",
            "Loss: -16.295534133911133\n",
            "Loss: -15.7285795211792\n",
            "Loss: -15.1680326461792\n",
            "Loss: -14.569042205810547\n",
            "Loss: -13.951308250427246\n",
            "Loss: -13.2854642868042\n",
            "Loss: -12.687588691711426\n",
            "Loss: -12.036271095275879\n",
            "Loss: -11.400402069091797\n",
            "Loss: -10.766642570495605\n",
            "Loss: -10.201544761657715\n",
            "Loss: -9.617862701416016\n",
            "Loss: -9.02277946472168\n",
            "Loss: -8.423750877380371\n",
            "Loss: -7.74824333190918\n",
            "Loss: -7.108737945556641\n",
            "Loss: -6.665482997894287\n",
            "Loss: -6.219005107879639\n",
            "We fooled the network after 29 iterations!\n",
            "New prediction: 5\n",
            "14618img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -14.609213829040527\n",
            "Loss: -13.97155475616455\n",
            "Loss: -13.299830436706543\n",
            "Loss: -12.623878479003906\n",
            "Loss: -11.966769218444824\n",
            "Loss: -11.395495414733887\n",
            "Loss: -10.844780921936035\n",
            "Loss: -10.301146507263184\n",
            "Loss: -9.771787643432617\n",
            "Loss: -9.24024486541748\n",
            "Loss: -8.724875450134277\n",
            "Loss: -8.22206974029541\n",
            "Loss: -7.787275314331055\n",
            "Loss: -7.316206932067871\n",
            "Loss: -6.861081123352051\n",
            "Loss: -6.425950050354004\n",
            "Loss: -5.9772796630859375\n",
            "Loss: -5.564428329467773\n",
            "Loss: -5.119646072387695\n",
            "Loss: -4.724024772644043\n",
            "Loss: -4.399308681488037\n",
            "Loss: -4.080286026000977\n",
            "Loss: -3.7793467044830322\n",
            "Loss: -3.448369026184082\n",
            "Loss: -3.166388750076294\n",
            "Loss: -2.8660171031951904\n",
            "We fooled the network after 25 iterations!\n",
            "New prediction: 8\n",
            "14632img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -8.849496841430664\n",
            "Loss: -8.29154109954834\n",
            "Loss: -7.705211639404297\n",
            "Loss: -7.160888671875\n",
            "Loss: -6.627874851226807\n",
            "Loss: -6.09101676940918\n",
            "Loss: -5.581670761108398\n",
            "Loss: -5.089576721191406\n",
            "Loss: -4.63311767578125\n",
            "Loss: -4.192122459411621\n",
            "Loss: -3.757103681564331\n",
            "We fooled the network after 10 iterations!\n",
            "New prediction: 8\n",
            "14634img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -24.024538040161133\n",
            "Loss: -23.48682975769043\n",
            "Loss: -22.9074764251709\n",
            "Loss: -22.34002113342285\n",
            "Loss: -21.796329498291016\n",
            "Loss: -21.264677047729492\n",
            "Loss: -20.734384536743164\n",
            "Loss: -20.22300910949707\n",
            "Loss: -19.712507247924805\n",
            "Loss: -19.177749633789062\n",
            "Loss: -18.62770652770996\n",
            "Loss: -18.076547622680664\n",
            "Loss: -17.529056549072266\n",
            "Loss: -16.987302780151367\n",
            "Loss: -16.47263526916504\n",
            "Loss: -15.970656394958496\n",
            "Loss: -15.480212211608887\n",
            "Loss: -14.989290237426758\n",
            "Loss: -14.48281192779541\n",
            "Loss: -13.991545677185059\n",
            "Loss: -13.483935356140137\n",
            "Loss: -12.991873741149902\n",
            "Loss: -12.510327339172363\n",
            "Loss: -12.050657272338867\n",
            "Loss: -11.61237907409668\n",
            "Loss: -11.157027244567871\n",
            "Loss: -10.73481273651123\n",
            "Loss: -10.29631233215332\n",
            "Loss: -9.839192390441895\n",
            "Loss: -9.389758110046387\n",
            "Loss: -8.955594062805176\n",
            "Loss: -8.511373519897461\n",
            "Loss: -8.086721420288086\n",
            "Loss: -7.668488502502441\n",
            "Loss: -7.256572246551514\n",
            "Loss: -6.757482051849365\n",
            "Loss: -6.2516093254089355\n",
            "Loss: -5.763125419616699\n",
            "Loss: -5.256556987762451\n",
            "Loss: -4.80328893661499\n",
            "Loss: -4.268518447875977\n",
            "Loss: -3.6804001331329346\n",
            "Loss: -3.108189105987549\n",
            "Loss: -2.5673389434814453\n",
            "We fooled the network after 43 iterations!\n",
            "New prediction: 4\n",
            "14641img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -22.0002384185791\n",
            "Loss: -21.4798583984375\n",
            "Loss: -20.946861267089844\n",
            "Loss: -20.399658203125\n",
            "Loss: -19.85789680480957\n",
            "Loss: -19.322280883789062\n",
            "Loss: -18.774106979370117\n",
            "Loss: -18.237497329711914\n",
            "Loss: -17.683412551879883\n",
            "Loss: -17.15351104736328\n",
            "Loss: -16.627182006835938\n",
            "Loss: -16.092954635620117\n",
            "Loss: -15.56204891204834\n",
            "Loss: -15.05744457244873\n",
            "Loss: -14.550317764282227\n",
            "Loss: -14.128617286682129\n",
            "Loss: -13.712895393371582\n",
            "Loss: -13.294469833374023\n",
            "Loss: -12.91552448272705\n",
            "Loss: -12.535271644592285\n",
            "Loss: -12.160637855529785\n",
            "Loss: -11.792247772216797\n",
            "Loss: -11.442048072814941\n",
            "Loss: -11.095630645751953\n",
            "Loss: -10.743416786193848\n",
            "Loss: -10.367792129516602\n",
            "Loss: -9.992459297180176\n",
            "Loss: -9.631219863891602\n",
            "Loss: -9.255702018737793\n",
            "Loss: -8.870119094848633\n",
            "Loss: -8.49169635772705\n",
            "Loss: -8.119206428527832\n",
            "Loss: -7.771679401397705\n",
            "Loss: -7.423779487609863\n",
            "Loss: -7.119812965393066\n",
            "Loss: -6.765164375305176\n",
            "Loss: -6.423434257507324\n",
            "Loss: -6.058255672454834\n",
            "Loss: -5.693904399871826\n",
            "Loss: -5.317084789276123\n",
            "Loss: -4.95834493637085\n",
            "Loss: -4.580756187438965\n",
            "Loss: -4.197881698608398\n",
            "Loss: -3.802424907684326\n",
            "Loss: -3.4178621768951416\n",
            "Loss: -3.107792377471924\n",
            "Loss: -2.8495593070983887\n",
            "Loss: -2.6089651584625244\n",
            "Loss: -2.378239870071411\n",
            "Loss: -2.1347970962524414\n",
            "Loss: -1.9238616228103638\n",
            "Loss: -1.6812974214553833\n",
            "Loss: -1.471490502357483\n",
            "Loss: -1.23723304271698\n",
            "Loss: -1.0360044240951538\n",
            "Loss: -0.8565947413444519\n",
            "Loss: -0.7449185848236084\n",
            "We fooled the network after 56 iterations!\n",
            "New prediction: 8\n",
            "14658img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -29.094797134399414\n",
            "Loss: -28.516714096069336\n",
            "Loss: -27.952600479125977\n",
            "Loss: -27.39533233642578\n",
            "Loss: -26.844324111938477\n",
            "Loss: -26.27902603149414\n",
            "Loss: -25.738693237304688\n",
            "Loss: -25.18070411682129\n",
            "Loss: -24.694734573364258\n",
            "Loss: -24.23546028137207\n",
            "Loss: -23.77269172668457\n",
            "Loss: -23.32467269897461\n",
            "Loss: -22.865737915039062\n",
            "Loss: -22.41133689880371\n",
            "Loss: -21.97822380065918\n",
            "Loss: -21.531469345092773\n",
            "Loss: -21.09724998474121\n",
            "Loss: -20.663301467895508\n",
            "Loss: -20.238428115844727\n",
            "Loss: -19.817720413208008\n",
            "Loss: -19.394350051879883\n",
            "Loss: -18.965665817260742\n",
            "Loss: -18.53269386291504\n",
            "Loss: -18.051891326904297\n",
            "Loss: -17.58351707458496\n",
            "Loss: -17.130643844604492\n",
            "Loss: -16.72266960144043\n",
            "Loss: -16.334671020507812\n",
            "Loss: -15.942412376403809\n",
            "Loss: -15.558073997497559\n",
            "Loss: -15.17819881439209\n",
            "Loss: -14.817778587341309\n",
            "Loss: -14.444769859313965\n",
            "Loss: -14.075695037841797\n",
            "Loss: -13.701783180236816\n",
            "Loss: -13.326701164245605\n",
            "Loss: -12.95279598236084\n",
            "Loss: -12.591161727905273\n",
            "Loss: -12.2262601852417\n",
            "Loss: -11.869874000549316\n",
            "Loss: -11.512794494628906\n",
            "Loss: -11.146851539611816\n",
            "Loss: -10.816997528076172\n",
            "Loss: -10.460616111755371\n",
            "Loss: -10.130814552307129\n",
            "Loss: -9.793853759765625\n",
            "Loss: -9.473987579345703\n",
            "Loss: -9.168587684631348\n",
            "Loss: -8.842803001403809\n",
            "Loss: -8.559560775756836\n",
            "Loss: -8.296492576599121\n",
            "Loss: -8.031959533691406\n",
            "Loss: -7.793684482574463\n",
            "Loss: -7.54421329498291\n",
            "Loss: -7.310056209564209\n",
            "Loss: -7.071419715881348\n",
            "Loss: -6.8631463050842285\n",
            "Loss: -6.641678333282471\n",
            "Loss: -6.448286056518555\n",
            "Loss: -6.233410835266113\n",
            "Loss: -6.018248558044434\n",
            "Loss: -5.815784931182861\n",
            "Loss: -5.591361999511719\n",
            "Loss: -5.401877403259277\n",
            "Loss: -5.184669494628906\n",
            "Loss: -4.993165493011475\n",
            "Loss: -4.793074131011963\n",
            "Loss: -4.598720073699951\n",
            "Loss: -4.398082733154297\n",
            "Loss: -4.204414367675781\n",
            "Loss: -4.00798225402832\n",
            "Loss: -3.8025026321411133\n",
            "Loss: -3.624647617340088\n",
            "We fooled the network after 72 iterations!\n",
            "New prediction: 5\n",
            "14660img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -26.07157325744629\n",
            "Loss: -25.584959030151367\n",
            "Loss: -25.073793411254883\n",
            "Loss: -24.599679946899414\n",
            "Loss: -24.09343910217285\n",
            "Loss: -23.619529724121094\n",
            "Loss: -23.132638931274414\n",
            "Loss: -22.65677261352539\n",
            "Loss: -22.14019203186035\n",
            "Loss: -21.6350040435791\n",
            "Loss: -21.16417121887207\n",
            "Loss: -20.695940017700195\n",
            "Loss: -20.23750877380371\n",
            "Loss: -19.75218391418457\n",
            "Loss: -19.237245559692383\n",
            "Loss: -18.735544204711914\n",
            "Loss: -18.2396297454834\n",
            "Loss: -17.73389434814453\n",
            "Loss: -17.24444007873535\n",
            "Loss: -16.737932205200195\n",
            "Loss: -16.291536331176758\n",
            "Loss: -15.83401870727539\n",
            "Loss: -15.39836597442627\n",
            "Loss: -14.963530540466309\n",
            "Loss: -14.520875930786133\n",
            "Loss: -14.05570125579834\n",
            "Loss: -13.615269660949707\n",
            "Loss: -13.197155952453613\n",
            "Loss: -12.804888725280762\n",
            "Loss: -12.419533729553223\n",
            "Loss: -12.083648681640625\n",
            "Loss: -11.726990699768066\n",
            "Loss: -11.411337852478027\n",
            "Loss: -11.067214012145996\n",
            "Loss: -10.742314338684082\n",
            "Loss: -10.46458911895752\n",
            "Loss: -10.109980583190918\n",
            "Loss: -9.853447914123535\n",
            "Loss: -9.508234977722168\n",
            "Loss: -9.20328140258789\n",
            "Loss: -8.872464179992676\n",
            "Loss: -8.497476577758789\n",
            "Loss: -8.135706901550293\n",
            "Loss: -7.766918659210205\n",
            "Loss: -7.533743858337402\n",
            "Loss: -7.177027702331543\n",
            "Loss: -6.841584205627441\n",
            "Loss: -6.535671710968018\n",
            "Loss: -6.178674221038818\n",
            "Loss: -5.834883213043213\n",
            "Loss: -5.494236469268799\n",
            "Loss: -5.195312023162842\n",
            "Loss: -4.83382511138916\n",
            "Loss: -4.486966609954834\n",
            "Loss: -3.9902260303497314\n",
            "Loss: -3.594771146774292\n",
            "Loss: -3.1538100242614746\n",
            "Loss: -2.67579984664917\n",
            "Loss: -2.39209246635437\n",
            "Loss: -1.963760256767273\n",
            "We fooled the network after 59 iterations!\n",
            "New prediction: 4\n",
            "14661img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -29.283632278442383\n",
            "Loss: -28.638113021850586\n",
            "Loss: -27.959640502929688\n",
            "Loss: -27.24173927307129\n",
            "Loss: -26.531034469604492\n",
            "Loss: -25.808692932128906\n",
            "Loss: -25.081378936767578\n",
            "Loss: -24.384233474731445\n",
            "Loss: -23.66117286682129\n",
            "Loss: -22.962614059448242\n",
            "Loss: -22.25346565246582\n",
            "Loss: -21.540544509887695\n",
            "Loss: -20.85616111755371\n",
            "Loss: -20.189435958862305\n",
            "Loss: -19.49784278869629\n",
            "Loss: -18.859872817993164\n",
            "Loss: -18.177358627319336\n",
            "Loss: -17.558612823486328\n",
            "Loss: -16.934629440307617\n",
            "Loss: -16.292451858520508\n",
            "Loss: -15.69088077545166\n",
            "Loss: -15.166224479675293\n",
            "Loss: -14.664445877075195\n",
            "Loss: -14.15640926361084\n",
            "Loss: -13.686373710632324\n",
            "Loss: -13.21366024017334\n",
            "Loss: -12.819432258605957\n",
            "Loss: -12.463242530822754\n",
            "Loss: -12.062530517578125\n",
            "Loss: -11.685907363891602\n",
            "Loss: -11.303481101989746\n",
            "Loss: -10.916783332824707\n",
            "Loss: -10.561655044555664\n",
            "Loss: -10.165772438049316\n",
            "Loss: -9.815520286560059\n",
            "Loss: -9.467816352844238\n",
            "Loss: -9.146191596984863\n",
            "Loss: -8.818279266357422\n",
            "Loss: -8.50544261932373\n",
            "Loss: -8.214097023010254\n",
            "Loss: -7.942322731018066\n",
            "Loss: -7.648226737976074\n",
            "Loss: -7.3768744468688965\n",
            "Loss: -7.105037689208984\n",
            "Loss: -6.814517021179199\n",
            "Loss: -6.496549606323242\n",
            "Loss: -6.218340873718262\n",
            "Loss: -5.914411544799805\n",
            "Loss: -5.644896984100342\n",
            "Loss: -5.345844745635986\n",
            "Loss: -5.100755214691162\n",
            "Loss: -4.812518119812012\n",
            "Loss: -4.55815315246582\n",
            "Loss: -4.306917190551758\n",
            "Loss: -4.0675554275512695\n",
            "Loss: -3.8234221935272217\n",
            "Loss: -3.6077961921691895\n",
            "Loss: -3.4322855472564697\n",
            "Loss: -3.2522993087768555\n",
            "Loss: -3.0749430656433105\n",
            "Loss: -2.887744426727295\n",
            "Loss: -2.7119224071502686\n",
            "Loss: -2.5347180366516113\n",
            "Loss: -2.352277994155884\n",
            "Loss: -2.1658713817596436\n",
            "Loss: -1.978601336479187\n",
            "Loss: -1.8072302341461182\n",
            "Loss: -1.6460434198379517\n",
            "Loss: -1.4749263525009155\n",
            "Loss: -1.3242460489273071\n",
            "Loss: -1.1676753759384155\n",
            "We fooled the network after 70 iterations!\n",
            "New prediction: 8\n",
            "14666img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -30.40108299255371\n",
            "Loss: -29.896713256835938\n",
            "Loss: -29.378755569458008\n",
            "Loss: -28.881189346313477\n",
            "Loss: -28.459030151367188\n",
            "Loss: -28.014921188354492\n",
            "Loss: -27.601165771484375\n",
            "Loss: -27.160106658935547\n",
            "Loss: -26.745729446411133\n",
            "Loss: -26.3100528717041\n",
            "Loss: -25.895008087158203\n",
            "Loss: -25.44682502746582\n",
            "Loss: -25.017780303955078\n",
            "Loss: -24.5718936920166\n",
            "Loss: -24.1529483795166\n",
            "Loss: -23.70875358581543\n",
            "Loss: -23.273576736450195\n",
            "Loss: -22.8424015045166\n",
            "Loss: -22.40427589416504\n",
            "Loss: -21.971172332763672\n",
            "Loss: -21.532493591308594\n",
            "Loss: -21.10295295715332\n",
            "Loss: -20.62338638305664\n",
            "Loss: -20.119749069213867\n",
            "Loss: -19.653173446655273\n",
            "Loss: -19.22542381286621\n",
            "Loss: -18.77798843383789\n",
            "Loss: -18.325010299682617\n",
            "Loss: -17.886995315551758\n",
            "Loss: -17.444507598876953\n",
            "Loss: -17.013996124267578\n",
            "Loss: -16.57737159729004\n",
            "Loss: -16.166730880737305\n",
            "Loss: -15.763123512268066\n",
            "Loss: -15.349695205688477\n",
            "Loss: -14.974374771118164\n",
            "Loss: -14.563017845153809\n",
            "Loss: -14.203535079956055\n",
            "Loss: -13.790667533874512\n",
            "Loss: -13.398911476135254\n",
            "Loss: -12.981626510620117\n",
            "Loss: -12.392367362976074\n",
            "Loss: -11.826245307922363\n",
            "Loss: -11.304483413696289\n",
            "Loss: -10.760398864746094\n",
            "Loss: -10.219813346862793\n",
            "Loss: -9.51393985748291\n",
            "Loss: -8.809711456298828\n",
            "Loss: -8.056208610534668\n",
            "Loss: -7.162751197814941\n",
            "Loss: -6.197432041168213\n",
            "Loss: -5.295112609863281\n",
            "We fooled the network after 51 iterations!\n",
            "New prediction: 4\n",
            "14671img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -15.415358543395996\n",
            "Loss: -14.654234886169434\n",
            "Loss: -13.879446983337402\n",
            "Loss: -13.080060005187988\n",
            "Loss: -12.304390907287598\n",
            "Loss: -11.494386672973633\n",
            "Loss: -10.697916984558105\n",
            "Loss: -9.965235710144043\n",
            "Loss: -9.196006774902344\n",
            "Loss: -8.432182312011719\n",
            "Loss: -7.746600151062012\n",
            "Loss: -7.12262487411499\n",
            "Loss: -6.566139221191406\n",
            "We fooled the network after 12 iterations!\n",
            "New prediction: 8\n",
            "14676img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -21.484216690063477\n",
            "Loss: -21.027753829956055\n",
            "Loss: -20.541234970092773\n",
            "Loss: -20.065107345581055\n",
            "Loss: -19.58559226989746\n",
            "Loss: -19.126676559448242\n",
            "Loss: -18.666229248046875\n",
            "Loss: -18.18851661682129\n",
            "Loss: -17.722867965698242\n",
            "Loss: -17.250091552734375\n",
            "Loss: -16.802682876586914\n",
            "Loss: -16.351526260375977\n",
            "Loss: -15.905718803405762\n",
            "Loss: -15.469000816345215\n",
            "Loss: -15.023151397705078\n",
            "Loss: -14.616159439086914\n",
            "Loss: -14.190526008605957\n",
            "Loss: -13.749114036560059\n",
            "Loss: -13.345076560974121\n",
            "Loss: -12.913689613342285\n",
            "Loss: -12.518294334411621\n",
            "Loss: -12.145454406738281\n",
            "Loss: -11.777138710021973\n",
            "Loss: -11.432671546936035\n",
            "Loss: -11.022359848022461\n",
            "Loss: -10.562851905822754\n",
            "Loss: -10.125266075134277\n",
            "Loss: -9.680634498596191\n",
            "Loss: -9.248834609985352\n",
            "Loss: -8.80576229095459\n",
            "Loss: -8.36238956451416\n",
            "Loss: -7.917903900146484\n",
            "Loss: -7.509530067443848\n",
            "Loss: -6.951147556304932\n",
            "Loss: -6.387625694274902\n",
            "Loss: -5.797143936157227\n",
            "Loss: -5.258210182189941\n",
            "Loss: -4.712180137634277\n",
            "Loss: -4.205813407897949\n",
            "Loss: -3.664052963256836\n",
            "Loss: -3.128044366836548\n",
            "Loss: -2.634988307952881\n",
            "We fooled the network after 41 iterations!\n",
            "New prediction: 4\n",
            "14677img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -31.874692916870117\n",
            "Loss: -31.180830001831055\n",
            "Loss: -30.45787239074707\n",
            "Loss: -29.728008270263672\n",
            "Loss: -29.000337600708008\n",
            "Loss: -28.281187057495117\n",
            "Loss: -27.547224044799805\n",
            "Loss: -26.82554817199707\n",
            "Loss: -26.127573013305664\n",
            "Loss: -25.384078979492188\n",
            "Loss: -24.67143440246582\n",
            "Loss: -23.945180892944336\n",
            "Loss: -23.230615615844727\n",
            "Loss: -22.533672332763672\n",
            "Loss: -21.792964935302734\n",
            "Loss: -21.112285614013672\n",
            "Loss: -20.42767906188965\n",
            "Loss: -19.737390518188477\n",
            "Loss: -19.07691764831543\n",
            "Loss: -18.34715461730957\n",
            "Loss: -17.603439331054688\n",
            "Loss: -16.779190063476562\n",
            "Loss: -16.083972930908203\n",
            "Loss: -15.546833038330078\n",
            "Loss: -15.001473426818848\n",
            "Loss: -14.460168838500977\n",
            "Loss: -13.918407440185547\n",
            "Loss: -13.378483772277832\n",
            "Loss: -12.892963409423828\n",
            "Loss: -12.423018455505371\n",
            "Loss: -11.946330070495605\n",
            "Loss: -11.47896671295166\n",
            "Loss: -11.019015312194824\n",
            "Loss: -10.549017906188965\n",
            "Loss: -10.09436321258545\n",
            "Loss: -9.624419212341309\n",
            "Loss: -9.203413963317871\n",
            "Loss: -8.784377098083496\n",
            "Loss: -8.381386756896973\n",
            "Loss: -7.975141525268555\n",
            "Loss: -7.598745346069336\n",
            "Loss: -7.190228462219238\n",
            "Loss: -6.800823211669922\n",
            "Loss: -6.4171142578125\n",
            "Loss: -6.051169395446777\n",
            "Loss: -5.66818904876709\n",
            "Loss: -5.399951457977295\n",
            "Loss: -5.146034240722656\n",
            "Loss: -4.882626056671143\n",
            "Loss: -4.615509033203125\n",
            "Loss: -4.313167572021484\n",
            "Loss: -3.9937164783477783\n",
            "Loss: -3.627816915512085\n",
            "Loss: -3.1697537899017334\n",
            "We fooled the network after 53 iterations!\n",
            "New prediction: 5\n",
            "14708img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -23.828229904174805\n",
            "Loss: -23.373598098754883\n",
            "Loss: -22.892871856689453\n",
            "Loss: -22.414363861083984\n",
            "Loss: -21.93882179260254\n",
            "Loss: -21.480295181274414\n",
            "Loss: -21.029035568237305\n",
            "Loss: -20.580120086669922\n",
            "Loss: -20.142793655395508\n",
            "Loss: -19.710323333740234\n",
            "Loss: -19.318212509155273\n",
            "Loss: -18.940881729125977\n",
            "Loss: -18.520652770996094\n",
            "Loss: -18.099924087524414\n",
            "Loss: -17.703447341918945\n",
            "Loss: -17.308626174926758\n",
            "Loss: -16.921842575073242\n",
            "Loss: -16.56136703491211\n",
            "Loss: -16.202404022216797\n",
            "Loss: -15.836956024169922\n",
            "Loss: -15.462260246276855\n",
            "Loss: -15.142928123474121\n",
            "Loss: -14.798039436340332\n",
            "Loss: -14.471245765686035\n",
            "Loss: -14.151113510131836\n",
            "Loss: -13.813753128051758\n",
            "Loss: -13.510001182556152\n",
            "Loss: -13.239145278930664\n",
            "Loss: -12.94299030303955\n",
            "Loss: -12.682250022888184\n",
            "Loss: -12.418164253234863\n",
            "Loss: -12.147295951843262\n",
            "Loss: -11.871994018554688\n",
            "Loss: -11.608954429626465\n",
            "Loss: -11.377318382263184\n",
            "Loss: -11.101555824279785\n",
            "Loss: -10.899645805358887\n",
            "Loss: -10.663712501525879\n",
            "Loss: -10.393417358398438\n",
            "Loss: -10.201315879821777\n",
            "Loss: -9.949051856994629\n",
            "Loss: -9.710319519042969\n",
            "Loss: -9.500550270080566\n",
            "Loss: -9.272199630737305\n",
            "Loss: -9.05312442779541\n",
            "Loss: -8.8304443359375\n",
            "Loss: -8.622015953063965\n",
            "Loss: -8.397149085998535\n",
            "Loss: -8.156590461730957\n",
            "Loss: -7.972725868225098\n",
            "Loss: -7.717785835266113\n",
            "Loss: -7.543825149536133\n",
            "Loss: -7.295225143432617\n",
            "Loss: -7.044464111328125\n",
            "Loss: -6.654098033905029\n",
            "Loss: -6.255401611328125\n",
            "Loss: -5.7429704666137695\n",
            "Loss: -5.110702991485596\n",
            "Loss: -4.534948825836182\n",
            "Loss: -3.941401720046997\n",
            "We fooled the network after 59 iterations!\n",
            "New prediction: 4\n",
            "14721img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -23.98259735107422\n",
            "Loss: -23.560941696166992\n",
            "Loss: -23.12220573425293\n",
            "Loss: -22.67620086669922\n",
            "Loss: -22.2136173248291\n",
            "Loss: -21.77141761779785\n",
            "Loss: -21.30229949951172\n",
            "Loss: -20.78715705871582\n",
            "Loss: -20.288339614868164\n",
            "Loss: -19.788400650024414\n",
            "Loss: -19.285703659057617\n",
            "Loss: -18.790203094482422\n",
            "Loss: -18.318254470825195\n",
            "Loss: -17.842370986938477\n",
            "Loss: -17.366334915161133\n",
            "Loss: -16.879806518554688\n",
            "Loss: -16.411706924438477\n",
            "Loss: -15.935999870300293\n",
            "Loss: -15.448168754577637\n",
            "Loss: -14.969054222106934\n",
            "Loss: -14.531198501586914\n",
            "Loss: -14.08222484588623\n",
            "Loss: -13.66325569152832\n",
            "Loss: -13.19676685333252\n",
            "Loss: -12.733567237854004\n",
            "Loss: -12.284896850585938\n",
            "Loss: -11.821724891662598\n",
            "Loss: -11.412125587463379\n",
            "Loss: -11.00812816619873\n",
            "Loss: -10.63740062713623\n",
            "Loss: -10.245037078857422\n",
            "Loss: -9.869507789611816\n",
            "Loss: -9.491278648376465\n",
            "Loss: -9.11892318725586\n",
            "Loss: -8.729524612426758\n",
            "Loss: -8.385090827941895\n",
            "Loss: -8.026477813720703\n",
            "Loss: -7.586562156677246\n",
            "Loss: -7.131656169891357\n",
            "Loss: -6.726646423339844\n",
            "Loss: -6.296082019805908\n",
            "Loss: -5.857985973358154\n",
            "Loss: -5.413544654846191\n",
            "Loss: -4.8236541748046875\n",
            "Loss: -4.255998611450195\n",
            "Loss: -3.727726697921753\n",
            "Loss: -3.215733766555786\n",
            "Loss: -2.6648178100585938\n",
            "Loss: -2.102678060531616\n",
            "We fooled the network after 48 iterations!\n",
            "New prediction: 4\n",
            "14731img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -21.2946834564209\n",
            "Loss: -20.626659393310547\n",
            "Loss: -19.935081481933594\n",
            "Loss: -19.28014373779297\n",
            "Loss: -18.672861099243164\n",
            "Loss: -18.13728904724121\n",
            "Loss: -17.586450576782227\n",
            "Loss: -17.05190086364746\n",
            "Loss: -16.52033233642578\n",
            "Loss: -15.98625659942627\n",
            "Loss: -15.499287605285645\n",
            "Loss: -15.0630521774292\n",
            "Loss: -14.588728904724121\n",
            "Loss: -14.156048774719238\n",
            "Loss: -13.709794044494629\n",
            "Loss: -13.293980598449707\n",
            "Loss: -12.886637687683105\n",
            "Loss: -12.482107162475586\n",
            "Loss: -12.062161445617676\n",
            "Loss: -11.647342681884766\n",
            "Loss: -11.235492706298828\n",
            "Loss: -10.794846534729004\n",
            "Loss: -10.350936889648438\n",
            "Loss: -9.941239356994629\n",
            "Loss: -9.611200332641602\n",
            "Loss: -9.257680892944336\n",
            "Loss: -8.863051414489746\n",
            "Loss: -8.475457191467285\n",
            "Loss: -8.16500473022461\n",
            "Loss: -7.882105827331543\n",
            "Loss: -7.592637062072754\n",
            "Loss: -7.290070533752441\n",
            "We fooled the network after 31 iterations!\n",
            "New prediction: 5\n",
            "14732img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -20.778383255004883\n",
            "Loss: -20.383764266967773\n",
            "Loss: -19.963903427124023\n",
            "Loss: -19.532629013061523\n",
            "Loss: -19.116331100463867\n",
            "Loss: -18.671937942504883\n",
            "Loss: -18.23505973815918\n",
            "Loss: -17.781360626220703\n",
            "Loss: -17.356523513793945\n",
            "Loss: -16.90535545349121\n",
            "Loss: -16.482492446899414\n",
            "Loss: -16.072357177734375\n",
            "Loss: -15.660223960876465\n",
            "Loss: -15.24980640411377\n",
            "Loss: -14.863983154296875\n",
            "Loss: -14.481782913208008\n",
            "Loss: -14.091537475585938\n",
            "Loss: -13.688669204711914\n",
            "Loss: -13.299381256103516\n",
            "Loss: -12.911972045898438\n",
            "Loss: -12.575831413269043\n",
            "Loss: -12.204012870788574\n",
            "Loss: -11.850129127502441\n",
            "Loss: -11.4613618850708\n",
            "Loss: -11.08444881439209\n",
            "Loss: -10.638543128967285\n",
            "Loss: -10.173473358154297\n",
            "Loss: -9.737163543701172\n",
            "Loss: -9.29961109161377\n",
            "Loss: -8.765178680419922\n",
            "Loss: -8.295990943908691\n",
            "We fooled the network after 30 iterations!\n",
            "New prediction: 5\n",
            "14747img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -3.944483995437622\n",
            "Loss: -3.668657064437866\n",
            "Loss: -3.415600061416626\n",
            "Loss: -3.150372266769409\n",
            "Loss: -2.9197733402252197\n",
            "Loss: -2.6799397468566895\n",
            "Loss: -2.3726296424865723\n",
            "Loss: -2.04675030708313\n",
            "Loss: -1.6696916818618774\n",
            "Loss: -1.2561863660812378\n",
            "We fooled the network after 9 iterations!\n",
            "New prediction: 4\n",
            "14764img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -30.53644561767578\n",
            "Loss: -30.049806594848633\n",
            "Loss: -29.528484344482422\n",
            "Loss: -29.009119033813477\n",
            "Loss: -28.53171730041504\n",
            "Loss: -28.031972885131836\n",
            "Loss: -27.531980514526367\n",
            "Loss: -27.036951065063477\n",
            "Loss: -26.551801681518555\n",
            "Loss: -26.06171226501465\n",
            "Loss: -25.5665283203125\n",
            "Loss: -25.072601318359375\n",
            "Loss: -24.58271026611328\n",
            "Loss: -24.161623001098633\n",
            "Loss: -23.72868537902832\n",
            "Loss: -23.330541610717773\n",
            "Loss: -22.91286277770996\n",
            "Loss: -22.492292404174805\n",
            "Loss: -22.07356834411621\n",
            "Loss: -21.714134216308594\n",
            "Loss: -21.337621688842773\n",
            "Loss: -20.95383644104004\n",
            "Loss: -20.575740814208984\n",
            "Loss: -20.210962295532227\n",
            "Loss: -19.83370018005371\n",
            "Loss: -19.471168518066406\n",
            "Loss: -19.106128692626953\n",
            "Loss: -18.730695724487305\n",
            "Loss: -18.33782386779785\n",
            "Loss: -17.932634353637695\n",
            "Loss: -17.545595169067383\n",
            "Loss: -17.176645278930664\n",
            "Loss: -16.804555892944336\n",
            "Loss: -16.42694664001465\n",
            "Loss: -16.05704116821289\n",
            "Loss: -15.690716743469238\n",
            "Loss: -15.337517738342285\n",
            "Loss: -14.970145225524902\n",
            "Loss: -14.610005378723145\n",
            "Loss: -14.259428024291992\n",
            "Loss: -13.914000511169434\n",
            "Loss: -13.56368637084961\n",
            "Loss: -13.204546928405762\n",
            "Loss: -12.822808265686035\n",
            "Loss: -12.49040412902832\n",
            "Loss: -12.134234428405762\n",
            "Loss: -11.787272453308105\n",
            "Loss: -11.42725658416748\n",
            "Loss: -11.075554847717285\n",
            "Loss: -10.73098087310791\n",
            "Loss: -10.350810050964355\n",
            "Loss: -9.96937084197998\n",
            "Loss: -9.594058990478516\n",
            "Loss: -9.234745979309082\n",
            "Loss: -8.938611030578613\n",
            "Loss: -8.642325401306152\n",
            "Loss: -8.355000495910645\n",
            "Loss: -8.072196006774902\n",
            "Loss: -7.795531749725342\n",
            "Loss: -7.535279273986816\n",
            "Loss: -7.262514114379883\n",
            "Loss: -6.9917168617248535\n",
            "Loss: -6.732378005981445\n",
            "Loss: -6.457821846008301\n",
            "Loss: -6.200531959533691\n",
            "Loss: -5.935784816741943\n",
            "Loss: -5.672685623168945\n",
            "Loss: -5.4067230224609375\n",
            "Loss: -5.151107311248779\n",
            "Loss: -4.892712593078613\n",
            "Loss: -4.62240743637085\n",
            "Loss: -4.373832702636719\n",
            "Loss: -4.100124835968018\n",
            "Loss: -3.8490612506866455\n",
            "Loss: -3.58445143699646\n",
            "Loss: -3.3100593090057373\n",
            "Loss: -3.017143487930298\n",
            "Loss: -2.7671520709991455\n",
            "Loss: -2.559346914291382\n",
            "Loss: -2.34356427192688\n",
            "Loss: -2.125302314758301\n",
            "Loss: -1.920923113822937\n",
            "Loss: -1.7164417505264282\n",
            "We fooled the network after 82 iterations!\n",
            "New prediction: 8\n",
            "14768img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -27.8120174407959\n",
            "Loss: -27.149446487426758\n",
            "Loss: -26.480302810668945\n",
            "Loss: -25.803720474243164\n",
            "Loss: -25.146299362182617\n",
            "Loss: -24.482318878173828\n",
            "Loss: -23.841901779174805\n",
            "Loss: -23.19441795349121\n",
            "Loss: -22.544767379760742\n",
            "Loss: -21.936750411987305\n",
            "Loss: -21.340049743652344\n",
            "Loss: -20.73208999633789\n",
            "Loss: -20.1201229095459\n",
            "Loss: -19.535547256469727\n",
            "Loss: -18.92629623413086\n",
            "Loss: -18.352102279663086\n",
            "Loss: -17.821979522705078\n",
            "Loss: -17.253145217895508\n",
            "Loss: -16.74446678161621\n",
            "Loss: -16.169652938842773\n",
            "Loss: -15.653433799743652\n",
            "Loss: -15.086750030517578\n",
            "Loss: -14.594460487365723\n",
            "Loss: -14.007981300354004\n",
            "Loss: -13.461176872253418\n",
            "Loss: -12.833511352539062\n",
            "Loss: -12.23067569732666\n",
            "Loss: -11.545165061950684\n",
            "Loss: -10.928486824035645\n",
            "Loss: -10.26722240447998\n",
            "Loss: -9.601027488708496\n",
            "Loss: -8.77271556854248\n",
            "Loss: -7.995628356933594\n",
            "Loss: -7.249353408813477\n",
            "Loss: -6.486640453338623\n",
            "Loss: -5.5654168128967285\n",
            "Loss: -4.554101467132568\n",
            "Loss: -3.551931142807007\n",
            "Loss: -2.5714852809906006\n",
            "Loss: -1.6415618658065796\n",
            "We fooled the network after 39 iterations!\n",
            "New prediction: 8\n",
            "14778img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -31.574121475219727\n",
            "Loss: -31.145139694213867\n",
            "Loss: -30.653263092041016\n",
            "Loss: -30.159818649291992\n",
            "Loss: -29.655134201049805\n",
            "Loss: -29.193159103393555\n",
            "Loss: -28.725234985351562\n",
            "Loss: -28.314586639404297\n",
            "Loss: -27.90577507019043\n",
            "Loss: -27.496938705444336\n",
            "Loss: -27.085601806640625\n",
            "Loss: -26.661121368408203\n",
            "Loss: -26.236328125\n",
            "Loss: -25.83296775817871\n",
            "Loss: -25.412691116333008\n",
            "Loss: -24.99251365661621\n",
            "Loss: -24.588483810424805\n",
            "Loss: -24.17133140563965\n",
            "Loss: -23.7465763092041\n",
            "Loss: -23.32769203186035\n",
            "Loss: -22.936655044555664\n",
            "Loss: -22.570695877075195\n",
            "Loss: -22.204885482788086\n",
            "Loss: -21.85780906677246\n",
            "Loss: -21.48482894897461\n",
            "Loss: -21.09880256652832\n",
            "Loss: -20.738798141479492\n",
            "Loss: -20.37934684753418\n",
            "Loss: -20.00067901611328\n",
            "Loss: -19.619979858398438\n",
            "Loss: -19.274538040161133\n",
            "Loss: -18.93770980834961\n",
            "Loss: -18.586429595947266\n",
            "Loss: -18.227970123291016\n",
            "Loss: -17.899003982543945\n",
            "Loss: -17.547008514404297\n",
            "Loss: -17.203001022338867\n",
            "Loss: -16.89266586303711\n",
            "Loss: -16.525409698486328\n",
            "Loss: -16.180444717407227\n",
            "Loss: -15.873513221740723\n",
            "Loss: -15.557112693786621\n",
            "Loss: -15.23290729522705\n",
            "Loss: -14.94669246673584\n",
            "Loss: -14.601827621459961\n",
            "Loss: -14.289191246032715\n",
            "Loss: -13.980466842651367\n",
            "Loss: -13.643132209777832\n",
            "Loss: -13.36674976348877\n",
            "Loss: -12.992159843444824\n",
            "Loss: -12.57547664642334\n",
            "Loss: -12.223176002502441\n",
            "Loss: -11.886301040649414\n",
            "Loss: -11.501363754272461\n",
            "Loss: -11.120585441589355\n",
            "Loss: -10.775949478149414\n",
            "Loss: -10.40302848815918\n",
            "Loss: -10.107023239135742\n",
            "Loss: -9.734414100646973\n",
            "Loss: -9.350838661193848\n",
            "Loss: -8.977813720703125\n",
            "Loss: -8.613914489746094\n",
            "Loss: -8.293671607971191\n",
            "Loss: -7.919332504272461\n",
            "Loss: -7.567801475524902\n",
            "Loss: -7.26402473449707\n",
            "Loss: -7.00338888168335\n",
            "Loss: -6.742064476013184\n",
            "Loss: -6.474742889404297\n",
            "Loss: -6.289190769195557\n",
            "Loss: -6.024643898010254\n",
            "Loss: -5.790606498718262\n",
            "Loss: -5.488595008850098\n",
            "Loss: -5.100900650024414\n",
            "Loss: -4.678258419036865\n",
            "Loss: -4.163289546966553\n",
            "Loss: -3.539494037628174\n",
            "Loss: -2.8949220180511475\n",
            "We fooled the network after 77 iterations!\n",
            "New prediction: 4\n",
            "14783img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -17.7072811126709\n",
            "Loss: -17.04294776916504\n",
            "Loss: -16.35796356201172\n",
            "Loss: -15.683377265930176\n",
            "Loss: -15.019251823425293\n",
            "Loss: -14.354368209838867\n",
            "Loss: -13.677877426147461\n",
            "Loss: -13.035541534423828\n",
            "Loss: -12.441301345825195\n",
            "Loss: -11.86279296875\n",
            "Loss: -11.29751205444336\n",
            "Loss: -10.810151100158691\n",
            "Loss: -10.294532775878906\n",
            "Loss: -9.777190208435059\n",
            "Loss: -9.140534400939941\n",
            "Loss: -8.495306015014648\n",
            "Loss: -7.841310501098633\n",
            "Loss: -7.331535816192627\n",
            "We fooled the network after 17 iterations!\n",
            "New prediction: 5\n",
            "14784img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -27.995737075805664\n",
            "Loss: -27.54412269592285\n",
            "Loss: -27.076152801513672\n",
            "Loss: -26.589529037475586\n",
            "Loss: -26.1213436126709\n",
            "Loss: -25.643136978149414\n",
            "Loss: -25.17449951171875\n",
            "Loss: -24.706514358520508\n",
            "Loss: -24.259775161743164\n",
            "Loss: -23.8031063079834\n",
            "Loss: -23.35736846923828\n",
            "Loss: -22.927391052246094\n",
            "Loss: -22.560741424560547\n",
            "Loss: -22.15336799621582\n",
            "Loss: -21.806642532348633\n",
            "Loss: -21.394445419311523\n",
            "Loss: -21.044631958007812\n",
            "Loss: -20.643774032592773\n",
            "Loss: -20.275442123413086\n",
            "Loss: -19.899112701416016\n",
            "Loss: -19.512943267822266\n",
            "Loss: -19.130146026611328\n",
            "Loss: -18.715208053588867\n",
            "Loss: -18.37891960144043\n",
            "Loss: -17.89081382751465\n",
            "Loss: -17.410261154174805\n",
            "Loss: -16.897916793823242\n",
            "Loss: -16.36091423034668\n",
            "Loss: -15.89908504486084\n",
            "Loss: -15.417184829711914\n",
            "Loss: -14.958794593811035\n",
            "Loss: -14.419026374816895\n",
            "Loss: -13.954697608947754\n",
            "Loss: -13.451062202453613\n",
            "Loss: -12.975663185119629\n",
            "Loss: -12.504308700561523\n",
            "Loss: -12.039590835571289\n",
            "Loss: -11.601975440979004\n",
            "Loss: -11.103394508361816\n",
            "Loss: -10.683684349060059\n",
            "Loss: -10.248870849609375\n",
            "Loss: -9.754786491394043\n",
            "Loss: -9.328722953796387\n",
            "Loss: -8.844304084777832\n",
            "Loss: -8.37479305267334\n",
            "Loss: -7.950016975402832\n",
            "Loss: -7.524277687072754\n",
            "Loss: -7.175863265991211\n",
            "Loss: -6.803871154785156\n",
            "Loss: -6.506678581237793\n",
            "Loss: -6.152414321899414\n",
            "Loss: -5.826556205749512\n",
            "Loss: -5.488303184509277\n",
            "Loss: -5.15761137008667\n",
            "Loss: -4.704694747924805\n",
            "Loss: -4.295966148376465\n",
            "Loss: -3.8501951694488525\n",
            "Loss: -3.438260316848755\n",
            "Loss: -3.0232019424438477\n",
            "Loss: -2.485581636428833\n",
            "Loss: -1.9559978246688843\n",
            "Loss: -1.3356578350067139\n",
            "We fooled the network after 61 iterations!\n",
            "New prediction: 4\n",
            "14801img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -26.846590042114258\n",
            "Loss: -26.234766006469727\n",
            "Loss: -25.656993865966797\n",
            "Loss: -25.087173461914062\n",
            "Loss: -24.539438247680664\n",
            "Loss: -23.991992950439453\n",
            "Loss: -23.490009307861328\n",
            "Loss: -23.057373046875\n",
            "Loss: -22.634475708007812\n",
            "Loss: -22.2318058013916\n",
            "Loss: -21.812633514404297\n",
            "Loss: -21.411115646362305\n",
            "Loss: -21.022422790527344\n",
            "Loss: -20.644119262695312\n",
            "Loss: -20.260473251342773\n",
            "Loss: -19.879335403442383\n",
            "Loss: -19.505626678466797\n",
            "Loss: -19.113508224487305\n",
            "Loss: -18.69965171813965\n",
            "Loss: -18.2772159576416\n",
            "Loss: -17.860578536987305\n",
            "Loss: -17.438222885131836\n",
            "Loss: -17.023366928100586\n",
            "Loss: -16.605070114135742\n",
            "Loss: -16.1895751953125\n",
            "Loss: -15.777177810668945\n",
            "Loss: -15.378766059875488\n",
            "Loss: -14.986398696899414\n",
            "Loss: -14.59449291229248\n",
            "Loss: -14.216139793395996\n",
            "Loss: -13.822491645812988\n",
            "Loss: -13.419442176818848\n",
            "Loss: -13.023390769958496\n",
            "Loss: -12.627073287963867\n",
            "Loss: -12.23865795135498\n",
            "Loss: -11.851890563964844\n",
            "Loss: -11.469771385192871\n",
            "Loss: -11.082072257995605\n",
            "Loss: -10.730172157287598\n",
            "Loss: -10.35054874420166\n",
            "Loss: -9.983474731445312\n",
            "Loss: -9.605156898498535\n",
            "Loss: -9.231253623962402\n",
            "Loss: -8.859893798828125\n",
            "Loss: -8.49097728729248\n",
            "Loss: -8.117745399475098\n",
            "Loss: -7.753791809082031\n",
            "Loss: -7.3921942710876465\n",
            "Loss: -7.0544023513793945\n",
            "Loss: -6.712785243988037\n",
            "Loss: -6.382241249084473\n",
            "Loss: -6.034653663635254\n",
            "Loss: -5.680138111114502\n",
            "Loss: -5.330284118652344\n",
            "Loss: -4.967505931854248\n",
            "Loss: -4.59783411026001\n",
            "Loss: -4.311796188354492\n",
            "Loss: -4.037001609802246\n",
            "Loss: -3.756471872329712\n",
            "Loss: -3.491370439529419\n",
            "Loss: -3.222886800765991\n",
            "Loss: -2.8873343467712402\n",
            "Loss: -2.222318410873413\n",
            "We fooled the network after 62 iterations!\n",
            "New prediction: 5\n",
            "14804img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -23.836902618408203\n",
            "Loss: -23.40081214904785\n",
            "Loss: -22.935949325561523\n",
            "Loss: -22.470815658569336\n",
            "Loss: -21.98597526550293\n",
            "Loss: -21.52055549621582\n",
            "Loss: -21.085206985473633\n",
            "Loss: -20.640336990356445\n",
            "Loss: -20.215835571289062\n",
            "Loss: -19.786657333374023\n",
            "Loss: -19.400941848754883\n",
            "Loss: -18.975736618041992\n",
            "Loss: -18.5859432220459\n",
            "Loss: -18.17790412902832\n",
            "Loss: -17.785390853881836\n",
            "Loss: -17.398555755615234\n",
            "Loss: -16.987668991088867\n",
            "Loss: -16.633211135864258\n",
            "Loss: -16.263010025024414\n",
            "Loss: -15.89378833770752\n",
            "Loss: -15.54406452178955\n",
            "Loss: -15.153624534606934\n",
            "Loss: -14.825837135314941\n",
            "Loss: -14.43274211883545\n",
            "Loss: -14.101693153381348\n",
            "Loss: -13.712372779846191\n",
            "Loss: -13.364701271057129\n",
            "Loss: -12.963318824768066\n",
            "Loss: -12.64844799041748\n",
            "Loss: -12.257160186767578\n",
            "Loss: -11.943511962890625\n",
            "Loss: -11.572274208068848\n",
            "Loss: -11.279783248901367\n",
            "Loss: -10.890402793884277\n",
            "Loss: -10.56596565246582\n",
            "Loss: -10.191339492797852\n",
            "Loss: -9.830020904541016\n",
            "Loss: -9.510926246643066\n",
            "Loss: -9.088412284851074\n",
            "Loss: -8.745684623718262\n",
            "Loss: -8.411125183105469\n",
            "Loss: -8.06375503540039\n",
            "Loss: -7.735471725463867\n",
            "Loss: -7.328054428100586\n",
            "Loss: -7.010532379150391\n",
            "Loss: -6.5859479904174805\n",
            "Loss: -6.291345119476318\n",
            "Loss: -5.902220249176025\n",
            "Loss: -5.591135025024414\n",
            "Loss: -5.281656265258789\n",
            "Loss: -4.960087299346924\n",
            "Loss: -4.700239181518555\n",
            "Loss: -4.350794792175293\n",
            "Loss: -4.09986686706543\n",
            "Loss: -3.701348304748535\n",
            "Loss: -3.4609057903289795\n",
            "We fooled the network after 55 iterations!\n",
            "New prediction: 5\n",
            "14805img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -15.430936813354492\n",
            "Loss: -15.007275581359863\n",
            "Loss: -14.544352531433105\n",
            "Loss: -14.08882999420166\n",
            "Loss: -13.650641441345215\n",
            "Loss: -13.211142539978027\n",
            "Loss: -12.777438163757324\n",
            "Loss: -12.317054748535156\n",
            "Loss: -11.901965141296387\n",
            "Loss: -11.476304054260254\n",
            "Loss: -11.084263801574707\n",
            "Loss: -10.71461009979248\n",
            "Loss: -10.330140113830566\n",
            "Loss: -9.944332122802734\n",
            "Loss: -9.561091423034668\n",
            "Loss: -9.183460235595703\n",
            "Loss: -8.75008487701416\n",
            "Loss: -8.206229209899902\n",
            "Loss: -7.723361015319824\n",
            "Loss: -7.231069087982178\n",
            "Loss: -6.783532619476318\n",
            "Loss: -6.367886543273926\n",
            "Loss: -5.9564127922058105\n",
            "Loss: -5.574349403381348\n",
            "Loss: -5.055769920349121\n",
            "Loss: -4.524479866027832\n",
            "Loss: -3.92277455329895\n",
            "Loss: -3.31535267829895\n",
            "We fooled the network after 27 iterations!\n",
            "New prediction: 4\n",
            "14813img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -29.25891876220703\n",
            "Loss: -28.652910232543945\n",
            "Loss: -28.033050537109375\n",
            "Loss: -27.407838821411133\n",
            "Loss: -26.806848526000977\n",
            "Loss: -26.21562385559082\n",
            "Loss: -25.659494400024414\n",
            "Loss: -25.0757999420166\n",
            "Loss: -24.492618560791016\n",
            "Loss: -23.91779136657715\n",
            "Loss: -23.36258888244629\n",
            "Loss: -22.816560745239258\n",
            "Loss: -22.266157150268555\n",
            "Loss: -21.70815658569336\n",
            "Loss: -21.1667423248291\n",
            "Loss: -20.6363582611084\n",
            "Loss: -20.094831466674805\n",
            "Loss: -19.538976669311523\n",
            "Loss: -19.019460678100586\n",
            "Loss: -18.497228622436523\n",
            "Loss: -17.974811553955078\n",
            "Loss: -17.476585388183594\n",
            "Loss: -16.98996353149414\n",
            "Loss: -16.513349533081055\n",
            "Loss: -16.055477142333984\n",
            "Loss: -15.629839897155762\n",
            "Loss: -15.158735275268555\n",
            "Loss: -14.667948722839355\n",
            "Loss: -14.137682914733887\n",
            "Loss: -13.5978422164917\n",
            "Loss: -13.088196754455566\n",
            "Loss: -12.555959701538086\n",
            "Loss: -12.047686576843262\n",
            "Loss: -11.558865547180176\n",
            "Loss: -11.053793907165527\n",
            "Loss: -10.565685272216797\n",
            "Loss: -10.068778038024902\n",
            "Loss: -9.576192855834961\n",
            "Loss: -8.997392654418945\n",
            "Loss: -8.423861503601074\n",
            "Loss: -7.95489501953125\n",
            "Loss: -7.54010534286499\n",
            "Loss: -7.138288974761963\n",
            "Loss: -6.711400985717773\n",
            "Loss: -6.282621383666992\n",
            "Loss: -5.858288288116455\n",
            "Loss: -5.452682018280029\n",
            "Loss: -5.076725959777832\n",
            "Loss: -4.707499027252197\n",
            "Loss: -4.337032318115234\n",
            "Loss: -3.957249402999878\n",
            "Loss: -3.5857386589050293\n",
            "Loss: -3.2409636974334717\n",
            "Loss: -2.889625072479248\n",
            "Loss: -2.6026012897491455\n",
            "Loss: -2.293956995010376\n",
            "Loss: -1.9837151765823364\n",
            "We fooled the network after 56 iterations!\n",
            "New prediction: 8\n",
            "14848img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -34.87253189086914\n",
            "Loss: -34.33451461791992\n",
            "Loss: -33.72548294067383\n",
            "Loss: -33.098392486572266\n",
            "Loss: -32.47056579589844\n",
            "Loss: -31.845792770385742\n",
            "Loss: -31.220556259155273\n",
            "Loss: -30.59664535522461\n",
            "Loss: -29.951400756835938\n",
            "Loss: -29.319828033447266\n",
            "Loss: -28.694984436035156\n",
            "Loss: -28.07282066345215\n",
            "Loss: -27.433120727539062\n",
            "Loss: -26.819292068481445\n",
            "Loss: -26.20294761657715\n",
            "Loss: -25.595277786254883\n",
            "Loss: -24.97623634338379\n",
            "Loss: -24.42066764831543\n",
            "Loss: -23.885465621948242\n",
            "Loss: -23.348379135131836\n",
            "Loss: -22.827524185180664\n",
            "Loss: -22.337934494018555\n",
            "Loss: -21.81825065612793\n",
            "Loss: -21.277509689331055\n",
            "Loss: -20.782068252563477\n",
            "Loss: -20.262714385986328\n",
            "Loss: -19.73386573791504\n",
            "Loss: -19.20168685913086\n",
            "Loss: -18.71793556213379\n",
            "Loss: -18.178007125854492\n",
            "Loss: -17.653268814086914\n",
            "Loss: -17.138595581054688\n",
            "Loss: -16.652315139770508\n",
            "Loss: -16.151334762573242\n",
            "Loss: -15.651599884033203\n",
            "Loss: -15.185555458068848\n",
            "Loss: -14.736346244812012\n",
            "Loss: -14.288148880004883\n",
            "Loss: -13.883708000183105\n",
            "Loss: -13.495035171508789\n",
            "Loss: -13.09232234954834\n",
            "Loss: -12.719799995422363\n",
            "Loss: -12.42260456085205\n",
            "Loss: -12.110994338989258\n",
            "Loss: -11.801108360290527\n",
            "Loss: -11.483269691467285\n",
            "Loss: -11.187447547912598\n",
            "Loss: -10.887934684753418\n",
            "Loss: -10.572978973388672\n",
            "Loss: -10.277470588684082\n",
            "Loss: -9.993149757385254\n",
            "Loss: -9.711617469787598\n",
            "Loss: -9.443707466125488\n",
            "Loss: -9.169965744018555\n",
            "Loss: -8.911297798156738\n",
            "Loss: -8.651989936828613\n",
            "Loss: -8.381072998046875\n",
            "Loss: -8.11789321899414\n",
            "Loss: -7.839896202087402\n",
            "Loss: -7.552093505859375\n",
            "Loss: -7.270739555358887\n",
            "Loss: -6.994174480438232\n",
            "Loss: -6.712427616119385\n",
            "Loss: -6.413756370544434\n",
            "Loss: -6.138678550720215\n",
            "Loss: -5.872054100036621\n",
            "Loss: -5.598416328430176\n",
            "Loss: -5.345846176147461\n",
            "Loss: -5.104551315307617\n",
            "Loss: -4.862222671508789\n",
            "Loss: -4.630877494812012\n",
            "Loss: -4.398451805114746\n",
            "Loss: -4.161151885986328\n",
            "Loss: -3.9431564807891846\n",
            "Loss: -3.7207634449005127\n",
            "Loss: -3.5145108699798584\n",
            "Loss: -3.3012125492095947\n",
            "Loss: -3.0886309146881104\n",
            "Loss: -2.891054391860962\n",
            "Loss: -2.6741867065429688\n",
            "Loss: -2.475043535232544\n",
            "Loss: -2.3161282539367676\n",
            "Loss: -2.1104164123535156\n",
            "Loss: -1.8890775442123413\n",
            "We fooled the network after 83 iterations!\n",
            "New prediction: 8\n",
            "14869img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -43.36567687988281\n",
            "Loss: -42.635379791259766\n",
            "Loss: -41.89266586303711\n",
            "Loss: -41.15560531616211\n",
            "Loss: -40.412479400634766\n",
            "Loss: -39.6734504699707\n",
            "Loss: -38.92227554321289\n",
            "Loss: -38.17503356933594\n",
            "Loss: -37.4227180480957\n",
            "Loss: -36.66847610473633\n",
            "Loss: -35.954952239990234\n",
            "Loss: -35.257225036621094\n",
            "Loss: -34.554622650146484\n",
            "Loss: -33.85686492919922\n",
            "Loss: -33.16524124145508\n",
            "Loss: -32.45954513549805\n",
            "Loss: -31.770845413208008\n",
            "Loss: -31.067676544189453\n",
            "Loss: -30.392499923706055\n",
            "Loss: -29.684206008911133\n",
            "Loss: -29.04014778137207\n",
            "Loss: -28.35344123840332\n",
            "Loss: -27.705408096313477\n",
            "Loss: -27.022016525268555\n",
            "Loss: -26.355224609375\n",
            "Loss: -25.672327041625977\n",
            "Loss: -24.92387580871582\n",
            "Loss: -24.113370895385742\n",
            "Loss: -23.341903686523438\n",
            "Loss: -22.592031478881836\n",
            "Loss: -21.87148094177246\n",
            "Loss: -21.158842086791992\n",
            "Loss: -20.434736251831055\n",
            "Loss: -19.727766036987305\n",
            "Loss: -19.042034149169922\n",
            "Loss: -18.332849502563477\n",
            "Loss: -17.697547912597656\n",
            "Loss: -17.20000648498535\n",
            "Loss: -16.7105655670166\n",
            "Loss: -16.224382400512695\n",
            "Loss: -15.766708374023438\n",
            "Loss: -15.289250373840332\n",
            "Loss: -14.766046524047852\n",
            "Loss: -14.306797981262207\n",
            "Loss: -13.85305118560791\n",
            "Loss: -13.404170036315918\n",
            "Loss: -12.958805084228516\n",
            "Loss: -12.546547889709473\n",
            "Loss: -12.088950157165527\n",
            "Loss: -11.688969612121582\n",
            "Loss: -11.264300346374512\n",
            "Loss: -10.803451538085938\n",
            "Loss: -10.327984809875488\n",
            "Loss: -9.847472190856934\n",
            "Loss: -9.347228050231934\n",
            "Loss: -8.842608451843262\n",
            "Loss: -8.37972354888916\n",
            "Loss: -7.879996299743652\n",
            "Loss: -7.440165042877197\n",
            "Loss: -6.973572254180908\n",
            "Loss: -6.6035919189453125\n",
            "Loss: -6.3431077003479\n",
            "Loss: -6.088976860046387\n",
            "Loss: -5.831707954406738\n",
            "Loss: -5.596109390258789\n",
            "Loss: -5.354650497436523\n",
            "Loss: -5.094898223876953\n",
            "Loss: -4.850016117095947\n",
            "Loss: -4.615143299102783\n",
            "Loss: -4.367555618286133\n",
            "Loss: -4.115349769592285\n",
            "Loss: -3.8750545978546143\n",
            "Loss: -3.6199586391448975\n",
            "Loss: -3.387883424758911\n",
            "Loss: -3.168593645095825\n",
            "Loss: -2.9257216453552246\n",
            "Loss: -2.6955366134643555\n",
            "Loss: -2.4463653564453125\n",
            "Loss: -2.2210400104522705\n",
            "Loss: -1.9754143953323364\n",
            "Loss: -1.741949439048767\n",
            "Loss: -1.4986933469772339\n",
            "Loss: -1.2377475500106812\n",
            "Loss: -0.981745183467865\n",
            "Loss: -0.7381629347801208\n",
            "We fooled the network after 84 iterations!\n",
            "New prediction: 8\n",
            "14889img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -15.622319221496582\n",
            "Loss: -14.973851203918457\n",
            "Loss: -14.268073081970215\n",
            "Loss: -13.600200653076172\n",
            "Loss: -13.077290534973145\n",
            "Loss: -12.557676315307617\n",
            "Loss: -12.042292594909668\n",
            "Loss: -11.52969741821289\n",
            "Loss: -10.99314022064209\n",
            "Loss: -10.475977897644043\n",
            "Loss: -9.963570594787598\n",
            "Loss: -9.437496185302734\n",
            "Loss: -8.935612678527832\n",
            "Loss: -8.463654518127441\n",
            "Loss: -8.005242347717285\n",
            "Loss: -7.5583086013793945\n",
            "Loss: -7.1111674308776855\n",
            "Loss: -6.644474983215332\n",
            "Loss: -6.176932334899902\n",
            "Loss: -5.742504596710205\n",
            "Loss: -5.329920768737793\n",
            "Loss: -4.94233512878418\n",
            "Loss: -4.575039863586426\n",
            "Loss: -4.224790573120117\n",
            "Loss: -3.913586378097534\n",
            "Loss: -3.6338346004486084\n",
            "Loss: -3.400944471359253\n",
            "Loss: -3.086768865585327\n",
            "Loss: -2.835645914077759\n",
            "Loss: -2.6001334190368652\n",
            "Loss: -2.3591294288635254\n",
            "Loss: -2.1323792934417725\n",
            "Loss: -1.9200800657272339\n",
            "Loss: -1.7561086416244507\n",
            "We fooled the network after 33 iterations!\n",
            "New prediction: 1\n",
            "14909img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -38.626583099365234\n",
            "Loss: -37.93955612182617\n",
            "Loss: -37.25299835205078\n",
            "Loss: -36.55064010620117\n",
            "Loss: -35.85020065307617\n",
            "Loss: -35.135440826416016\n",
            "Loss: -34.43136215209961\n",
            "Loss: -33.72766876220703\n",
            "Loss: -33.018497467041016\n",
            "Loss: -32.32183837890625\n",
            "Loss: -31.623926162719727\n",
            "Loss: -30.921056747436523\n",
            "Loss: -30.22089195251465\n",
            "Loss: -29.506357192993164\n",
            "Loss: -28.786813735961914\n",
            "Loss: -28.07611083984375\n",
            "Loss: -27.375219345092773\n",
            "Loss: -26.641149520874023\n",
            "Loss: -25.950279235839844\n",
            "Loss: -25.2558536529541\n",
            "Loss: -24.571596145629883\n",
            "Loss: -23.881093978881836\n",
            "Loss: -23.200746536254883\n",
            "Loss: -22.51737403869629\n",
            "Loss: -21.84883689880371\n",
            "Loss: -21.15528678894043\n",
            "Loss: -20.590518951416016\n",
            "Loss: -20.005823135375977\n",
            "Loss: -19.389324188232422\n",
            "Loss: -18.787002563476562\n",
            "Loss: -18.182199478149414\n",
            "Loss: -17.700746536254883\n",
            "Loss: -17.19675064086914\n",
            "Loss: -16.6967830657959\n",
            "Loss: -16.220211029052734\n",
            "Loss: -15.722373962402344\n",
            "Loss: -15.256299018859863\n",
            "Loss: -14.80652141571045\n",
            "Loss: -14.341069221496582\n",
            "Loss: -13.87343692779541\n",
            "Loss: -13.38817310333252\n",
            "Loss: -13.039101600646973\n",
            "Loss: -12.718863487243652\n",
            "Loss: -12.393575668334961\n",
            "Loss: -12.083691596984863\n",
            "Loss: -11.764254570007324\n",
            "Loss: -11.445413589477539\n",
            "Loss: -11.132415771484375\n",
            "Loss: -10.825196266174316\n",
            "Loss: -10.509631156921387\n",
            "Loss: -10.185929298400879\n",
            "Loss: -9.876173973083496\n",
            "Loss: -9.548792839050293\n",
            "Loss: -9.250487327575684\n",
            "Loss: -8.91791820526123\n",
            "Loss: -8.580567359924316\n",
            "Loss: -8.225011825561523\n",
            "Loss: -7.897967338562012\n",
            "Loss: -7.563685417175293\n",
            "Loss: -7.243716716766357\n",
            "Loss: -6.926801681518555\n",
            "Loss: -6.622226715087891\n",
            "Loss: -6.329442977905273\n",
            "Loss: -6.030430793762207\n",
            "Loss: -5.73110818862915\n",
            "Loss: -5.439779281616211\n",
            "Loss: -5.144632339477539\n",
            "Loss: -4.855452537536621\n",
            "Loss: -4.561019420623779\n",
            "Loss: -4.283716678619385\n",
            "Loss: -3.994492769241333\n",
            "We fooled the network after 70 iterations!\n",
            "New prediction: 8\n",
            "14910img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -7.0569257736206055\n",
            "Loss: -6.673171520233154\n",
            "Loss: -6.3009724617004395\n",
            "We fooled the network after 2 iterations!\n",
            "New prediction: 0\n",
            "14912img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -36.78182601928711\n",
            "Loss: -36.119171142578125\n",
            "Loss: -35.421295166015625\n",
            "Loss: -34.727169036865234\n",
            "Loss: -33.98648452758789\n",
            "Loss: -33.26797866821289\n",
            "Loss: -32.528812408447266\n",
            "Loss: -31.778886795043945\n",
            "Loss: -31.023778915405273\n",
            "Loss: -30.27126121520996\n",
            "Loss: -29.51946449279785\n",
            "Loss: -28.736957550048828\n",
            "Loss: -27.970211029052734\n",
            "Loss: -27.258852005004883\n",
            "Loss: -26.66326904296875\n",
            "Loss: -26.075902938842773\n",
            "Loss: -25.471694946289062\n",
            "Loss: -24.902067184448242\n",
            "Loss: -24.31193733215332\n",
            "Loss: -23.72626304626465\n",
            "Loss: -23.121427536010742\n",
            "Loss: -22.502351760864258\n",
            "Loss: -21.89655113220215\n",
            "Loss: -21.277746200561523\n",
            "Loss: -20.664546966552734\n",
            "Loss: -20.086294174194336\n",
            "Loss: -19.47657585144043\n",
            "Loss: -18.903074264526367\n",
            "Loss: -18.309755325317383\n",
            "Loss: -17.736032485961914\n",
            "Loss: -17.1768741607666\n",
            "Loss: -16.597713470458984\n",
            "Loss: -16.01262092590332\n",
            "Loss: -15.38779354095459\n",
            "Loss: -14.814154624938965\n",
            "Loss: -14.175375938415527\n",
            "Loss: -13.609140396118164\n",
            "Loss: -12.997061729431152\n",
            "Loss: -12.432513236999512\n",
            "Loss: -11.894927024841309\n",
            "Loss: -11.368694305419922\n",
            "Loss: -10.821759223937988\n",
            "Loss: -10.305179595947266\n",
            "Loss: -9.775836944580078\n",
            "Loss: -9.307090759277344\n",
            "Loss: -8.850035667419434\n",
            "Loss: -8.384175300598145\n",
            "Loss: -8.071318626403809\n",
            "Loss: -7.885663986206055\n",
            "Loss: -7.661525726318359\n",
            "Loss: -7.469080924987793\n",
            "Loss: -7.2658467292785645\n",
            "Loss: -7.065179347991943\n",
            "Loss: -6.871225833892822\n",
            "Loss: -6.661888122558594\n",
            "Loss: -6.471810340881348\n",
            "Loss: -6.270862579345703\n",
            "Loss: -6.065507411956787\n",
            "Loss: -5.85347843170166\n",
            "Loss: -5.650184631347656\n",
            "Loss: -5.439041614532471\n",
            "Loss: -5.250048637390137\n",
            "Loss: -5.030735015869141\n",
            "Loss: -4.856316089630127\n",
            "Loss: -4.6659440994262695\n",
            "Loss: -4.4794020652771\n",
            "Loss: -4.302839756011963\n",
            "Loss: -4.114874839782715\n",
            "Loss: -3.9475739002227783\n",
            "Loss: -3.7777650356292725\n",
            "Loss: -3.603895902633667\n",
            "Loss: -3.453737497329712\n",
            "Loss: -3.2709877490997314\n",
            "Loss: -3.082078218460083\n",
            "Loss: -2.8881356716156006\n",
            "Loss: -2.6924760341644287\n",
            "Loss: -2.501088857650757\n",
            "Loss: -2.302736520767212\n",
            "Loss: -2.0929057598114014\n",
            "Loss: -1.8989778757095337\n",
            "Loss: -1.6943442821502686\n",
            "Loss: -1.4802231788635254\n",
            "Loss: -1.1769812107086182\n",
            "We fooled the network after 82 iterations!\n",
            "New prediction: 4\n",
            "14921img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -7.65740442276001\n",
            "Loss: -7.100872993469238\n",
            "Loss: -6.4677958488464355\n",
            "Loss: -5.81916618347168\n",
            "Loss: -5.151351451873779\n",
            "Loss: -4.5604753494262695\n",
            "Loss: -3.9755136966705322\n",
            "Loss: -3.2775049209594727\n",
            "We fooled the network after 7 iterations!\n",
            "New prediction: 8\n",
            "14940img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -13.174296379089355\n",
            "Loss: -12.599282264709473\n",
            "Loss: -12.034306526184082\n",
            "Loss: -11.451532363891602\n",
            "Loss: -10.879043579101562\n",
            "Loss: -10.285623550415039\n",
            "Loss: -9.706942558288574\n",
            "Loss: -9.124560356140137\n",
            "Loss: -8.538777351379395\n",
            "Loss: -8.01028060913086\n",
            "Loss: -7.521731853485107\n",
            "Loss: -7.016633987426758\n",
            "Loss: -6.493738651275635\n",
            "Loss: -5.9734296798706055\n",
            "Loss: -5.495713710784912\n",
            "Loss: -5.064459800720215\n",
            "Loss: -4.6512250900268555\n",
            "Loss: -4.253533363342285\n",
            "Loss: -3.948277711868286\n",
            "Loss: -3.63094162940979\n",
            "Loss: -3.311654806137085\n",
            "Loss: -3.0586111545562744\n",
            "We fooled the network after 21 iterations!\n",
            "New prediction: 8\n",
            "14952img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -34.182987213134766\n",
            "Loss: -33.6026496887207\n",
            "Loss: -32.98198318481445\n",
            "Loss: -32.27482223510742\n",
            "Loss: -31.582645416259766\n",
            "Loss: -30.92027473449707\n",
            "Loss: -30.33869171142578\n",
            "Loss: -29.749073028564453\n",
            "Loss: -29.161821365356445\n",
            "Loss: -28.580278396606445\n",
            "Loss: -28.011178970336914\n",
            "Loss: -27.42074966430664\n",
            "Loss: -26.84001922607422\n",
            "Loss: -26.25432014465332\n",
            "Loss: -25.670555114746094\n",
            "Loss: -25.08757209777832\n",
            "Loss: -24.507240295410156\n",
            "Loss: -23.9450626373291\n",
            "Loss: -23.379749298095703\n",
            "Loss: -22.83243751525879\n",
            "Loss: -22.259422302246094\n",
            "Loss: -21.695905685424805\n",
            "Loss: -21.136425018310547\n",
            "Loss: -20.573766708374023\n",
            "Loss: -20.068628311157227\n",
            "Loss: -19.607507705688477\n",
            "Loss: -19.148908615112305\n",
            "Loss: -18.701894760131836\n",
            "Loss: -18.24249839782715\n",
            "Loss: -17.87779426574707\n",
            "Loss: -17.488792419433594\n",
            "Loss: -17.10749626159668\n",
            "Loss: -16.71003532409668\n",
            "Loss: -16.32505989074707\n",
            "Loss: -15.979334831237793\n",
            "Loss: -15.6347017288208\n",
            "Loss: -15.292922019958496\n",
            "Loss: -14.957715034484863\n",
            "Loss: -14.617913246154785\n",
            "Loss: -14.278695106506348\n",
            "Loss: -13.947924613952637\n",
            "Loss: -13.613683700561523\n",
            "Loss: -13.30159854888916\n",
            "Loss: -12.963042259216309\n",
            "Loss: -12.634881973266602\n",
            "Loss: -12.31113052368164\n",
            "Loss: -11.975114822387695\n",
            "Loss: -11.634121894836426\n",
            "Loss: -11.30099868774414\n",
            "Loss: -10.973115921020508\n",
            "Loss: -10.628878593444824\n",
            "Loss: -10.303143501281738\n",
            "Loss: -9.972679138183594\n",
            "Loss: -9.634827613830566\n",
            "Loss: -9.313803672790527\n",
            "Loss: -8.977584838867188\n",
            "Loss: -8.644952774047852\n",
            "Loss: -8.306535720825195\n",
            "Loss: -7.975811958312988\n",
            "Loss: -7.654736518859863\n",
            "Loss: -7.323698043823242\n",
            "Loss: -6.986865043640137\n",
            "Loss: -6.688577651977539\n",
            "Loss: -6.361887454986572\n",
            "Loss: -6.03547477722168\n",
            "Loss: -5.770388603210449\n",
            "Loss: -5.523974418640137\n",
            "Loss: -5.2752485275268555\n",
            "Loss: -5.0393147468566895\n",
            "Loss: -4.8179473876953125\n",
            "Loss: -4.569876670837402\n",
            "Loss: -4.335760116577148\n",
            "Loss: -4.118280410766602\n",
            "Loss: -3.9374730587005615\n",
            "Loss: -3.7436459064483643\n",
            "Loss: -3.5619213581085205\n",
            "Loss: -3.3769681453704834\n",
            "Loss: -3.1987693309783936\n",
            "Loss: -3.0582687854766846\n",
            "Loss: -2.892209768295288\n",
            "Loss: -2.75622820854187\n",
            "Loss: -2.6368255615234375\n",
            "Loss: -2.5026533603668213\n",
            "Loss: -2.3759658336639404\n",
            "Loss: -2.2477214336395264\n",
            "Loss: -2.1122710704803467\n",
            "Loss: -2.007056474685669\n",
            "Loss: -1.933254599571228\n",
            "Loss: -1.8590351343154907\n",
            "We fooled the network after 88 iterations!\n",
            "New prediction: 8\n",
            "14960img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -36.401180267333984\n",
            "Loss: -35.79243850708008\n",
            "Loss: -35.136383056640625\n",
            "Loss: -34.500553131103516\n",
            "Loss: -33.8525276184082\n",
            "Loss: -33.2401008605957\n",
            "Loss: -32.623748779296875\n",
            "Loss: -32.00939178466797\n",
            "Loss: -31.384737014770508\n",
            "Loss: -30.74483871459961\n",
            "Loss: -30.119888305664062\n",
            "Loss: -29.51952362060547\n",
            "Loss: -28.90423011779785\n",
            "Loss: -28.28327751159668\n",
            "Loss: -27.66754722595215\n",
            "Loss: -27.09421730041504\n",
            "Loss: -26.585744857788086\n",
            "Loss: -26.08835792541504\n",
            "Loss: -25.606184005737305\n",
            "Loss: -25.12287712097168\n",
            "Loss: -24.63258171081543\n",
            "Loss: -24.16905975341797\n",
            "Loss: -23.691150665283203\n",
            "Loss: -23.220853805541992\n",
            "Loss: -22.75010108947754\n",
            "Loss: -22.28529930114746\n",
            "Loss: -21.815561294555664\n",
            "Loss: -21.367151260375977\n",
            "Loss: -20.914499282836914\n",
            "Loss: -20.477067947387695\n",
            "Loss: -20.040552139282227\n",
            "Loss: -19.662599563598633\n",
            "Loss: -19.214738845825195\n",
            "Loss: -18.85930824279785\n",
            "Loss: -18.40721321105957\n",
            "Loss: -18.013437271118164\n",
            "Loss: -17.616859436035156\n",
            "Loss: -17.240890502929688\n",
            "Loss: -16.87498664855957\n",
            "Loss: -16.494962692260742\n",
            "Loss: -16.104883193969727\n",
            "Loss: -15.722887992858887\n",
            "Loss: -15.35737419128418\n",
            "Loss: -14.98401927947998\n",
            "Loss: -14.634467124938965\n",
            "Loss: -14.330565452575684\n",
            "Loss: -14.041487693786621\n",
            "Loss: -13.73885440826416\n",
            "Loss: -13.41368579864502\n",
            "Loss: -13.083029747009277\n",
            "Loss: -12.789628982543945\n",
            "Loss: -12.45688533782959\n",
            "Loss: -12.158489227294922\n",
            "Loss: -11.810934066772461\n",
            "Loss: -11.495097160339355\n",
            "Loss: -11.150569915771484\n",
            "Loss: -10.855144500732422\n",
            "Loss: -10.538361549377441\n",
            "Loss: -10.202049255371094\n",
            "Loss: -9.93239974975586\n",
            "Loss: -9.576872825622559\n",
            "Loss: -9.24216365814209\n",
            "Loss: -8.934102058410645\n",
            "Loss: -8.580674171447754\n",
            "Loss: -8.237279891967773\n",
            "Loss: -7.92476749420166\n",
            "Loss: -7.557074069976807\n",
            "Loss: -7.223306655883789\n",
            "Loss: -6.916112899780273\n",
            "Loss: -6.574714183807373\n",
            "Loss: -6.245609760284424\n",
            "Loss: -5.731631278991699\n",
            "Loss: -5.192123889923096\n",
            "Loss: -4.725622653961182\n",
            "Loss: -4.268000602722168\n",
            "Loss: -3.8721845149993896\n",
            "Loss: -3.4543774127960205\n",
            "Loss: -3.0411102771759033\n",
            "Loss: -2.6625490188598633\n",
            "We fooled the network after 78 iterations!\n",
            "New prediction: 4\n",
            "14963img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -45.53764724731445\n",
            "Loss: -44.933650970458984\n",
            "Loss: -44.300418853759766\n",
            "Loss: -43.66675567626953\n",
            "Loss: -43.02268600463867\n",
            "Loss: -42.372859954833984\n",
            "Loss: -41.7313117980957\n",
            "Loss: -41.08599853515625\n",
            "Loss: -40.464515686035156\n",
            "Loss: -39.83700180053711\n",
            "Loss: -39.20828628540039\n",
            "Loss: -38.60000228881836\n",
            "Loss: -38.00313186645508\n",
            "Loss: -37.38411331176758\n",
            "Loss: -36.77408218383789\n",
            "Loss: -36.17168045043945\n",
            "Loss: -35.55630874633789\n",
            "Loss: -34.98788070678711\n",
            "Loss: -34.4338264465332\n",
            "Loss: -33.84716033935547\n",
            "Loss: -33.2742919921875\n",
            "Loss: -32.73624038696289\n",
            "Loss: -32.18301010131836\n",
            "Loss: -31.54424476623535\n",
            "Loss: -30.87981605529785\n",
            "Loss: -30.32745933532715\n",
            "Loss: -29.791500091552734\n",
            "Loss: -29.286636352539062\n",
            "Loss: -28.75204086303711\n",
            "Loss: -28.228540420532227\n",
            "Loss: -27.707170486450195\n",
            "Loss: -27.186105728149414\n",
            "Loss: -26.652910232543945\n",
            "Loss: -26.13463020324707\n",
            "Loss: -25.58524513244629\n",
            "Loss: -25.069013595581055\n",
            "Loss: -24.579633712768555\n",
            "Loss: -24.062660217285156\n",
            "Loss: -23.5256290435791\n",
            "Loss: -23.028499603271484\n",
            "Loss: -22.504484176635742\n",
            "Loss: -21.992122650146484\n",
            "Loss: -21.512235641479492\n",
            "Loss: -21.00361442565918\n",
            "Loss: -20.512792587280273\n",
            "Loss: -19.998628616333008\n",
            "Loss: -19.51563262939453\n",
            "Loss: -19.04060935974121\n",
            "Loss: -18.5648136138916\n",
            "Loss: -18.087730407714844\n",
            "Loss: -17.59679412841797\n",
            "Loss: -17.156444549560547\n",
            "Loss: -16.66163444519043\n",
            "Loss: -16.17548370361328\n",
            "Loss: -15.682719230651855\n",
            "Loss: -15.18127727508545\n",
            "Loss: -14.708495140075684\n",
            "Loss: -14.201727867126465\n",
            "Loss: -13.678534507751465\n",
            "Loss: -13.162835121154785\n",
            "Loss: -12.652729034423828\n",
            "Loss: -12.138855934143066\n",
            "Loss: -11.635235786437988\n",
            "Loss: -11.102734565734863\n",
            "Loss: -10.581427574157715\n",
            "Loss: -10.042134284973145\n",
            "Loss: -9.54870319366455\n",
            "Loss: -9.053914070129395\n",
            "Loss: -8.543074607849121\n",
            "Loss: -8.05606746673584\n",
            "Loss: -7.637560844421387\n",
            "Loss: -7.221441268920898\n",
            "Loss: -6.821597576141357\n",
            "Loss: -6.413565635681152\n",
            "Loss: -5.9584126472473145\n",
            "Loss: -5.506628036499023\n",
            "Loss: -5.091286659240723\n",
            "Loss: -4.633269309997559\n",
            "Loss: -4.209010124206543\n",
            "Loss: -3.8084707260131836\n",
            "Loss: -3.3433406352996826\n",
            "Loss: -2.9079084396362305\n",
            "Loss: -2.4635748863220215\n",
            "Loss: -2.021132230758667\n",
            "Loss: -1.7990384101867676\n",
            "Loss: -1.5889006853103638\n",
            "Loss: -1.3780755996704102\n",
            "Loss: -1.162101149559021\n",
            "Loss: -0.9329943060874939\n",
            "We fooled the network after 88 iterations!\n",
            "New prediction: 8\n",
            "14973img is appending...\n",
            "Original prediction: 6\n",
            "Loss: -33.31003189086914\n",
            "Loss: -32.658348083496094\n",
            "Loss: -31.987394332885742\n",
            "Loss: -31.27093505859375\n",
            "Loss: -30.57283592224121\n",
            "Loss: -29.87790870666504\n",
            "Loss: -29.18755340576172\n",
            "Loss: -28.549455642700195\n",
            "Loss: -27.92160415649414\n",
            "Loss: -27.304563522338867\n",
            "Loss: -26.698293685913086\n",
            "Loss: -26.0884952545166\n",
            "Loss: -25.47677993774414\n",
            "Loss: -24.88323402404785\n",
            "Loss: -24.28605079650879\n",
            "Loss: -23.684133529663086\n",
            "Loss: -23.09646987915039\n",
            "Loss: -22.53928565979004\n",
            "Loss: -21.93797492980957\n",
            "Loss: -21.386484146118164\n",
            "Loss: -20.85013771057129\n",
            "Loss: -20.290491104125977\n",
            "Loss: -19.743244171142578\n",
            "Loss: -19.217435836791992\n",
            "Loss: -18.669057846069336\n",
            "Loss: -18.10449981689453\n",
            "Loss: -17.549091339111328\n",
            "Loss: -17.028291702270508\n",
            "Loss: -16.60196876525879\n",
            "Loss: -16.181840896606445\n",
            "Loss: -15.756632804870605\n",
            "Loss: -15.33138370513916\n",
            "Loss: -14.900572776794434\n",
            "Loss: -14.516310691833496\n",
            "Loss: -14.116119384765625\n",
            "Loss: -13.739985466003418\n",
            "Loss: -13.359352111816406\n",
            "Loss: -12.993730545043945\n",
            "Loss: -12.617169380187988\n",
            "Loss: -12.255541801452637\n",
            "Loss: -11.908520698547363\n",
            "Loss: -11.502270698547363\n",
            "Loss: -11.14925479888916\n",
            "Loss: -10.815705299377441\n",
            "Loss: -10.484050750732422\n",
            "Loss: -10.1549654006958\n",
            "Loss: -9.831594467163086\n",
            "Loss: -9.5142183303833\n",
            "Loss: -9.204676628112793\n",
            "Loss: -8.874250411987305\n",
            "Loss: -8.528362274169922\n",
            "Loss: -8.181868553161621\n",
            "Loss: -7.85612154006958\n",
            "Loss: -7.504034042358398\n",
            "Loss: -7.176872253417969\n",
            "Loss: -6.8389739990234375\n",
            "Loss: -6.4797515869140625\n",
            "Loss: -6.130250930786133\n",
            "Loss: -5.793329238891602\n",
            "Loss: -5.48836612701416\n",
            "Loss: -5.191632270812988\n",
            "Loss: -4.890174865722656\n",
            "Loss: -4.627269268035889\n",
            "Loss: -4.328304767608643\n",
            "Loss: -4.0309906005859375\n",
            "Loss: -3.733894109725952\n",
            "Loss: -3.411548376083374\n",
            "Loss: -3.0779969692230225\n",
            "Loss: -2.7092037200927734\n",
            "Loss: -2.356611967086792\n",
            "Loss: -2.036680221557617\n",
            "Loss: -1.8104748725891113\n",
            "Loss: -1.5948117971420288\n",
            "Loss: -1.3897141218185425\n",
            "Loss: -1.1985021829605103\n",
            "Loss: -1.0248302221298218\n",
            "Loss: -0.8635353446006775\n",
            "We fooled the network after 76 iterations!\n",
            "New prediction: 8\n",
            "14989img is appending...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg6G5-jJBncW",
        "outputId": "edc0a156-44b0-46cc-d5ac-28e877489343"
      },
      "source": [
        "train_data[1][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD-DUekWB56E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}